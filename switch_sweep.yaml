# program: train.py
method: random
metric:
  goal: minimize
  name: val_loss
parameters:
  mlp_dropout:
    values: [0.10, 0.15, 0.2, 0.3, 0.4]
  expert_dropout:
    values: [0.10, 0.15, 0.2, 0.3, 0.4]
  LR:
    values: [0.0005, 0.001, 0.003, 0.005, 0.01]
  batch_size:
    values: [16, 32, 64, 128, 256, 512]
  optimizer:
    values: ['adamw', 'sgd', 'momentum']
  activation:
    values: ['GELU', 'GEGLU', 'SwiGLU']
  n_experts:
    values: [2, 4, 8, 16, 32, 64]
  capacity_factor:
    values: [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]
  aux_loss_coef:
    values: [0.005, 0.01, 0.05, 0.1, 0.2]
  norm_first:
    values: [True, False]
  switch_first:
    values: [True, False]
  every_n_switch:
    values: [1, 2, 3, 4]
    
early_terminate:
  type: hyperband
  s: 2
  eta: 3
  max_iter: 27


# sweep using specified distribution
# parameters_dict.update({
#     'learning_rate': {
#         # a flat distribution between 0 and 0.1
#         'distribution': 'uniform',
#         'min': 0,
#         'max': 0.1
#       }})