{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d40f5-2bc2-4a3c-ad20-f973978ff1b2",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b05b899-86b9-4414-9f80-e197b9d3d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "MAX_ITERS = 50000  # max num batches to train\n",
    "PRINT_ITERS = 50  # frequency to print train loss\n",
    "EVAL_ITERS = 500  # frequency to evaluate val loss and generate text from model\n",
    "EVAL_ITER_COUNT = 100  # number of batches to estimate val loss with\n",
    "# given a 10% val split, we have 111540 char, so 100 batches * batch size 16 * seq len 128 = roughly 2x num of chars chosen\n",
    "# EVAL_ITER_COUNT * BATCH_SIZE\n",
    "SAVE_ITERS = 1000  # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4\n",
    "\n",
    "# automatic mixed precision (will be disabled if CPU, not available)\n",
    "USE_AMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1328ae-f388-4c15-8e78-a1c9430c88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN\n"
     ]
    }
   ],
   "source": [
    "## SMoE-specific hyperparameters TODO\n",
    "# CAPACITY_FACTOR = 1.25\n",
    "# N_EXPERT = 2\n",
    "# AUX_LOSS_COEF = 0.01\n",
    "\n",
    "MODEL_NAME = f\"smoe_{N_LAYER}_LAYERs_{N_HEAD}_HEAD_{N_EMBD}_EMBD_DIM_{SEQ_LEN}_SEQ_LEN\"\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b757be9-b268-493c-88a1-53b95db6e9a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6e1107-9998-4282-a8ca-0636e3f8e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002c526f-cb1a-4834-ad5e-ff1bce1dec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93bcb63-2867-4027-bee9-b0b4ca2a58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "root = root if drive is None else \"/content/drive/MyDrive/moe-kit\"\n",
    "path = path if drive is None else \"/content/drive/MyDrive/moe-kit/switch_transformer\"\n",
    "\n",
    "# cannot train in mixed precision on CPU (GradScaler needs cuda)\n",
    "USE_AMP = USE_AMP if device.type == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb48bc4-c538-4b31-add1-2750a08bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686a47d1-9b20-455d-8775-774bff55aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047b726-32b1-4b78-96d0-147f98398aa5",
   "metadata": {},
   "source": [
    "# Model (TODO——implement SMoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32f767-bd4a-4a8a-819f-96d1aaf825fb",
   "metadata": {},
   "source": [
    "# Action items:\n",
    "\n",
    "## implement noisy top-k gating (gate weight Wg, softplus of x•W_noise + ε~N(0, 1)), set non-topk to -∞, take softmax\n",
    "## compute coefficient of varation^2 for importance loss \n",
    "## router class to dispatch inputs, collect expert outputs, combine and return\n",
    "## maybe add ST-MoE's router z loss for fun (logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58da87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### below is switch code unmodified yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7584ae-6949-4608-8e80-6785c3502b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "## Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, n_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(n_ff, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = (\n",
    "            n_embd // n_head\n",
    "        )  # Dimension of each head's key, query, and value\n",
    "\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, D = x.size()\n",
    "        # split dimension into n_head * head_dim, then transpose the sequence length w/ n_head\n",
    "        # output: [B, n_head, S, head_dim]\n",
    "        return x.view(B, S, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, S, head_dim = x.size()  # _ is n_head which we will merge\n",
    "        # output: [B, S, n_embd]\n",
    "        return x.transpose(1, 2).contiguous().view(B, S, self.n_embd)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, dropout, mask=None):\n",
    "        # q,k,v are [B, n_head, S, head_dim]\n",
    "        # wei = [B, n_head, S, S]\n",
    "        wei = q @ k.transpose(-2, -1) / np.sqrt(self.head_dim)\n",
    "        # mask is [B, 1, S, S]\n",
    "        if mask is not None:\n",
    "            wei = wei.masked_fill(mask, float(\"-inf\"))\n",
    "        wei = dropout(F.softmax(wei, dim=-1))\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, S, n_embd)\n",
    "        # Step 1 and 2: Project full query, key, value, then split via reshaping\n",
    "        q = self.split_heads(self.query(x))\n",
    "        k = self.split_heads(self.key(x))\n",
    "        v = self.split_heads(self.value(x))\n",
    "\n",
    "        # Step 3: Compute scaled dot-product attention with causal mask\n",
    "        attn = self.scaled_dot_product(q, k, v, self.drop, mask)\n",
    "\n",
    "        # Step 4 and 5: Concatenate attention scores, return projected output matrix\n",
    "        out = self.out(self.combine_heads(attn))  # (B, S, n_embd)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SwitchFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Switch FeedForward Layer.\n",
    "    TODO\n",
    "    Inputs:\n",
    "        -\n",
    "    Returns: Tuple of length 4\n",
    "        -Layer output\n",
    "        -Token count per expert (for auxiliary loss)\n",
    "        -Sum of token probs per expert (for auxiliary loss)\n",
    "        -Token count dropped (for logging)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        n_ff,\n",
    "        use_amp,\n",
    "        capacity_factor,\n",
    "        drop_tokens: bool,\n",
    "        n_experts: int,\n",
    "        expert: MLP,\n",
    "        noise=0.1,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_amp = use_amp\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.n_experts = n_experts\n",
    "        self.drop_tokens = drop_tokens\n",
    "        self.noise = noise\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [copy.deepcopy(expert(d_model, n_ff, dropout)) for _ in range(n_experts)]\n",
    "        )\n",
    "\n",
    "        # Routing layer\n",
    "        self.switch = nn.Linear(d_model, n_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.float()  # cast to float32 for stability\n",
    "        B, S, n_embd = x.shape\n",
    "\n",
    "        # apply multiplicative jitter\n",
    "        if self.noise > 0:\n",
    "            x *= torch.empty_like(x).uniform_(1.0 - self.noise, 1.0 + self.noise)\n",
    "\n",
    "        x = rearrange(x, \"b s d -> (b s) d\")\n",
    "        probs = F.softmax(self.switch(x), dim=-1)  # (b*s) x n_experts\n",
    "\n",
    "        # convert to half precision\n",
    "        if self.use_amp:\n",
    "            probs = probs.half()\n",
    "            x = x.half()\n",
    "\n",
    "        max_prob, route_idx = torch.max(probs, dim=-1)\n",
    "\n",
    "        # compute expert capacity\n",
    "        # (num tokens * CF) / n_experts\n",
    "        capacity = int(x.shape[0] * self.capacity_factor / self.n_experts)\n",
    "\n",
    "        # obtain token idx for each expert\n",
    "        # list of len (n_expert) of tensors indicating token idx going to that expert\n",
    "        token_indices = [\n",
    "            torch.eq(route_idx, i).nonzero() for i in range(self.n_experts)\n",
    "        ]\n",
    "\n",
    "        # num tokens of each expert\n",
    "        # new_tensor ensures same dtype and device\n",
    "        expert_token_counts = x.new_tensor(\n",
    "            [len(token_indices[i]) for i in range(self.n_experts)]\n",
    "        )\n",
    "\n",
    "        # check capacity and drop tokens\n",
    "        dropped = []\n",
    "        if self.drop_tokens:\n",
    "            for i in range(self.n_experts):\n",
    "                if expert_token_counts[i] > capacity:\n",
    "                    # no shuffle——drop earlier tokens\n",
    "                    dropped.append(token_indices[i][capacity:])\n",
    "                    token_indices[i] = token_indices[i][:capacity]\n",
    "\n",
    "        # feed tokens to relevant experts\n",
    "        out = torch.zeros_like(x)\n",
    "        expert_out = [\n",
    "            self.experts[i](x[token_indices[i], :]) for i in range(self.n_experts)\n",
    "        ]\n",
    "\n",
    "        for i in range(self.n_experts):\n",
    "            out[token_indices[i], :] = expert_out[i]\n",
    "        if dropped:\n",
    "            # concat dropped tokens, skip experts\n",
    "            dropped = torch.cat(dropped)\n",
    "            out[dropped, :] = x[dropped, :]\n",
    "\n",
    "        # scale values by gating probabilities\n",
    "        # unsqueeze max_prob for broadcasting\n",
    "        out * rearrange(max_prob, \"num_tokens -> num_tokens ()\")\n",
    "\n",
    "        # separate batch_size and seq_len\n",
    "        # do not use SEQ_LEN or BATCH_SIZE. if inference, may have batch_size=1 and/or smaller seq len, for example\n",
    "        out = rearrange(out, \"(b s) d -> b s d\", s=S)\n",
    "\n",
    "        return out, expert_token_counts, probs.sum(0), len(dropped)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd,\n",
    "        n_head,\n",
    "        n_ff,\n",
    "        norm_first,\n",
    "        use_amp,\n",
    "        switch,\n",
    "        capacity_factor,\n",
    "        drop_tokens,\n",
    "        n_experts,\n",
    "        expert,\n",
    "        noise=0.1,\n",
    "        mlp_dropout=0.1,\n",
    "        expert_dropout=0.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, mlp_dropout)\n",
    "        if switch:\n",
    "            self.ff = SwitchFeedForward(\n",
    "                n_embd,\n",
    "                n_ff,\n",
    "                use_amp,\n",
    "                capacity_factor,\n",
    "                drop_tokens,\n",
    "                n_experts,\n",
    "                expert=MLP,\n",
    "                noise=noise,\n",
    "                dropout=mlp_dropout,\n",
    "            )  # no change to dropout here\n",
    "        else:\n",
    "            self.ff = MLP(n_embd, n_ff, dropout=mlp_dropout)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.norm_first = norm_first\n",
    "        self.mlp_drop = nn.Dropout(p=mlp_dropout)\n",
    "        self.expert_drop = nn.Dropout(p=expert_dropout)\n",
    "        self.switch = switch\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # residual connection (stream)\n",
    "\n",
    "        # pre layer norm\n",
    "        if self.norm_first:\n",
    "            x = x + self.mlp_drop(self.sa(self.ln1(x), mask))\n",
    "            if self.switch:\n",
    "                out, expert_token_counts, prob_sum, n_dropped = self.ff(self.ln2(x))\n",
    "                x = x + self.expert_drop(out)  # expert dropout\n",
    "                return x, expert_token_counts, prob_sum, n_dropped\n",
    "            else:\n",
    "                x = x + self.mlp_drop(self.ff(self.ln2(x)))\n",
    "        else:\n",
    "            x = self.ln1(x + self.mlp_drop(self.sa(x, mask)))\n",
    "            if self.switch:\n",
    "                out, expert_token_counts, prob_sum, n_dropped = self.ff(x)\n",
    "                x = self.ln1(x + self.expert_drop(out))  # expert dropout\n",
    "                return x, expert_token_counts, prob_sum, n_dropped\n",
    "            else:\n",
    "                x = self.ln2(x + self.mlp_drop(self.ff(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Formula taken from the original Transformer paper:\n",
    "    PE(pos, 2i (even)) = sin(pos/(10000^{2i/d_model}))\n",
    "    PE(pos, 2i+1 (odd)) = cos(pos/(10000^{2i/d_model}))\n",
    "\n",
    "    See reference for more details:\n",
    "    https://kikaben.com/transformers-positional-encoding/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len):\n",
    "        # just set d_model = n_embd and max_len = seq_len\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # [max_len, 1]\n",
    "        divisor = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model)\n",
    "        )  # [d_model / 2, half for each of sin and cos]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * divisor)\n",
    "        pe[:, 1::2] = torch.cos(position * divisor)\n",
    "        self.register_buffer(\n",
    "            \"pe\", pe\n",
    "        )  # result: self.pe = [max_len, d_model], mapping each token index to a vector of length d_model as desired\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.arange(seq_length) has shape [seq_length], so x.size(0) extracts it, then we index self.pe for the first seq_length mappings\n",
    "        # note we do not add the positional embeddings to x itself yet, we simply return them\n",
    "        # output = (seq_length, d_model=n_embd)\n",
    "        return self.pe[: x.size(0)]\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "\n",
    "class SMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    switch (bool): Indicates whether to insert Switch MoE layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        n_embd,\n",
    "        n_head,\n",
    "        n_ff,\n",
    "        n_layer,\n",
    "        device,\n",
    "        norm_first=True,\n",
    "        use_amp=False,\n",
    "        switch=False,\n",
    "        capacity_factor=None,\n",
    "        drop_tokens=None,\n",
    "        n_experts=None,\n",
    "        expert=None,\n",
    "        noise=0.1,\n",
    "        mlp_dropout=0.1,\n",
    "        expert_dropout=0.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if switch:\n",
    "            assert (\n",
    "                isinstance(capacity_factor, (int, float))\n",
    "                and isinstance(drop_tokens, bool)\n",
    "                and isinstance(n_experts, int)\n",
    "                and expert is not None\n",
    "            ), \"For a switch transformer, you must provide a numeric `capacity_factor`, boolean `drop_tokens`, \\\n",
    "                    integer `n_experts` and a MLP class `expert` to serve as the experts.\"\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = PositionalEncoding(n_embd, seq_length)\n",
    "\n",
    "        ### Alternate blocks with switch = True/False\n",
    "        switch_args = np.full((n_layer,), False)\n",
    "        if switch:\n",
    "            switch_args[::2], switch_args[1::2] = True, False\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(\n",
    "                    n_embd,\n",
    "                    n_head,\n",
    "                    n_ff,\n",
    "                    norm_first,\n",
    "                    use_amp,\n",
    "                    switch_args[i],\n",
    "                    capacity_factor,\n",
    "                    drop_tokens,\n",
    "                    n_experts,\n",
    "                    expert,\n",
    "                    noise,\n",
    "                    mlp_dropout,\n",
    "                    expert_dropout,\n",
    "                )\n",
    "                for i in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.drop = nn.Dropout(mlp_dropout)\n",
    "        self.switch = switch\n",
    "        self.seq_length = seq_length\n",
    "        self.device = device\n",
    "        self.use_amp = use_amp\n",
    "        self.init_params()\n",
    "\n",
    "    # weight initialization (Xavier uniform)\n",
    "    def init_params(self, default_initialization=False):\n",
    "        if not default_initialization:\n",
    "            for name, p in self.named_parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Remark: Xavier normal is not supported at this time.\n",
    "\n",
    "    def get_causal_mask(self, x):\n",
    "        \"\"\"\n",
    "        Generates causal mask for decoding\n",
    "        \"\"\"\n",
    "        B, S = x.shape  # x = (batch_size x seq_len)\n",
    "        attn_shape = (B, 1, S, S)\n",
    "        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\n",
    "            \"uint8\"\n",
    "        )  # k = 1 shifts the diagonal, so that the main diagonal gets 0's\n",
    "        return (torch.from_numpy(subsequent_mask) == 0).to(self.device)\n",
    "        # True along main diagonal + below, False elsewhere\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.to(torch.int64)\n",
    "        B, S = x.shape\n",
    "\n",
    "        # get mask\n",
    "        mask = self.get_causal_mask(x).to(self.device)\n",
    "        # mask = (B x 1 x S x S)\n",
    "\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(S))\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        # (B, S, n_embd)\n",
    "\n",
    "        expert_token_counts, prob_sum, n_dropped = [], [], []\n",
    "        for block in self.blocks:\n",
    "            if block.switch:\n",
    "                x, counts_i, prob_sum_i, n_dropped_i = block(x, ~mask)\n",
    "                expert_token_counts.append(counts_i)\n",
    "                prob_sum.append(prob_sum_i)\n",
    "                n_dropped.append(n_dropped_i)\n",
    "            else:\n",
    "                x = block(x, ~mask)  # (B, S, n_embd)\n",
    "        # negate mask to fill originally False values with -inf later\n",
    "        logits = self.lm_head(x)  # (B, S, vocab_size)\n",
    "\n",
    "        if self.switch:\n",
    "            return (\n",
    "                logits,\n",
    "                torch.stack(expert_token_counts),\n",
    "                torch.stack(prob_sum),\n",
    "                n_dropped,\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        method=\"multinomial\",\n",
    "        max_new_tokens=1000,\n",
    "        temp=None,\n",
    "        num_beams=None,\n",
    "        p_nucleus=None,\n",
    "        k=None,\n",
    "    ):\n",
    "\n",
    "        # input_ids begins as (B, S)\n",
    "        self.eval()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if method in [\"multinomial\", \"temperature\", \"greedy\", \"nucleus\", \"top-k\"]:\n",
    "                # i) Truncate to the most recent `max length` tokens\n",
    "                text_cond = input_ids[:, -self.seq_length :]\n",
    "                # ii) Retrieve predictions\n",
    "                with torch.no_grad():\n",
    "                    with torch.autocast(\n",
    "                        device_type=self.device.type,\n",
    "                        dtype=torch.bfloat16,\n",
    "                        enabled=self.use_amp,\n",
    "                    ):\n",
    "                        if self.switch:\n",
    "                            logits, _, _, _ = self(text_cond)\n",
    "                        else:\n",
    "                            logits = self(text_cond)\n",
    "                # model output: (B, S, vocab_size)\n",
    "                # iii) Find last token logits of each\n",
    "                logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "                # if temperature sampling, divide logits by temp before applying softmax\n",
    "                if method == \"temperature\":\n",
    "                    logits = logits / temp\n",
    "\n",
    "                # iv) Take softmax along each\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # v) Sample next token depending on method\n",
    "                if method == \"greedy\":\n",
    "                    next_idx = probs.argmax(dim=-1).unsqueeze(-1)\n",
    "\n",
    "                elif method in [\"multinomial\", \"temperature\", \"nucleus\", \"top-k\"]:\n",
    "                    if method == \"nucleus\":\n",
    "                        assert (\n",
    "                            p_nucleus is not None\n",
    "                            and (0 < p_nucleus)\n",
    "                            and (p_nucleus <= 1)\n",
    "                        )\n",
    "\n",
    "                        sorted_probs, sorted_idx = probs.sort(dim=-1, descending=True)\n",
    "                        prob_cumsum = sorted_probs.cumsum(dim=-1)\n",
    "                        idx_remove = prob_cumsum > p_nucleus\n",
    "                        # shift one right to ensure the first token is above the threshold\n",
    "                        idx_remove[..., 1:] = idx_remove[..., :-1].clone()\n",
    "                        idx_remove[..., 0] = False\n",
    "                        # retrieve original indices by reverse-sorting\n",
    "                        remove_mask = idx_remove.gather(\n",
    "                            dim=-1, index=sorted_idx.argsort(dim=-1)\n",
    "                        )\n",
    "                        # ^ specifically, we do this by first argsorting the indices which were returned from argsort\n",
    "                        # you can show that this returns indices that when used to subset a sorted array, returns the original array in unsorted order\n",
    "                        # https://stackoverflow.com/questions/52127723/pytorch-better-way-to-get-back-original-tensor-order-after-torch-sort\n",
    "                        probs[remove_mask] = 0\n",
    "\n",
    "                    if method == \"top-k\":\n",
    "                        remove_mask = (\n",
    "                            probs < torch.topk(probs, k).values[..., -1, None]\n",
    "                        )  # topk returns (B, 1), leaving only the\n",
    "                        # kth largest probs (i.e. the cutoff value for each). Then mask is same size as probs (B, vocab_size)\n",
    "                        probs[remove_mask] = 0\n",
    "\n",
    "                    # Sample probabilistically via scores\n",
    "                    next_idx = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "                # vi) Autoregressively append to input_text\n",
    "                input_ids = torch.cat((input_ids, next_idx), dim=-1)\n",
    "\n",
    "                # now input_text = (B, S + 1)\n",
    "\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713212-4a60-4e60-90f4-f6e0a704a299",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b6d9fe-8f18-474d-aefb-4e44fd0b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f\"Vocab: {chars}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545415c0-5e49-42c9-b10a-1432028233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 47, 52, 63, 7, 57, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 57, 1, 57, 47, 41, 49]\n",
      "tiny-shakespeare is sick\n"
     ]
    }
   ],
   "source": [
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "print(encode(\"tiny-shakespeare is sick\"))\n",
    "print(decode(encode(\"tiny-shakespeare is sick\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0124c4-58be-4acd-8249-fd9e5cecbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 1003854 val_data len: 111540\n"
     ]
    }
   ],
   "source": [
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data len:\", len(train_data), \"val_data len:\", len(val_data))\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + SEQ_LEN + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af611b-be70-4737-84df-f7bf13b057c9",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679ef31f-5253-49f9-9e24-c64b05655cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Terru/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "model = SMoE(\n",
    "    VOCAB_SIZE,\n",
    "    SEQ_LEN,\n",
    "    N_EMBD,\n",
    "    N_HEAD,\n",
    "    N_FF,\n",
    "    N_LAYER,\n",
    "    device=device,\n",
    "    norm_first=True,\n",
    "    switch=True,\n",
    "    capacity_factor=CAPACITY_FACTOR,\n",
    "    drop_tokens=True,\n",
    "    n_experts=N_EXPERT,\n",
    "    expert=MLP,\n",
    "    use_amp=USE_AMP,\n",
    "    mlp_dropout=0.1,\n",
    "    expert_dropout=0.4,\n",
    ")\n",
    "\n",
    "# Gradient scaling for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eacebc5-8b3a-4c5d-9c89-f946fd25e948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "├─Embedding: 1-1                                   8,320\n",
       "├─PositionalEncoding: 1-2                          --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─Block: 2-1                                  --\n",
       "│    │    └─MultiHeadAttention: 3-1                65,536\n",
       "│    │    └─SwitchFeedForward: 3-2                 263,682\n",
       "│    │    └─LayerNorm: 3-3                         256\n",
       "│    │    └─LayerNorm: 3-4                         256\n",
       "│    │    └─Dropout: 3-5                           --\n",
       "│    │    └─Dropout: 3-6                           --\n",
       "│    └─Block: 2-2                                  --\n",
       "│    │    └─MultiHeadAttention: 3-7                65,536\n",
       "│    │    └─MLP: 3-8                               131,712\n",
       "│    │    └─LayerNorm: 3-9                         256\n",
       "│    │    └─LayerNorm: 3-10                        256\n",
       "│    │    └─Dropout: 3-11                          --\n",
       "│    │    └─Dropout: 3-12                          --\n",
       "│    └─Block: 2-3                                  --\n",
       "│    │    └─MultiHeadAttention: 3-13               65,536\n",
       "│    │    └─SwitchFeedForward: 3-14                263,682\n",
       "│    │    └─LayerNorm: 3-15                        256\n",
       "│    │    └─LayerNorm: 3-16                        256\n",
       "│    │    └─Dropout: 3-17                          --\n",
       "│    │    └─Dropout: 3-18                          --\n",
       "│    └─Block: 2-4                                  --\n",
       "│    │    └─MultiHeadAttention: 3-19               65,536\n",
       "│    │    └─MLP: 3-20                              131,712\n",
       "│    │    └─LayerNorm: 3-21                        256\n",
       "│    │    └─LayerNorm: 3-22                        256\n",
       "│    │    └─Dropout: 3-23                          --\n",
       "│    │    └─Dropout: 3-24                          --\n",
       "├─Linear: 1-4                                      8,385\n",
       "├─Dropout: 1-5                                     --\n",
       "===========================================================================\n",
       "Total params: 1,071,685\n",
       "Trainable params: 1,071,685\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c9b8a-bf50-427a-9b9c-e6f95a3c63c7",
   "metadata": {},
   "source": [
    "## Computing activated parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96619ec1-24cf-4f35-b22d-91e2409d5461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807745"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690640c-df7d-4d0c-bee7-f8ff428df809",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48e940d-c7af-4367-81a9-3310531dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ce_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Computes cross-entropy loss.\n",
    "    Inputs:\n",
    "        -logits: Model output of shape (B, S, vocab_size)\n",
    "        -counts:\n",
    "    \"\"\"\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdbf6e1a-3965-49ed-ac0e-9031dc9a5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aux_loss(counts, prob_sum):\n",
    "    \"\"\"\n",
    "    Computes Switch Transformer auxiliary loss.\n",
    "    Inputs:\n",
    "        -counts: Number of tokens passed to each expert in each switch layer (num_switch_layers x n_experts)\n",
    "        Note this is NOT equivalent to n_layer; num_switch_layers = (n_layer//2) + (n_layer % 2)\n",
    "        -prob_sum: Sum of probs across all tokens for each expert (num_switch_layers x n_experts)\n",
    "    \"\"\"\n",
    "\n",
    "    # total number of tokens routed in that layer\n",
    "    token_count = counts.sum(dim=-1, keepdims=True)\n",
    "\n",
    "    # prop of tokens dispatched to each expert\n",
    "    route_frac = counts / token_count\n",
    "\n",
    "    # fraction of total probability allocated for each expert\n",
    "    # recall prob_sum := softmaxed probs, which added to 1 across the experts for each token\n",
    "    # we divide by num_tokens so that the overall 2D scalar sum of prob_frac is 1\n",
    "    # intuitively we are forcing the total prob for each layer across the experts to be 1 so we can take proportions,\n",
    "    # the same way as above\n",
    "    prob_frac = prob_sum / token_count\n",
    "\n",
    "    # Auxiliary loss\n",
    "    # L = N \\sum_{i=1}^N f_i • P_i\n",
    "    aux_loss = N_EXPERT * (route_frac * prob_frac).sum()\n",
    "    return aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca27c6e-2d5f-4131-87fa-275d8bc35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    device,\n",
    "    train_loss_list=None,\n",
    "    val_loss_list=None,\n",
    "    train_time_list=None,\n",
    "    val_aux_loss_list=None,\n",
    "    dropped_list=None,\n",
    "):\n",
    "\n",
    "    train_losses = train_loss_list if train_loss_list is not None else []\n",
    "    val_losses = val_loss_list if val_loss_list is not None else []\n",
    "    train_times = train_time_list if train_time_list is not None else []\n",
    "    val_aux_losses = val_aux_loss_list if val_aux_loss_list is not None else []\n",
    "    dropped = dropped_list if dropped_list is not None else []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up prompt generation\n",
    "    generation_file_path = f\"{path}/outputs/OUTPUT_{MODEL_NAME}_SEED_{SEED}.txt\"\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "    cond_prompts = [\"KING TERRY: Thou art\", \"DANIEL: Ay, my dear,\"]\n",
    "\n",
    "    cond_token_list = [encode(prompt) for prompt in cond_prompts]\n",
    "\n",
    "    for step in range(MAX_ITERS):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs, targets = get_batch(\"train\")\n",
    "        with torch.autocast(\n",
    "            device_type=device.type, dtype=torch.bfloat16, enabled=USE_AMP\n",
    "        ):\n",
    "            if model.switch:\n",
    "                logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                loss = calc_ce_loss(logits, targets)\n",
    "                aux_loss = calc_aux_loss(counts, prob_sum)\n",
    "                loss += AUX_LOSS_COEF * aux_loss\n",
    "                drop_frac = (np.array(n_dropped) / (BATCH_SIZE * SEQ_LEN)).tolist()\n",
    "                dropped.append(drop_frac)  # for logging purposes\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                loss = calc_ce_loss(logits, targets)\n",
    "\n",
    "        train_losses.append(loss.item())  # for printing\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Monitor gradient norm\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        with torch.autocast(\n",
    "            device_type=device.type, dtype=torch.bfloat16, enabled=USE_AMP\n",
    "        ):\n",
    "            grads = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            norm = torch.cat(grads).norm()\n",
    "\n",
    "        train_time = time.perf_counter() - start\n",
    "        tokens_per_sec = (1 / train_time) * BATCH_SIZE * SEQ_LEN\n",
    "        train_times.append(tokens_per_sec)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # print training statistics\n",
    "        if step % PRINT_ITERS == 0 and step != 0:\n",
    "            print(\n",
    "                f\"Step {step}/{MAX_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.5f} |\",\n",
    "                f\"Grad Norm: {norm:.3f} | Running Avg Tokens/Sec: {np.mean(train_times):.3f} |\",\n",
    "            )\n",
    "\n",
    "        # estimate val loss, generate text and save\n",
    "        if step % EVAL_ITERS == 0 and step != 0:\n",
    "            val_losses, val_aux_losses = estimate_loss(\n",
    "                model, val_losses, val_aux_losses, device\n",
    "            )\n",
    "            generate(model, generation_file_path, empty_tokens, cond_token_list, step)\n",
    "            model.train()\n",
    "\n",
    "        # save model, val losses (not train_losses), train times\n",
    "        if step % SAVE_ITERS == 0 and step != 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f\"{path}/checkpoints/{MODEL_NAME}_STEP_{step}_SEED_{SEED}.pt\",\n",
    "            )\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "            ) as f:\n",
    "                json.dump(val_losses, f)\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_aux_losses.json\", \"w\"\n",
    "            ) as f2:\n",
    "                json.dump(val_aux_losses, f2)\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_times.json\", \"w\"\n",
    "            ) as f3:\n",
    "                json.dump(\n",
    "                    train_times[EVAL_ITERS::EVAL_ITERS], f3\n",
    "                )  # match freq of val_losses\n",
    "                # note this means if you load from checkpoint to continue training you will have a sparser train_times\n",
    "                # list in computing running avg\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_dropped.json\", \"w\"\n",
    "            ) as f4:\n",
    "                json.dump(dropped[EVAL_ITERS::EVAL_ITERS], f4)  # same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0abba78-5cd4-4ee7-b116-203fdeef3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_losses, val_aux_losses, device):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    aux_losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch(\"test\")\n",
    "        with torch.autocast(\n",
    "            device_type=device.type, dtype=torch.bfloat16, enabled=USE_AMP\n",
    "        ):\n",
    "            if model.switch:\n",
    "                logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "                aux_losses[k] = calc_aux_loss(counts, prob_sum)\n",
    "                losses[k] += AUX_LOSS_COEF * aux_losses[k]\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "    val_loss, val_aux_loss = losses.mean().item(), aux_losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    val_aux_losses.append(val_aux_loss)  # track separate aux loss for logging\n",
    "    # keep model in eval, next call is to .generate() anyway\n",
    "    print(f\"Est. Val Loss: {val_loss:.5f} | Est. Aux Loss: {val_aux_loss:.5f}\")\n",
    "    return val_losses, val_aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a44ceb7-9dfe-4591-b7e4-b2486bb0eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, generation_file_path, empty_tokens, cond_token_list, step):\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    uncond_res1 = decode(\n",
    "        model.generate(empty_tokens, method=\"top-k\", k=5, max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    "    uncond_res2 = decode(\n",
    "        model.generate(\n",
    "            empty_tokens, method=\"nucleus\", p_nucleus=0.5, max_new_tokens=500\n",
    "        )[0].tolist()\n",
    "    )\n",
    "\n",
    "    cond_res_list = []\n",
    "    for prompt in cond_token_list:\n",
    "        cond_res = decode(\n",
    "            model.generate(\n",
    "                torch.tensor(prompt).unsqueeze(0).long().to(device),\n",
    "                method=\"top-k\",\n",
    "                k=5,\n",
    "                max_new_tokens=500,\n",
    "            )[0].tolist()\n",
    "        )\n",
    "        cond_res_list.append(cond_res)\n",
    "\n",
    "    cond_res_list = \"\\n\\n\".join(cond_res_list)\n",
    "\n",
    "    generation_text = f\"\"\"{MODEL_NAME} Output, Step {step}\n",
    "    UNCONDITIONAL GENERATION:\n",
    "\n",
    "    Top-k (5) (500 max_tokens):\n",
    "    {uncond_res1}\n",
    "\n",
    "    Nucleus (0.5) (500 max_tokens):\n",
    "    {uncond_res2}\n",
    "\n",
    "    #####################################################\n",
    "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
    "    {cond_res_list}\n",
    "    -----------------------------------------------------\n",
    "    \"\"\"\n",
    "    with open(generation_file_path, \"a\") as file:\n",
    "        file.write(generation_text)\n",
    "    print(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66bb7926-5ec9-492c-b22a-b3159a78de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50/50000 | Running Avg Train Loss: 4.22206 | Grad Norm: 1.482 | Running Avg Tokens/Sec: 6810.113 | Running Avg Route Frac: [[0.528 0.472]\n",
      " [0.420 0.580]]\n",
      "Step 100/50000 | Running Avg Train Loss: 3.78290 | Grad Norm: 1.492 | Running Avg Tokens/Sec: 6853.738 | Running Avg Route Frac: [[0.515 0.485]\n",
      " [0.454 0.546]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Driver code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 56\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device, train_loss_list, val_loss_list, train_time_list, val_aux_loss_list, dropped_list, route_frac_list)\u001b[0m\n\u001b[1;32m     52\u001b[0m         loss \u001b[38;5;241m=\u001b[39m calc_ce_loss(logits, targets)\n\u001b[1;32m     54\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# for printing\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Monitor gradient norm\u001b[39;00m\n\u001b[1;32m     60\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train(model, optimizer, scaler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "e5840a0c-62b9-4011-a370-8cdf1ba4be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50/50000 | Running Avg Train Loss: 4.26320 | Grad Norm: 1.009 | Running Avg Tokens/Sec: 6688.876\n",
      "Step 100/50000 | Running Avg Train Loss: 3.80411 | Grad Norm: 1.250 | Running Avg Tokens/Sec: 6746.328\n",
      "Step 150/50000 | Running Avg Train Loss: 3.56538 | Grad Norm: 1.857 | Running Avg Tokens/Sec: 6763.554\n",
      "Step 200/50000 | Running Avg Train Loss: 3.39562 | Grad Norm: 1.342 | Running Avg Tokens/Sec: 6709.624\n",
      "Step 250/50000 | Running Avg Train Loss: 3.27404 | Grad Norm: 1.251 | Running Avg Tokens/Sec: 6643.860\n",
      "Est. Val Loss: 2.62356 | Est. Aux Loss: 2.01001\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 250\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "\n",
      "Are  coor oooto thelanttsst sond bateses m man m win m bes tounderthe an withilouneselle t thirer toulir seng t terllore bour athes w b wore ssessearate alllllese serol lallel soulland ssss wind t ararathilan s tor thind angor sens tene sthan anerouss arl s astele toung thit wer therer seras we wes thand boulathe alle s cis as athinde t the bend se woust thes tele t torelon winsousel bowinlland and ase as tat arorales toras llendarale aran thes arerarorllererore s te thalllllloulalessthel toron\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ing sother s athend s thinde out manenore t moun me s there the manderere ben s s thend thillle t t sore there s ther t ben courlare lesthe ce solllllleralll ler thesous souson lerour bll ans t s s sellelelel balerathell wisel tor thend t the tor sound wang t se an sthesend tor thalerone won ll atoular and the thelllle worl werourel l t t l athalatoulere t allllllellllleller the t worerel all bend tore chand lllllllll s bengansthand theror thalere sour lller llerous s lant tous merthanthe the th\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou artherst ke w at t s the d thatas we s t b borth t thisote we s t banelat as bal tothe bllle westhe wisteris anor t wourlel wangelealleralllllessss taralangelllllallore wind cher s sellangerounssoundesthengot t t t se boulan s son bat we blllant we wond,\n",
      "\n",
      "\n",
      "CAlland,\n",
      "Thille the sss the soure angoutorongel terlell t ben win atathelllllllllend walle sowis we tero bll t barous sel t be tourelesel withowotounsorelasthir t werouthalllarerares allin terenst tantathangoresst the tor sowathoundathan tesorlle\n",
      "\n",
      "DANIEL: Ay, my dear, we tsth warereeanourid d t thon won borele ander mandout alerteng bl b thestend be blengotherenor wire b thanest t wan sthe bllllilasoun asterer t athes sthil ar sesonensthen t thout se t bell stherenonout thas wis sestererarongand cer allllle balest ther tes bere tarasessorene alandatoun te talas waras bll alle w tellire towelastotour thelore send t towhane arenest wines wel astharond th boulliner sender thest w llaler bor we t arellat at wor bllles t welllourelen tonowol cator s bland wand te\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 300/50000 | Running Avg Train Loss: 3.18377 | Grad Norm: 0.772 | Running Avg Tokens/Sec: 6610.147\n",
      "Step 350/50000 | Running Avg Train Loss: 3.11229 | Grad Norm: 1.231 | Running Avg Tokens/Sec: 6577.654\n",
      "Step 400/50000 | Running Avg Train Loss: 3.05471 | Grad Norm: 0.962 | Running Avg Tokens/Sec: 6568.742\n",
      "Step 450/50000 | Running Avg Train Loss: 3.00631 | Grad Norm: 0.852 | Running Avg Tokens/Sec: 6539.872\n",
      "Step 500/50000 | Running Avg Train Loss: 2.96397 | Grad Norm: 1.061 | Running Avg Tokens/Sec: 6554.312\n",
      "Est. Val Loss: 2.46547 | Est. Aux Loss: 2.00401\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 500\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "Ans thaso hesong thes so withir wour mees winghal win me shast s senen t manear thon to soulorirer d\n",
      "An miseng t teallase m mandites m o wore shes darore mall te s se han thyoun tho toulllll mind t tharathil thowis t s wine wor send ther seran dandend.\n",
      "\n",
      "\n",
      "IUSENGURGENENGUThe E:\n",
      "Wer therer meras we wes thand bo myour the thar s te me shour t wherend se woust thes te wathere\n",
      "And windise win winorind we dase me th minoranth to tharind wenge toun d\n",
      "As mendarore tishowe s te thalleller t thesthin d the\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "Whe meat hes me tore the tofous me therore soure tond hend this man there the s sererer thand me t the is there shand to f thaserourere\n",
      "\n",
      "The thas s s shasore de mather t souso theso thesor mare s thas wo thatharon se\n",
      "Whe me the she d thane de tor sour s wharo se on our thendes meso than then me mond thes seseseserd\n",
      "\n",
      "Whe s wourer we s there there s marousord s s wen thashe so thase we tous thandend tore mes the thas se s dond mathand the meror were sour s mengonde d the mand d s meron tore me d t\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art sesouse wastit s thind thato wone thin tore, teshis te we s thithalll menes\n",
      "Wor my wand and the wis aris t or toularond wenge s sthe ond seshishe t share\n",
      "Ther tour tingr s this teand meroun tounde mas towat d thatharis the son math thando merer thilland w mandere than the she the soure weris\n",
      "Whinden thelell t sto who marendellld thasher mane sowis we ouro tor mienenes se s thy tilonde s tisthowond myore te\n",
      "Wourer s be thericherares mende terene t manger thereser sthithind\n",
      "\n",
      "Whisendou mathes my t\n",
      "\n",
      "DANIEL: Ay, my dear, we ther ware, thes wind wer mone on torere matere anto m aler wer than moustend te t isof sere\n",
      "Ant wis s thane mer t o she thare t owoun owe ser tilares\n",
      "For t me he s whithen mithou the t this shelontont t thas wiseres wisengong\n",
      "Bu ter warathe malest ther tes sere theases whene wen soust the tharo thowise t male d me tie these matord w the myousend titowhane m tendo wines we t shas onder thye sther mender theso wind mer d me malend me withano th s s me malllowiterd\n",
      "Tondan sha se tinondowonder, \n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 550/50000 | Running Avg Train Loss: 2.92532 | Grad Norm: 1.029 | Running Avg Tokens/Sec: 6543.461\n",
      "Step 600/50000 | Running Avg Train Loss: 2.88968 | Grad Norm: 0.833 | Running Avg Tokens/Sec: 6539.411\n",
      "Step 650/50000 | Running Avg Train Loss: 2.85716 | Grad Norm: 0.768 | Running Avg Tokens/Sec: 6529.158\n",
      "Step 700/50000 | Running Avg Train Loss: 2.82681 | Grad Norm: 0.840 | Running Avg Tokens/Sec: 6522.984\n",
      "Step 750/50000 | Running Avg Train Loss: 2.79871 | Grad Norm: 0.738 | Running Avg Tokens/Sec: 6508.906\n",
      "Est. Val Loss: 2.31441 | Est. Aux Loss: 2.00828\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 750\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "And this bers tour thant the hend be mees\n",
      "But war win thiss to me mane t mane mard\n",
      "Monge the tith thes is meande that\n",
      "Whe mour athes me soure shes ticken ash the bo thit these\n",
      "Thours thish son bo tith bathes tho tor thind tower seas ther sur sulleres\n",
      "And this stee tof my thie ber thorst mears be tral ange\n",
      "Wit tour the thar sute he bure the tof sus he ous\n",
      "Tout wis wat tore the tof se dre dint thange ston be tat\n",
      "To be me tof shor thar bee she mand arit,\n",
      "The tish shee to be sthie bul thesthe ound b\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ARUS:\n",
      "Wher we the thand the the mand tot me mere torer tore the man there the send\n",
      "Bo the me the than the the at the tof shat sheare ang we my she thent sof\n",
      "The mat sour she be ther theas a s the thald the se bererd ther wit\n",
      "There wit hat the the sof se he wof me shears\n",
      "Wof mer thare me here me mend thease the thend be the\n",
      "Theat bear ther my thas therere win be ben thase ond thas\n",
      "The the mas wous me mean so and me ber songe thathe theall ther sof\n",
      "Ther me mer sour dous thand be as ben hare the he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, bord me thant shes to that suse sin my mes\n",
      "And, thote he she manth, tour so to my thourd thing there\n",
      "This that wou tea s the bust, thoulless, tharthe, wen tou sor tithe a bule teand ter\n",
      "Terange mim sha the the se bour the sof mat be be o mered this\n",
      "And thous mit the tere she the soured this\n",
      "Whinthene hit sthast bur bearenderers thas brices\n",
      "Wofas mat ourd of sour mimas all the tou me simy\n",
      "Athom thous mar sour tror tout there thares as\n",
      "Is thatest than thangerter thes tof ho thises\n",
      "Whart than my \n",
      "\n",
      "DANIEL: Ay, my dear, woutst, ware, ther be my trat, tho wistere ane my anto meat wing tor\n",
      "Thoul than be the son trend to mat sund tha ton so the blle thow,\n",
      "Thyo whe thinghes sthinger he she to he mith be se tof mend were thast thaser,\n",
      "I me waseng my wis mis that the berices, thean ane hard.\n",
      "\n",
      "GUCINGENGUS:\n",
      "Tou the thare tho son thar the me the the and,\n",
      "And wath tore send tof bener a tices beres o be ang thof thande\n",
      "Whyom mende tho sof merse mour as thend meat thand, bus bung, thind,\n",
      "Wheath mean hou se tinth hous trow\n",
      "    -----------------------------------------------------\n",
      "    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[810], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Driver code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[807], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device, train_loss_list, val_loss_list, train_time_list, val_aux_loss_list, dropped_list)\u001b[0m\n\u001b[1;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calc_ce_loss(logits, targets)\n\u001b[1;32m     38\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m  \u001b[38;5;66;03m# Monitor gradient norm\u001b[39;00m\n\u001b[1;32m     42\u001b[0m grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     43\u001b[0m         param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95154-d641-4d9b-9e8a-c054d5921921",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5157dd6-d6d6-42af-9104-272094033ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "772a7473-2e8f-4c61-87d9-612e01150b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRY: thou arte a my she\n",
      "Which have may and of contain.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good as I tall no knrow, for shalt agarnt\n",
      "And mpo; and Kong a m, not outhpile Mesce.\n",
      "\n",
      "HENRY VI:\n",
      "When I will thy lookess, oner the pexstrey\n",
      "The the the hee voagh gresed livioe.\n",
      "\n",
      "MENCIO:\n",
      "My her callis his peaced of to that\n",
      "We where's by shall bore: as shall myselvea\n",
      "The plender feuls!\n",
      "\n",
      "PAPELLANT:\n",
      "In the the into balby me dods to love,\n",
      "In but the giving of nyou ase. I tall it-me?'e Goveuling\n",
      "The theer haught art praver count madeng Camen:\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"TERRY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
