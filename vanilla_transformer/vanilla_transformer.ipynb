{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d40f5-2bc2-4a3c-ad20-f973978ff1b2",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b05b899-86b9-4414-9f80-e197b9d3d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "MAX_ITERS = 50000  # max num batches to train\n",
    "PRINT_ITERS = 50  # frequency to print train loss\n",
    "EVAL_ITERS = 500  # frequency to evaluate val loss and generate text from model\n",
    "EVAL_ITER_COUNT = 100  # number of batches to estimate val loss with\n",
    "# given a 10% val split, we have 111540 char, so 100 batches * batch size 16 * seq len 128 = roughly 2x num of chars chosen\n",
    "# EVAL_ITER_COUNT * BATCH_SIZE\n",
    "SAVE_ITERS = 1000  # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4\n",
    "\n",
    "MODEL_NAME = f\"vt_{N_LAYER}_LAYERs_{N_HEAD}_HEAD_{N_EMBD}_EMBD_DIM_{SEQ_LEN}_SEQ_LEN\"\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b757be9-b268-493c-88a1-53b95db6e9a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6e1107-9998-4282-a8ca-0636e3f8e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb48bc4-c538-4b31-add1-2750a08bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed, device\n",
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686a47d1-9b20-455d-8775-774bff55aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713212-4a60-4e60-90f4-f6e0a704a299",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b6d9fe-8f18-474d-aefb-4e44fd0b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f\"Vocab: {chars}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "545415c0-5e49-42c9-b10a-1432028233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 47, 52, 63, 7, 57, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 57, 1, 57, 47, 41, 49]\n",
      "tiny-shakespeare is sick\n"
     ]
    }
   ],
   "source": [
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "print(encode(\"tiny-shakespeare is sick\"))\n",
    "print(decode(encode(\"tiny-shakespeare is sick\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0124c4-58be-4acd-8249-fd9e5cecbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 1003854 val_data len: 111540\n"
     ]
    }
   ],
   "source": [
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data len:\", len(train_data), \"val_data len:\", len(val_data))\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + SEQ_LEN + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af611b-be70-4737-84df-f7bf13b057c9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679ef31f-5253-49f9-9e24-c64b05655cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "model = Transformer(\n",
    "    VOCAB_SIZE,\n",
    "    SEQ_LEN,\n",
    "    N_EMBD,\n",
    "    N_HEAD,\n",
    "    N_FF,\n",
    "    N_LAYER,\n",
    "    device=device,\n",
    "    switch=False,\n",
    "    mlp_dropout=0.1,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eacebc5-8b3a-4c5d-9c89-f946fd25e948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Transformer                              --\n",
       "├─Embedding: 1-1                         8,320\n",
       "├─PositionalEncoding: 1-2                --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Block: 2-1                        --\n",
       "│    │    └─MultiHeadAttention: 3-1      65,536\n",
       "│    │    └─MLP: 3-2                     131,712\n",
       "│    │    └─LayerNorm: 3-3               256\n",
       "│    │    └─LayerNorm: 3-4               256\n",
       "│    │    └─Dropout: 3-5                 --\n",
       "│    │    └─Dropout: 3-6                 --\n",
       "│    └─Block: 2-2                        --\n",
       "│    │    └─MultiHeadAttention: 3-7      65,536\n",
       "│    │    └─MLP: 3-8                     131,712\n",
       "│    │    └─LayerNorm: 3-9               256\n",
       "│    │    └─LayerNorm: 3-10              256\n",
       "│    │    └─Dropout: 3-11                --\n",
       "│    │    └─Dropout: 3-12                --\n",
       "│    └─Block: 2-3                        --\n",
       "│    │    └─MultiHeadAttention: 3-13     65,536\n",
       "│    │    └─MLP: 3-14                    131,712\n",
       "│    │    └─LayerNorm: 3-15              256\n",
       "│    │    └─LayerNorm: 3-16              256\n",
       "│    │    └─Dropout: 3-17                --\n",
       "│    │    └─Dropout: 3-18                --\n",
       "│    └─Block: 2-4                        --\n",
       "│    │    └─MultiHeadAttention: 3-19     65,536\n",
       "│    │    └─MLP: 3-20                    131,712\n",
       "│    │    └─LayerNorm: 3-21              256\n",
       "│    │    └─LayerNorm: 3-22              256\n",
       "│    │    └─Dropout: 3-23                --\n",
       "│    │    └─Dropout: 3-24                --\n",
       "├─Linear: 1-4                            8,385\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 807,745\n",
       "Trainable params: 807,745\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48e940d-c7af-4367-81a9-3310531dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, targets):\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca27c6e-2d5f-4131-87fa-275d8bc35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    device,\n",
    "    train_loss_list=None,\n",
    "    val_loss_list=None,\n",
    "    train_time_list=None,\n",
    "):\n",
    "\n",
    "    train_losses = train_loss_list if train_loss_list is not None else []\n",
    "    val_losses = val_loss_list if val_loss_list is not None else []\n",
    "    train_times = train_time_list if train_time_list is not None else []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up prompt generation\n",
    "    generation_file_path = f\"{path}/outputs/OUTPUT_{MODEL_NAME}_SEED_{SEED}.txt\"\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "    cond_prompts = [\"KING TERRY: Thou art\", \"DANIEL: Ay, my dear,\"]\n",
    "\n",
    "    cond_token_list = [encode(prompt) for prompt in cond_prompts]\n",
    "\n",
    "    for step in range(MAX_ITERS):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        inputs, targets = get_batch(\"train\")\n",
    "        logits = model(inputs)\n",
    "        loss = calc_loss(logits, targets)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Monitor gradient norm\n",
    "        grads = [\n",
    "            param.grad.detach().flatten()\n",
    "            for param in model.parameters()\n",
    "            if param.grad is not None\n",
    "        ]\n",
    "        norm = torch.cat(grads).norm()\n",
    "\n",
    "        train_time = time.perf_counter() - start\n",
    "        tokens_per_sec = (1 / train_time) * BATCH_SIZE * SEQ_LEN\n",
    "        train_times.append(tokens_per_sec)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training statistics\n",
    "        if step % PRINT_ITERS == 0 and step != 0:\n",
    "            print(\n",
    "                f\"Step {step}/{MAX_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.5f} |\",\n",
    "                f\"Grad Norm: {norm:.3f} | Running Avg Tokens/Sec: {np.mean(train_times):.3f}\",\n",
    "            )\n",
    "\n",
    "        # estimate val loss, generate text and save\n",
    "        if step % EVAL_ITERS == 0 and step != 0:\n",
    "            val_losses = estimate_loss(model, val_losses)\n",
    "            generate(model, generation_file_path, empty_tokens, cond_token_list, step)\n",
    "            model.train()\n",
    "\n",
    "        # save model, val losses (not train_losses), train times\n",
    "        if step % SAVE_ITERS == 0 and step != 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f\"{path}/checkpoints/{MODEL_NAME}_STEP_{step}_SEED_{SEED}.pt\",\n",
    "            )\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "            ) as f:\n",
    "                json.dump(val_losses, f)\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_times.json\", \"w\"\n",
    "            ) as f2:\n",
    "                json.dump(\n",
    "                    train_times[EVAL_ITERS::EVAL_ITERS], f2\n",
    "                )  # match freq of val_losses\n",
    "                # note this means if you load from checkpoint to continue training you will have a sparser train_times\n",
    "                # list in computing running avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0abba78-5cd4-4ee7-b116-203fdeef3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_losses):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch(\"test\")\n",
    "        logits = model(inputs)\n",
    "        losses[k] = calc_loss(logits, targets).item()\n",
    "    val_loss = losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    # keep model in eval, next call is to .generate() anyway\n",
    "    print(f\"Est. Val Loss: {val_loss:.5f}\")\n",
    "    return val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a44ceb7-9dfe-4591-b7e4-b2486bb0eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, generation_file_path, empty_tokens, cond_token_list, step):\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    uncond_res1 = decode(\n",
    "        model.generate(empty_tokens, method=\"top-k\", k=5, max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    "    uncond_res2 = decode(\n",
    "        model.generate(\n",
    "            empty_tokens, method=\"nucleus\", p_nucleus=0.5, max_new_tokens=500\n",
    "        )[0].tolist()\n",
    "    )\n",
    "\n",
    "    cond_res_list = []\n",
    "    for prompt in cond_token_list:\n",
    "        cond_res = decode(\n",
    "            model.generate(\n",
    "                torch.tensor(prompt).unsqueeze(0).long().to(device),\n",
    "                method=\"top-k\",\n",
    "                k=5,\n",
    "                max_new_tokens=500,\n",
    "            )[0].tolist()\n",
    "        )\n",
    "        cond_res_list.append(cond_res)\n",
    "\n",
    "    cond_res_list = \"\\n\\n\".join(cond_res_list)\n",
    "\n",
    "    generation_text = f\"\"\"{MODEL_NAME} Output, Step {step}\n",
    "    UNCONDITIONAL GENERATION:\n",
    "\n",
    "    Top-k (5) (500 max_tokens):\n",
    "    {uncond_res1}\n",
    "\n",
    "    Nucleus (0.5) (500 max_tokens):\n",
    "    {uncond_res2}\n",
    "\n",
    "    #####################################################\n",
    "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
    "    {cond_res_list}\n",
    "    -----------------------------------------------------\n",
    "    \"\"\"\n",
    "    with open(generation_file_path, \"a\") as file:\n",
    "        file.write(generation_text)\n",
    "    print(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5840a0c-62b9-4011-a370-8cdf1ba4be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/50000 | Running Avg Train Loss: 6.24023 | Grad Norm: 20.527 | Running Avg Tokens/Sec: 2993.337\n",
      "Step 50/50000 | Running Avg Train Loss: 4.21355 | Grad Norm: 1.394 | Running Avg Tokens/Sec: 6741.455\n",
      "Step 100/50000 | Running Avg Train Loss: 3.77272 | Grad Norm: 2.327 | Running Avg Tokens/Sec: 6705.369\n",
      "Est. Val Loss: 3.10417\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 100\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "  ae  t ae e ha t ses sthist sh toue hee th   h at s sh hes ho s he th a  as hi thon enes t t t ha ate h  seat s teatothe  tha hh ta h h s\n",
      "\n",
      "e shes harot  a he t he s hhh  aresh  thoshe  sh s\n",
      "h ho t hhhe hhea h sh s h s hhhe hoth  he he\n",
      "\n",
      " s\n",
      "hah ha hos s ahh he s\n",
      "heh  hhho hahe he  thha t he at   toth tha a hothhar se  h h h s a a th ho h t hh h sh she anhan h s tha  t  the he han\n",
      "\n",
      "he h s h har a soh ha t ah ha   a ha th tos shor h arhe thohh  tho arorarohe\n",
      "\n",
      "\n",
      " hos\n",
      "\n",
      "\n",
      "ho  thar h at\n",
      "he thos hhe t\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "e t  othe  e  torolos th the she t th s e t t th  athahe t t l mos eehers es th l lihe seshes h t e sh as seseas thas a seh th s t he e\n",
      " es h thhe is sh soth ihe ha hh h shhan theoo   has a s t s s he athae t s thos th h h t e thar h h at the  he sou s  h aro he  e hat th he s   th i anehehe e thon she a s the he th eo t houe\n",
      "athaeo h t t  thor  he hane the\n",
      " os he  thashohor hous\n",
      " th eohar   hath  the a sh t h aroh se s i he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "an hhh he\n",
      "hahhe he s h th heothe\n",
      "on s s  athe ah  e h shathe  he ho\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou artte s h t s at t s hhheh thho h se s a h  t th t th w te he e t ha th h as es h ssss haehaeass th  shathhe a hhh  a a hes hanhe haheheahohh  tho hearan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "he t he\n",
      " ahhh h t  s hehha teho t\n",
      "hohe h ha thot t h th a horhan   s a hath  hah a h thehoh hha h   the h h thhe the sh athe s\n",
      "\n",
      "he a anotheo thohe h the t he   hhhe\n",
      "\n",
      "\n",
      " tare s t\n",
      "ahhha  h hoso s   t  t\n",
      "\n",
      " tot h t  heo he h  h hho  ahe ar a thos\n",
      "\n",
      "athot\n",
      "a hath  he haro th  shheheare\n",
      "hat\n",
      "\n",
      "a thathat  a s thhhat that hho the ho aha he\n",
      "athha t s h h \n",
      "\n",
      "DANIEL: Ay, my dear, te thea ha testhe hahea l t t thas athatou an e har to t s th te   han as s has ee hahes t the h t wis h tha he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " s sh   t\n",
      " s t hahhe ase he  t athes\n",
      " thha ar he a hhothhe t th a hhe\n",
      "  h hon\n",
      "hhe thos\n",
      " t th hehe\n",
      "he s hatharon\n",
      "\n",
      "a  are a hathe  s ara theohhes he t ha athe h ane h t sotot han ha ah  a a  h h a t he he the ath he\n",
      "ath a s th hore s ho t t hha t a t h t h sosho    anha h ath th h s  hh  hose\n",
      "he s\n",
      " s  h se t s     he he  tat ha hos h shh ar ha  hor theathorhan hha s\n",
      " s hothhorane he\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 150/50000 | Running Avg Train Loss: 3.52977 | Grad Norm: 1.253 | Running Avg Tokens/Sec: 6653.827\n",
      "Step 200/50000 | Running Avg Train Loss: 3.35472 | Grad Norm: 1.103 | Running Avg Tokens/Sec: 6666.295\n",
      "Est. Val Loss: 2.65699\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 200\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "U on s sor asalas s lang s ther thal less m mas l se se bes me s senthe mis alil son tals t t t ar porer moungal te llase bone le male asesat seraser ben alle tee s ser s aleso sout and s s an bo athingathen herast an s men mor seneneeer saran an st mere t s as tes totho thit ter therer me as be men thand\n",
      "\n",
      "\n",
      "Woutit aren s,\n",
      "As as an s me me he mend se mangan thal ble t toreend bandise men menorind an sase me th minorane ato as s andoraleesoun mano mend,\n",
      "\n",
      "\n",
      "\n",
      "Wereronges t ses s s atothales, the toron\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "OI\n",
      "UONe thas me tolos the thesse mes t t mes t me athathare so mes merend the p lend mesesthe me and hanthe se s thas an se there me lenges me so s s sere thene me asotherothe theso t bon an me s s ses the se manerathe the t mese s hen ane thande sound mang t se an s m thene man therenen ben me m m s t asengese beng me t me mend asor t s t bothaseng bereng angounen thase her hen m than har m t the mare mes t llaron se s beng mathand has merer sere son me mengond t s s thane an as hen athe me the\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou arton souse s at t s soren le mas se ser as mesast thimore an ple manthe bes pal thes thand an then the ar s anor t alarond hange alllere as seser tharangare mate t ber mind ale se se sangeronesange mithengot t t t sathorerer mo areee anore asotill thillan b mound,\n",
      "H t an ble sas the se be angouthen thone alere t sto be he atheerend thas mas s beno s mat lero bll mot anes se s ale til me sal an sonothousalalasthingere the th s heren as menge tereest th sathangores t sthesar so atisee athan t s more\n",
      "\n",
      "DANIEL: Ay, my dear, te ther byand te s an at t thon thalasalan an man anto m ales te t man mouseend al banes theren tit mat sand se at an b the s me t asoun ase hee tilares sthanger he s s s hen m male hee t be s s herenono t thas hiserer sasengongan sorisallle t balest ther hes sene harases s mat aland toun te tharo t as me man t be me tit athenengothale thee se send t t beneril tindo alo s se b ang hand th the st hig mousater s m meraler s me malend me t at an borine allerallorithen honoron mator s bland\n",
      "\n",
      "Wher a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 250/50000 | Running Avg Train Loss: 3.22721 | Grad Norm: 1.102 | Running Avg Tokens/Sec: 6621.057\n",
      "Step 300/50000 | Running Avg Train Loss: 3.13205 | Grad Norm: 1.051 | Running Avg Tokens/Sec: 6625.210\n",
      "Est. Val Loss: 2.55629\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 300\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "ETon s mor asong thes sthisthit that mes th man at s be bes me s se thean an mil sou tile t thirer ate thisengav teallale bour athes m asorat shes harote alll te s se han alle m thothe s hal mino athingathil herast an s man mot send heer sthan an mind alonger ste t theng hine ter therer me as anderal angand\n",
      "Whe m seren h thas s he shour t he mendise hange ang sthar t areeend handise men henorind an s me me hatigo mane ato ashorendoraleesoun mano mend,\n",
      "\n",
      "\n",
      "\n",
      "Whishoreee t mendothilende thesthil t th \n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ORUSONThe, he tono t athe there be theso wes thes athalest t he hin there the the thand thallat t f s an s arenenghal an se at s borerer he me so s s sere thenghar anotherothe theso t bon an mees s hes the atharonoure the s al thare the t the the sound mang t se an hat thenges me s aron here me mang t se serese beng me t me menoure bat st manore hen berengou boun t thango so hen mere thes al thend more mes t alero ber s benge sthand haller me s he all me hengond t s s thand al al henghare me the\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou arthe souse s atit s sheend be th se ser m torer t thimore anor t manghe bes mal thes thand and, th wiste to anor t alarond hango allleralou heshishe athenge\n",
      "\n",
      "\n",
      "Whit her more ale as se soune, anghighig mas tot t he t athorerer mo arenot he he ang he thillanou mound,\n",
      "Hathat the s s the soure, thisouso thone hithe t ato ath marenderend thasher alle me s mat hero tot mot anoushe s ale hil me hal an songesthouralasthinge meralor t mererares menge thathat han mer hat that hes thindo atisendou male s mor \n",
      "\n",
      "DANIEL: Ay, my dear, te ther balere tes anom her moutho alile t an manginto mesthileng f an mousteno al banes therenorit mat sthanee at an t he there t asoun asee th tilares sthinger heer heril al thale hee t this sheerenono t that hind ar hasengongang ale allle t as ast ther hes mere harases s mat athesotoun thond as thor me han t he me tit athenengothale the\n",
      "\n",
      "\n",
      "\n",
      "Thendig tothane arene, halo s he,\n",
      "Wang hand th the s he h mouthind s m merale tino s al he me t at has has hang he tourithen honoro hou se s hothe maner a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 350/50000 | Running Avg Train Loss: 3.05555 | Grad Norm: 1.041 | Running Avg Tokens/Sec: 6595.583\n",
      "Step 400/50000 | Running Avg Train Loss: 2.99273 | Grad Norm: 0.981 | Running Avg Tokens/Sec: 6591.276\n",
      "Est. Val Loss: 2.48151\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 400\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "Ano thass, aseng the ang whelllll hateses wingh whes whe shath s hen hean an fowound tale atorirar atear mous.\n",
      "\n",
      "Worallase wour atheale as ste sheasear te alll tee s shot aree t athothe s halll ho athingathe, herator, s hino mor senene he seran arert my araser steat,\n",
      "Then hile ter therer merat anderal angr hitheall ar m h tharas he ande her h mendishe angan hes thar therenghe hand\n",
      "We men henorind an s, ther hatigo mare ato as arendind thar, teano areas the tishore s tileralle\n",
      "\n",
      "\n",
      "\n",
      "Wh ther her toro \n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "HIUS:\n",
      "Whe, he toro thath menoust math so wesours mathe here so me sond me me me sendond fore at to the atheare sthathar are at s w athee we athat s s seralourer me another the thero thenoran me s s he woneeathar s s theree the thare theat the the sound manghe\n",
      "Thare hat then mer thaleron here me mangot se s the heren me woroun athath m t thathere hen angheren s hathathanghend hen m whanous ar w the mareal he\n",
      "\n",
      "Tharo mer s ar therond ther wor me were all me hengondond ar thand al al henghe he he he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou arthe wouse what t senorend whind we sthin torth te his te wo mat manthe beralll these hand and,\n",
      "Th wis, whe anor t alarond hango alllerer h heshisheart marden t at her more ale as seand meroungange mithengot t he\n",
      "Thathar an s s areath he heasourer thillan h mile myo that the she ande atheal angous, thone his,\n",
      "Whast we h athenderend thas my alle meas mat hero tor mothenoushe s, he hilono hal an s merothouralas\n",
      "Wh her arou th shereraren menge, ar hat han mer hat that hen thindo atis,\n",
      "\n",
      "Wh male s my t\n",
      "\n",
      "DANIEL: Ay, my dear, we ther warersther anom her moutho while.\n",
      "\n",
      "ATher ar to meale meng f an an stean al thees therenor, wis ather se at ano she there t asoun atheate t marend thal me heer heril al thale he\n",
      "\n",
      "Thishend herear, at that his mer hasengongan s, t allle t are malllloures, ane harases h ane athe atoun te tharo thor me han t he me titeathealart,\n",
      "The thee at sero titothane arene, halo she t arear horer thar st har maseater s,\n",
      "Theraler s m s alend meat at has h she arend toulleard\n",
      "Tond t houto thinother, ser a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 450/50000 | Running Avg Train Loss: 2.93710 | Grad Norm: 1.042 | Running Avg Tokens/Sec: 6580.950\n",
      "Step 500/50000 | Running Avg Train Loss: 2.88767 | Grad Norm: 0.937 | Running Avg Tokens/Sec: 6587.414\n",
      "Est. Val Loss: 2.39974\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 500\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "Ano thass, aseng the anckist shar at mes thangh whes wit shat ander thean fath\n",
      "ALUCONG:\n",
      "TICECY:\n",
      "TE:\n",
      "\n",
      "CELI IO:\n",
      "TE:\n",
      "WARC:\n",
      "CIUS:\n",
      "W:\n",
      "TIOLIC:\n",
      "WECICICING VICET:\n",
      "NT:\n",
      "MONI mee, sare, wo thothe silll my meat theat sath shyor, sin,\n",
      "\n",
      "\n",
      "\n",
      "Me sealle he sar hil arend ald sis steer, alll hale her thorst mear,\n",
      "Theer theand hith tit aren ar as as he ande here ar sas heat, watht wis than theend hand.\n",
      "\n",
      "\n",
      "CORCKONTOMIOM:\n",
      "\n",
      "MANICONG:\n",
      "T:\n",
      "\n",
      "OUKENINORCETEOM:\n",
      "M:\n",
      "WINGOMOLIN:\n",
      "\n",
      "NOLO:\n",
      "\n",
      "\n",
      "\n",
      "MOO:\n",
      "WER:\n",
      "WENO:\n",
      "WI:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NT:\n",
      "WENG:\n",
      "TINor I\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "HIUS:\n",
      "Wher he tor so ath fore he mat sore far wis athe har the mean shers fof the sarerse beang the so at sereat thal an shat serar,\n",
      "The wheat she mand malll are athen har that ther theng an the send\n",
      "Whal mat maler the the the thar heno the thar, sound mang the the hat then mear thear thar,\n",
      "\n",
      "That an thend where woun me wo win ath sar ther ather and sang than sar that se o so hen math thear wand\n",
      "Thand man she and whand sher thang the hall thar were all me hend shand ar thand al al hangh the se he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, sor, me athit senore, that sate ser hat stave this to he ald\n",
      "Wanthe berars horsth band and, thy shath ser ard sall heare and,\n",
      "Thersthou heen, hear hange mat at her mord are as seand meroung she mitheng sat hen sath myong ho areng alo hal mare,\n",
      "\n",
      "Willl har ale mit that the sas and satheal aloth anghen ardere that hero athend,\n",
      "Thald asher alle so so at hero tor mat anearse he he hil mee art sis meastheale asth her hard tha sharer,\n",
      "\n",
      "\n",
      "And san arend thand thang, that han thind athis, alald the ar he\n",
      "\n",
      "DANIEL: Ay, my dear, we ther hare, ther an at he tont hay.\n",
      "\n",
      "Fou an mang, to mand wheng fran an stean al bu that when wis wat sund seat an ard wo alld thas,\n",
      "Thyot hee wilares so harer hear heril at that he the this sheere tha t that his mer are ard\n",
      "\n",
      "\n",
      "Whoulld, hat theal, then, thean ang hardsen hard theand,\n",
      "\n",
      "\n",
      "ANGNTECENGTEONIT:\n",
      "TETICOLONG:\n",
      "\n",
      "CINCIONO:\n",
      "\n",
      "TOMIONING:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TE:\n",
      "C:\n",
      "MENGONGT:\n",
      "\n",
      "\n",
      "MO:\n",
      "NINICET:\n",
      "TETO:\n",
      "\n",
      "\n",
      "WENOMI VE:\n",
      "WICI:\n",
      "\n",
      "MOLER:\n",
      "\n",
      "NERO:\n",
      "MIONG:\n",
      "Tom VICI V:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wayo thar, sal, mallllly yoth mean my ald sint ard, ser a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 550/50000 | Running Avg Train Loss: 2.84237 | Grad Norm: 0.972 | Running Avg Tokens/Sec: 6598.996\n",
      "Step 600/50000 | Running Avg Train Loss: 2.80137 | Grad Norm: 0.814 | Running Avg Tokens/Sec: 6611.828\n",
      "Est. Val Loss: 2.35347\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 600\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "EThe sass, aseng the ant,\n",
      "Buthe that mees a to som sast bes, the henthea ay alisth songe\n",
      "That trave thar moung thord byo so mand thale a stat sis sear\n",
      "We alll the shis, sarend, wou sand shy anth, a ar hath and shyor, singhe mor sen\n",
      "Theer sar hilardeand arand at that beng hale wer the\n",
      "Thard at a tor thancand thatit aren ar as as hee has her her fas heat,\n",
      "\n",
      "That wis tha aren, ar and seen soungald, sou sa the that ar mare\n",
      "Not andir hasengeat an mand areas thear shore shalld.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NOMENUS:\n",
      "\n",
      "CINUKIN\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ORUS:\n",
      "Wherang ton so ath fore fo mat sare fard so athe har the meand.\n",
      "\n",
      "\n",
      "CENOMELEN:\n",
      "By wand meat the shand share shat ar my sat sot athe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CENGCECONCICETET:\n",
      "\n",
      "Thar and hard and, sor thall an my sot sot thal thard,\n",
      "\n",
      "Whe the the thar hen athe thar by wis mang the the hat then my my sard thar the thand theath the here\n",
      "The the an ath bar the an for and sand thal bard theand hand hand\n",
      "The thear al brer the alllll all all thand ale sond theall thar sere\n",
      "Thar shard sor that shand hal al hangh the so he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, sor, me ath thathe and brind weanck hat thave this to hyor,\n",
      "\n",
      "Mand by ar bal therath mar athe shes mards angh toull thy hang, allly\n",
      "And haten, hear hange wat athigh atht a by bereand tar angange him sat sat hen sath my\n",
      "The so heat alo hyo my heen thin har mand,\n",
      "Thill thar sas the satheang atous, thy therere that see hearend,\n",
      "Tha thar har and so sit that sand shit and she save hate mas alld thon\n",
      "Thous mallllin har aral this herear shaldse thaten thand thang,\n",
      "That han thind athis, ath bal hang he\n",
      "\n",
      "DANIEL: Ay, my dear, we ther by, wous soun a her ton wo athat.\n",
      "\n",
      "\n",
      "LARDTES:\n",
      "LOMELE:\n",
      "ME:\n",
      "BENO, my sand mis thave theren at, wit sund.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CINGUS:\n",
      "\n",
      "TERO:\n",
      "MENUSTICO:\n",
      "TARENONGOMER:\n",
      "CICO:\n",
      "NGBENG VI ENGONTET:\n",
      "Tor, sor, myot sa t that and, ar sateng my\n",
      "\n",
      "Thalldalll at bavert thard, a sear haveden ateng a theate sofe,\n",
      "Tave tho son han thot me tit ath andat thee thee\n",
      "Wous to tit thand arend, hatiler thy ang,\n",
      "Thath thar singh mend, thard, willle wou me wal hee\n",
      "\n",
      "No at hat har sang halllllleard hors to hath thingh his ser a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 650/50000 | Running Avg Train Loss: 2.76287 | Grad Norm: 0.924 | Running Avg Tokens/Sec: 6610.303\n",
      "Step 700/50000 | Running Avg Train Loss: 2.72739 | Grad Norm: 0.851 | Running Avg Tokens/Sec: 6619.495\n",
      "Est. Val Loss: 2.30874\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 700\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "\n",
      "The shes, aseng the ant shalst bore ases o to som sa shing,\n",
      "A dis thing maved wou songes at,\n",
      "Ar wo thar moung thordisthe buncert male a sore\n",
      "Whandear fe alll thee sis, sare so sou sand shy an ho a ar hath a heraver,\n",
      "With ande a theneent sar hilave and ath so seeat, beng thit tat the\n",
      "Whard at benerallllar bot tour aren a tha as ath sou her tor sus seaver\n",
      "That sis tra toreene hand seeere wing thange sa ther ta min manch to ancer\n",
      "Whard thard tou sear as thear shore shave salld\n",
      "Tor haldsprall to sh\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "MINIO:\n",
      "\n",
      "MELENTET:\n",
      "My you ment he mat tore thang buther hare.\n",
      "\n",
      "\n",
      "MELINT:\n",
      "Bus mat and mand you sat the shant shat sthat bea shat shat athe\n",
      "The athe sheat shalll are atheat sou and ther the\n",
      "\n",
      "Thand theand sat athe math sou thar the thar hen athe thare sof sher,\n",
      "Thar fou hat ther my ath ard thar ther ath she athe\n",
      "The brer fo the an ath ber ther ath thand therser and ance thange\n",
      "The tha the thear as bre thar athe the ath bur sher.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CENUKENTETET:\n",
      "There ast my my there so so and, a so shangh the so he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, be ank bak bund,\n",
      "Then geat hath heath tort, thill fon hyor thith.\n",
      "\n",
      "\n",
      "LOLETEN:\n",
      "Nomy band my sil this, bys a brat all thy a and,\n",
      "Thersthoulle,\n",
      "As, arand, wen that thatight a by bereand tar angange hithe\n",
      "Wot thand sath my ar to a mat a ath athtill thilll\n",
      "Whe the sit that bre sar thillllow andisth anthe therere that\n",
      "Thith thenderens thather fror so sim to tha bat then tous alld by tho me halt\n",
      "A so santheale ath so this alor trithtear shald\n",
      "Thond bat thanger hat that tha thinghath, beath mal hang he\n",
      "\n",
      "DANIEL: Ay, my dear, we ther by, meeand an a her you,\n",
      "And have and mand ath toul ar mu ar bars sangore\n",
      "And that tren at brave tha sea ton a she thars tha ath atheate tilthe\n",
      "Tha harer hear hyow hat that thee tor hen thereat,\n",
      "Whilll fris mer are ard trat alis and at bale\n",
      "Then athea bear havesens ting a theate shal thars tho son to ale thee\n",
      "Tit ath andath bur thee toure ath to hand arene\n",
      "Thatill hen asthard ath thar singh shaveater sof my\n",
      "Ther so a my and brat atand, athard ther toulle\n",
      "Thathand ber as thinth tor ser a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 750/50000 | Running Avg Train Loss: 2.69359 | Grad Norm: 0.939 | Running Avg Tokens/Sec: 6597.856\n",
      "Step 800/50000 | Running Avg Train Loss: 2.66174 | Grad Norm: 0.994 | Running Avg Tokens/Sec: 6589.954\n",
      "Est. Val Loss: 2.28507\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 800\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "\n",
      "The shes, aseng the ant the so to a dees a tus and\n",
      "a henes, the he thean fat a thouseals tor\n",
      "Ar wo thar moung thord byse buncer thale a sore supreer ate ath theee\n",
      "With sithe to tho and shy an bof thire hall that tris sencke\n",
      "And wheneeer sur supave a frart so sofer, ben a tit ber the\n",
      "Whard at benerallllar be thall an ber thar sunce sof the to tous seavest and\n",
      "Nrear thereee sof sof shere hing thange sa ther ta bur barth to as shat ard\n",
      "And an fand areave art a to beere sis ofere\n",
      "The theath to sor \n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ORUS:\n",
      "The and to non and the and wit to she an me pof hear the for ber he fald\n",
      "ald pand and she the so ath the sore sur ben at so theave.\n",
      "\n",
      "\n",
      "CENTING ERD:\n",
      "Bot ther an sou sof and sis so bear mard sis seald a sthard\n",
      "There the the thar ath an theat sis ber frear ance shalll\n",
      "Whe her thalll thar bre thalll that there wour be wo the\n",
      "Thar my ther ath thar ath thar a bean theare a sther seath the\n",
      "Whalll shave there the and ber sher that pand thalll thave\n",
      "That ith bre so me so shaver an a shat athee surs\n",
      "\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, be an to at thathe and by ave henck\n",
      "An met at thangte he be asthave ber bave feprthand and,\n",
      "Thy shath sis brat a a thareaver ar thealld seper\n",
      "Thart andeer to a bre anth a bund tous aner and sunde theat thild thy\n",
      "Thath ar to a mat ancer as trer of meave thous sit that bre\n",
      "War thilll beallllot sinth seavit sut and ath at\n",
      "Thave thith shill and so sim to tro balll ber\n",
      "Norse shee thil bee antis sof too buteat thinger,\n",
      "Bust thereerear shald an bateat than that trealt thavtie\n",
      "Whath beerath athe and t\n",
      "\n",
      "DANIEL: Ay, my dear, we ther by, put as an a the ton to athe.\n",
      "\n",
      "\n",
      "MELIUS:\n",
      "And gor thy mu bran foustean be bupes therer at brave and.\n",
      "\n",
      "\n",
      "Than sish theard thard sist a that mithe sur, and a ar bere\n",
      "Nat that a sepe bus asthereat a tilll bris ferteate as\n",
      "Trat and a theat brarst that thal ane ta asencereat a theate\n",
      "Note thars tho son to a the thatir ath andate bur the\n",
      "Whe sendit to sane a tinge andear thy and,\n",
      "Whith thar singh meneeat myst therave tin a bereat brat ato a both bun\n",
      "That as there ton an shat athintand sand to\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 850/50000 | Running Avg Train Loss: 2.63138 | Grad Norm: 1.045 | Running Avg Tokens/Sec: 6602.758\n",
      "Step 900/50000 | Running Avg Train Loss: 2.60209 | Grad Norm: 1.029 | Running Avg Tokens/Sec: 6581.185\n",
      "Est. Val Loss: 2.28202\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 900\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "Mare shes, ash at be ant thel the be ase the whallds wit.\n",
      "PAUSENENTE:\n",
      "Man yham wou bunges bere and and thisen.\n",
      "\n",
      "MELY Myou bomand thale bealte sith tar tene thiteee sench sure.\n",
      "\n",
      "NORKINCING:\n",
      "\n",
      "\n",
      "No, sethis me shat shy shasing and, be as ther sur suprereand\n",
      "\n",
      "Nowis speer, ber a tit ito ther to supp be wof thave\n",
      "Wit to ther ber thas spee ande the to surs sencer\n",
      "That sis tro to my sexand seeere winel a spatit ther ta bur be that are\n",
      "That arave a an dand areaves ange to bee to bralld\n",
      "To thalll thil to se\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "Met so he bus to of sull for ast make.\n",
      "\n",
      "\n",
      "\n",
      "KIN:\n",
      "Whathe bear the mest the he she alll and angllles a to ant there\n",
      "that a mere and an and ther and sof she there and a and ther\n",
      "Wit ther there a seee sen beat a sprat so there with beeer\n",
      "That ith there sof ren in the the surs beree a dor and\n",
      "The the thalll thave pries ber be wit preat a winge that thald\n",
      "What so bre sun the sof and that the the the with low the so depere\n",
      "That the that pre and with beare wis bre prith preall shaves prises sen\n",
      "That surs \n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, be a me ast thathated that hate cre cre tor to ang\n",
      "To be be ast bust ancear therst and and the beate\n",
      "To a that a a thaveaver ar theall brees, thart and\n",
      "To thit tre anth a bund seas aner andave sepeas to apld thy\n",
      "That and to a beat bear and the of meave thound, wat an bre\n",
      "Whe the sof ta thisth in that asere to to a my to there\n",
      "Theallle wille so so an ance balll ber bover thee\n",
      "Noul bee antis so beat butalll brure with thilld arer as and\n",
      "Fit ande thas applow, that the to as a tisee\n",
      "What the fish \n",
      "\n",
      "DANIEL: Ay, my dear, we the shald, the wing by and,\n",
      "Nou wis con an mand sto to therem to f mof seeave.\n",
      "\n",
      "\n",
      "MARDWAUS:\n",
      "Ond, bre a that, a to an she thers thas, sist thee willt she\n",
      "Thater seer by by a thille fre to sur sinere thath that.\n",
      "\n",
      "\n",
      "SININ:\n",
      "\n",
      "No, my, so mis and an brord and, thealdere theate site.\n",
      "\n",
      "\n",
      "NUK:\n",
      "Got the thar, tho so sis a bot thatir ato and.-\n",
      "\n",
      "\n",
      "\n",
      "NoNTIV:\n",
      "\n",
      "Teen, tit that an tin, shat she thave a dorer thee singh menee\n",
      "Wer sof my ant tin a ber ance a sen wo a that and.\n",
      "\n",
      "\n",
      "NUS:\n",
      "That, and, an as thin and sheer, \n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 950/50000 | Running Avg Train Loss: 2.57421 | Grad Norm: 1.167 | Running Avg Tokens/Sec: 6590.202\n",
      "Step 1000/50000 | Running Avg Train Loss: 2.54698 | Grad Norm: 1.286 | Running Avg Tokens/Sec: 6608.480\n",
      "Est. Val Loss: 2.28857\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 1000\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "\n",
      "The shes, ash gons my this the fore lees a to som sust\n",
      "And thends thied ifhald thou to britir.\n",
      "\n",
      "SIO:\n",
      "\n",
      "Cace and, thalllle bomand thalll so an supleer ben a priteee\n",
      "Witht ithe wo a that ish son bore ande this thatis a sing.\n",
      "\n",
      "\n",
      "CUK:\n",
      "\n",
      "Tit no shith andeen witil so sthe with sheall wer thitht meating ter\n",
      "Now ar to my lis the a thas sine ande a thie.\n",
      "GUKENT:\n",
      "\n",
      "Do, my sis my soree, so and sise bow mald, so hig ther ta bur mart\n",
      "Not, shis thar betis my to bureave arer those she\n",
      "The sis a beaversst susen se\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "OMENTIO:\n",
      "Whe to now, and with that the the so gong.\n",
      "\n",
      "\n",
      "MENTERE:\n",
      "The be, me the the thand the she the so bestereat the cre cald\n",
      "Whe the sis that she as sen and and a and to so beseese\n",
      "The she she wis sunce and to the the the the thave the the there spre\n",
      "What thave and an the sis suple wene with an the thee\n",
      "That she that the wou and be and that the spreavesere\n",
      "Thave the she and that the the the with thee andere andees.\n",
      "\n",
      "\n",
      "DUK:\n",
      "No, the the sis so sis she my ling the and in thatere\n",
      "Nom my and ithe the\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, be a me ast thathaten geate, wever,\n",
      "An my a to bus to byou thith.\n",
      "\n",
      "SIO:\n",
      "The the my to reat the comy gos a bre will thy a and,\n",
      "The senou heence that my wen to a ser with a this seas aner and sunde the\n",
      "Wo my asperat and a so an mat incer and the thille\n",
      "Whe in wing that bre she thies, that and win thy lis weer.\n",
      "\n",
      "\n",
      "DUKIN:\n",
      "\n",
      "\n",
      "NG INT:\n",
      "Nomes my and, so mit to tho malll be to she shee thil me sinte the\n",
      "Os theate astis tror to thilld arear whaldsere ande thas and\n",
      "Thart ato havere wo a ande and a thatish \n",
      "\n",
      "DANIEL: Ay, my dear, we the shat meeat my and to mon.\n",
      "\n",
      "DUTENVESBET:\n",
      "Nomant, meallll I'd that my thave beatht at ther and mat sunchee\n",
      "That sist to sprit as thisteere thillles surt and a ar brith a mithteer.\n",
      "\n",
      "\n",
      "KENCIN:\n",
      "Ne:\n",
      "To the the whis me thast as tris and a the there spather tea bear thease\n",
      "Witene a theate sof mealll will be in a bove breare to and\n",
      "With that tore senceatis sher a tinge withere thave\n",
      "That thit me sine wo aneater sof my aner brove an ance\n",
      "Whit and botins and a to andere toner thil sepene\n",
      "The inde th\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 1050/50000 | Running Avg Train Loss: 2.52120 | Grad Norm: 1.271 | Running Avg Tokens/Sec: 6611.185\n",
      "Step 1100/50000 | Running Avg Train Loss: 2.49627 | Grad Norm: 1.429 | Running Avg Tokens/Sec: 6619.149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Driver code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device, train_loss_list, val_loss_list, train_time_list)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# estimate val loss, generate text and save\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m EVAL_ITERS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     generate(model, generation_file_path, empty_tokens, cond_token_list, step)\n\u001b[1;32m     55\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, val_losses)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EVAL_ITER_COUNT):\n\u001b[1;32m      6\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m calc_loss(logits, targets)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      9\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/self-learn/moe-kit/vanilla_transformer/../models/transformer.py:167\u001b[0m, in \u001b[0;36mVanillaTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# (B, S, n_embd)\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 167\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, n_embd)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# negate mask to fill originally False values with -inf later\u001b[39;00m\n\u001b[1;32m    169\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, S, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/self-learn/moe-kit/vanilla_transformer/../models/transformer.py:85\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# residual connection (stream)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# pre layer norm\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x)))\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/self-learn/moe-kit/vanilla_transformer/../models/transformer.py:63\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# x: (B, S, n_embd)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Step 1 and 2: Project full query, key, value, then split via reshaping\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x))\n\u001b[0;32m---> 63\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x))\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Step 3: Compute scaled dot-product attention with causal mask\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95154-d641-4d9b-9e8a-c054d5921921",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ee940-9794-467a-8088-2b59ac5aaeae",
   "metadata": {},
   "source": [
    "*After 2250 steps *  16 batch_size, training loss 1.8277:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67bda2ae-5fa0-4120-9b55-41121d9ca152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AUCENTIO:\n",
      "Which the may sich that nough to the slay'd one so to;\n",
      "His be knot, I Vistrengs, thy the good our kim in to call:\n",
      "No, thou man I good, Say, for pmburds tell eack.\n",
      "\n",
      "HESSend.\n",
      "\n",
      "MENCIO:\n",
      "Dever and will'd my Vaing, life to suke in lise,\n",
      "These'll was of yret, fol smy his no Fear shom gestard:\n",
      "Retil appoutis commentex e'epon tend her his him buse,\n",
      "And what ityer am the iends, come; God foll ding:\n",
      "by appeerk.\n",
      "\n",
      "LOUCIO:\n",
      "Petwild, bake you, that I same, what wear;\n",
      "from in in or my speak as For Jul\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "772a7473-2e8f-4c61-87d9-612e01150b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRY: thou arte a my she\n",
      "Which have may and of contain.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good as I tall no knrow, for shalt agarnt\n",
      "And mpo; and Kong a m, not outhpile Mesce.\n",
      "\n",
      "HENRY VI:\n",
      "When I will thy lookess, oner the pexstrey\n",
      "The the the hee voagh gresed livioe.\n",
      "\n",
      "MENCIO:\n",
      "My her callis his peaced of to that\n",
      "We where's by shall bore: as shall myselvea\n",
      "The plender feuls!\n",
      "\n",
      "PAPELLANT:\n",
      "In the the into balby me dods to love,\n",
      "In but the giving of nyou ase. I tall it-me?'e Goveuling\n",
      "The theer haught art praver count madeng Camen:\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"TERRY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
