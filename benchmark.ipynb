{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853fb4d7-ade1-4952-929b-582e6b26bff8",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd4a31-9a93-44e1-81a6-795469abddb0",
   "metadata": {},
   "source": [
    "This notebook provides official time and memory benchmarking experiments on Switch vs. vanilla decoder transformers and various architectural and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6e43b6-b751-41b3-b98c-ba972b0f3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=(FutureWarning, UserWarning))\n",
    "\n",
    "# !pip -q install einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5289c7-9233-4f31-b12b-7dc62b8466f7",
   "metadata": {},
   "source": [
    "# Models and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2869574a-762a-427a-9496-bd74abb4f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = path\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "BENCH_ITERS = 100  # max num batches to benchmark\n",
    "\n",
    "#### ^ 2000 later\n",
    "\n",
    "PRINT_ITERS = 100  # frequency to print avg train loss\n",
    "EVAL_ITERS = 250  # frequency to evaluate val loss\n",
    "EVAL_ITER_COUNT = 50  # number of batches to estimate val loss with\n",
    "# given a 10% val split, we have 111540 char, so 50 batches * batch size 16 * if seq len 128\n",
    "# = roughly equal to all chars chosen\n",
    "\n",
    "# automatic mixed precision (will be disabled if CPU, not available)\n",
    "USE_AMP = True\n",
    "\n",
    "### will not benchmark/modify: switch transformer auxiliary loss coefficient\n",
    "AUX_LOSS_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b4831b-29d4-4c4d-a5df-24bbdd6386d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "root = root if drive is None else \"/content/drive/MyDrive/self-learn/moe-kit\"\n",
    "path = root\n",
    "\n",
    "# cannot train in mixed precision on CPU (GradScaler needs cuda)\n",
    "USE_AMP = USE_AMP if device.type == \"cuda\" else False\n",
    "# Tesla T4 does not support bfloat16, CPU does not support float16\n",
    "AMP_DTYPE = torch.float16 if device.type == \"cuda\" else torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458c235e-09ee-4d47-b353-04940e670ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed\n",
    "from models.transformer import MLP, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77e2ad0-010e-49cd-9a07-6c95a33d072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9f761e-fd58-4405-a086-f6139d6fc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split, seq_len):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - seq_len, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + seq_len] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + seq_len + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be310bd-c56c-4ed3-b68b-39e658354771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ce_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Computes cross-entropy loss.\n",
    "    Inputs:\n",
    "        -logits: Model output of shape (B, S, vocab_size)\n",
    "        -counts:\n",
    "    \"\"\"\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0c7654-ba15-4361-9ec2-4ea836dba142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aux_loss(counts, prob_sum, n_experts):\n",
    "    \"\"\"\n",
    "    Computes Switch Transformer auxiliary loss.\n",
    "    Inputs:\n",
    "        -counts: Number of tokens passed to each expert in each switch layer (num_switch_layers x n_experts)\n",
    "        Note this is NOT equivalent to n_layer; num_switch_layers depends on `switch_first` and `every_n_switch`\n",
    "        -prob_sum: Sum of probs across all tokens for each expert (num_switch_layers x n_experts)\n",
    "    \"\"\"\n",
    "\n",
    "    # total number of tokens routed in that layer\n",
    "    token_count = counts.sum(dim=-1, keepdims=True)\n",
    "\n",
    "    # prop of tokens dispatched to each expert\n",
    "    route_frac = counts / token_count\n",
    "\n",
    "    # fraction of total probability allocated for each expert\n",
    "    # recall prob_sum := softmaxed probs, which added to 1 across the experts for each token\n",
    "    # we divide by num_tokens so that the overall 2D scalar sum of prob_frac is 1\n",
    "    # intuitively we are forcing the total prob for each layer across the experts to be 1 so we can take proportions,\n",
    "    # the same way as above\n",
    "    prob_frac = prob_sum / token_count\n",
    "\n",
    "    # Auxiliary loss\n",
    "    # L = N \\sum_{i=1}^N f_i • P_i\n",
    "    aux_loss = n_experts * (route_frac * prob_frac).sum()\n",
    "    return aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a43788-4b83-42ff-9e39-d44ef283e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_losses, val_aux_losses, device):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    aux_losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch(\"test\", model.seq_length)\n",
    "        with torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP):\n",
    "            if model.switch:\n",
    "                logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "                aux_losses[k] = calc_aux_loss(counts, prob_sum, model.n_experts)\n",
    "                losses[k] += AUX_LOSS_COEF * aux_losses[k]\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "    val_loss, val_aux_loss = losses.mean().item(), aux_losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    val_aux_losses.append(val_aux_loss)  # track separate aux loss for logging\n",
    "    # keep model in eval\n",
    "    print(f\"Est. Val Loss: {val_loss:.3f} | Est. Aux Loss: {val_aux_loss:.3f}\")\n",
    "    return val_losses, val_aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc343f2d-a27d-48df-8255-6cf0414cc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, empty_tokens, num_tokens):\n",
    "    generation_text = decode(\n",
    "        model.generate(\n",
    "            empty_tokens, method=\"top-k\", k=5, max_new_tokens=num_tokens, uncond=True\n",
    "        )[0].tolist()\n",
    "    )\n",
    "    return generation_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4b46f-257c-4b84-9d17-cf0d36949a6a",
   "metadata": {},
   "source": [
    "## Benchmarking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ed6d21-2e14-4e3b-b28d-9ea117d9b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    set_seed(SEED)\n",
    "    model = Transformer(\n",
    "        VOCAB_SIZE,\n",
    "        config[\"seq_len\"],\n",
    "        config[\"n_embd\"],\n",
    "        config[\"n_head\"],\n",
    "        config[\"n_embd\"] * config[\"n_ff_ratio\"],\n",
    "        config[\"n_layer\"],\n",
    "        device=device,\n",
    "        n_kv_head=config[\"n_kv_head\"],\n",
    "        norm_first=config[\"norm_first\"],\n",
    "        use_rotary_embd=config[\"use_rotary_embd\"],\n",
    "        softmax_off_by_one=config[\"softmax_off_by_one\"],\n",
    "        switch=config[\"switch\"],\n",
    "        switch_first=config[\"switch_first\"],\n",
    "        every_n_switch=config[\"every_n_switch\"],\n",
    "        capacity_factor=config[\"capacity_factor\"],\n",
    "        drop_tokens=config[\"drop_tokens\"],\n",
    "        n_experts=config[\"n_experts\"],\n",
    "        expert=MLP,\n",
    "        use_amp=USE_AMP,\n",
    "        amp_dtype=AMP_DTYPE,\n",
    "        activation=config[\"activation\"],\n",
    "        mlp_dropout=config[\"mlp_dropout\"],\n",
    "        expert_dropout=config[\"expert_dropout\"],\n",
    "        scale=config[\"scale\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9259123-7388-4dad-9a60-bc4b0ee29650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_train(config, device):\n",
    "\n",
    "    # Initialize model and training setup\n",
    "    model = get_model(config)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # num params\n",
    "    num_params = sum([p.numel() for p in model.parameters()])\n",
    "    if not model.switch:\n",
    "        num_active_params = num_params\n",
    "    else:\n",
    "        switch_additional_params = 0\n",
    "        switch_layer_names = [f\"experts.{i}\" for i in range(1, model.n_experts)] + [\n",
    "            \"switch\"\n",
    "        ]\n",
    "        for (name, layer) in model.named_parameters():\n",
    "            # if experts.i (i > 0) or switch in name, subtract from total count\n",
    "            if any(substr in name for substr in switch_layer_names):\n",
    "                switch_additional_params += layer.numel()\n",
    "        num_active_params = num_params - switch_additional_params\n",
    "\n",
    "    # model size in bytes\n",
    "    model_size = sum(\n",
    "        [\n",
    "            p.numel() * p.dtype.itemsize\n",
    "            for p in itertools.chain(model.parameters(), model.buffers())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # GQA\n",
    "    gqa = model.is_gqa\n",
    "\n",
    "    # Train fn\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_times = []\n",
    "    # below two are only relevant if benchmarking switch\n",
    "    val_aux_losses = []\n",
    "    dropped = []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # text generation setup\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "    for step in range(BENCH_ITERS):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs, targets = get_batch(\"train\", model.seq_length)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP):\n",
    "            if model.switch:\n",
    "                logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                loss = calc_ce_loss(logits, targets)\n",
    "                aux_loss = calc_aux_loss(counts, prob_sum, model.n_experts)\n",
    "                loss += AUX_LOSS_COEF * aux_loss\n",
    "                drop_frac = (\n",
    "                    np.array(n_dropped) / (BATCH_SIZE * model.seq_length)\n",
    "                ).tolist()\n",
    "                dropped.append(drop_frac)\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                loss = calc_ce_loss(logits, targets)\n",
    "\n",
    "        train_time = time.perf_counter() - start\n",
    "        tokens_per_sec = (1 / train_time) * BATCH_SIZE * model.seq_length\n",
    "        train_times.append(tokens_per_sec)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Monitor gradient norm\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP):\n",
    "            grads = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            norm = torch.cat(grads).norm()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # print training statistics\n",
    "        if step % PRINT_ITERS == 0 and step != 0:\n",
    "            print(\n",
    "                f\"Step {step}/{BENCH_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.3f} |\",\n",
    "                f\"Grad Norm: {norm:.2f} | Running Avg Tokens/Sec: {np.mean(train_times):.2f} |\",\n",
    "                f\"Bandwidth: {model_size * np.mean(train_times) / 1e9:.2f} GB/s\",\n",
    "            )\n",
    "\n",
    "        # estimate val loss\n",
    "        if step % EVAL_ITERS == 0 and step != 0:\n",
    "            val_losses, val_aux_losses = estimate_loss(\n",
    "                model, val_losses, val_aux_losses, device\n",
    "            )\n",
    "            model.train()\n",
    "\n",
    "    generated_text = generate(model, empty_tokens, num_tokens=500)\n",
    "\n",
    "    mem_data = {\n",
    "        \"reserved_memory\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"allocated_memory\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return (\n",
    "        train_losses,\n",
    "        val_losses,\n",
    "        train_times,\n",
    "        val_aux_losses,\n",
    "        dropped,\n",
    "        generated_text,\n",
    "        mem_data,\n",
    "        num_params,\n",
    "        num_active_params,\n",
    "        model_size,\n",
    "        gqa,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ec6e3f-35e1-4ccc-ba18-21d13dcb696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bench_inference(config, device, num_tokens=500, gen_count=5):\n",
    "\n",
    "    # Initialize model and training setup\n",
    "    model = get_model(config)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "    gen_times = []\n",
    "\n",
    "    # model generation params, fixed\n",
    "    p_nucleus = None\n",
    "    k = None\n",
    "    if config[\"decode_method\"] == \"nucleus\":\n",
    "        p_nucleus = 0.5\n",
    "    elif config[\"decode_method\"] == \"top-k\":\n",
    "        k = 5\n",
    "\n",
    "    for i in range(gen_count):\n",
    "        start = time.perf_counter()\n",
    "        res = model.generate(\n",
    "            empty_tokens,\n",
    "            max_new_tokens=num_tokens,\n",
    "            method=config[\"decode_method\"],\n",
    "            p_nucleus=p_nucleus,\n",
    "            k=k,\n",
    "        )\n",
    "        gen_times.append(time.perf_counter() - start)\n",
    "\n",
    "    mem_data = {\n",
    "        \"reserved_memory\": torch.cuda.memory_reserved(0) / 1e9,\n",
    "        \"allocated_memory\": torch.cuda.memory_allocated(0) / 1e9,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return gen_times, mem_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dab740-34af-47d6-9d41-db62220a4e52",
   "metadata": {},
   "source": [
    "#### ––––"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543060c4-1115-4eb1-90ec-a965c9ef858d",
   "metadata": {},
   "source": [
    "## Driver code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff191ab-8c4a-4941-8226-e775b0ea6dbe",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "        -Training: architecture vs. samples/s, loss vs. samples seen (sample efficiency)\n",
    "        -Inference: Decoding method vs. tokens/s, architecture vs tokens/s\n",
    "        \n",
    "        -Architectures to-vary: switch (1/2/4/8 experts, capacity) vs. vanilla, hidden_dim/n_layer/n_head, total_params, ROPE vs. cos/sin PE, etc.\n",
    "        -Also, benchmark memory, e.g. for effect of GQA\n",
    "        -Roadmap: Benchmark specific time/memory spent for MLP vs. switch FF, attention vs. MLP proportions within a block, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc55083-d1b0-42b1-800c-a34a97cf44a2",
   "metadata": {},
   "source": [
    "## todo: be more selective w/ architecture params lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "658b8830-0b1f-4bc0-91ac-3fc3a7affa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "metric_keys = [\n",
    "    \"Train Loss\",\n",
    "    \"Val Loss\",\n",
    "    \"Train Tokens/s\",\n",
    "    \"Bandwidth (GB/s)\",\n",
    "    \"Train Memory (GB)\",\n",
    "    \"Inference Tokens/s\",\n",
    "    \"Inference Memory (GB)\",\n",
    "]\n",
    "hp_keys = [\n",
    "    \"seq_len\",\n",
    "    \"n_embd\",\n",
    "    \"n_ff_ratio\",\n",
    "    \"n_head\",\n",
    "    \"n_kv_head\",\n",
    "    \"n_layer\",\n",
    "    \"norm_first\",\n",
    "    \"use_rms_norm\",\n",
    "    \"switch_first\",\n",
    "    \"use_rotary_embd\",\n",
    "    \"softmax_off_by_one\",\n",
    "    \"switch\",\n",
    "    \"every_n_switch\",\n",
    "    \"capacity_factor\",\n",
    "    \"drop_tokens\",\n",
    "    \"n_experts\",\n",
    "    \"activation\",\n",
    "    \"mlp_dropout\",\n",
    "    \"expert_dropout\",\n",
    "    \"scale\",\n",
    "    \"decode_method\",\n",
    "]\n",
    "param_keys = [\"num_params\", \"num_active_params\"]\n",
    "other_keys = [\"gqa\", \"top5-generated_text\"]\n",
    "\n",
    "seq_len = np.array([128])\n",
    "n_embd = np.array([128, 256, 384])\n",
    "n_ff_ratio = np.array([2, 4])\n",
    "n_head = np.array([4, 8])\n",
    "n_kv_head = np.array([1, 2, 4, 8])\n",
    "n_layer = np.array([4, 6])\n",
    "norm_first = np.array([True])\n",
    "use_rms_norm = np.array([True, False])\n",
    "switch_first = np.array([True])\n",
    "use_rotary_embd = np.array([True, False])\n",
    "softmax_off_by_one = np.array([True, False])\n",
    "switch = np.array([True, False])\n",
    "every_n_switch = np.array([2])\n",
    "capacity_factor = np.array([1, 1.25])\n",
    "drop_tokens = np.array([True])\n",
    "n_experts = np.array([2, 4])\n",
    "activation = np.array([\"GELU\"])\n",
    "mlp_dropout = np.array([0.1])\n",
    "expert_dropout = np.array(\n",
    "    [0.2]\n",
    ")  #### HMMMMMM. want to check this first. do in TODO manual benchmarking ipynb!\n",
    "scale = np.array([0.5])  # RoPE\n",
    "decode_method = np.array([\"multinomial\", \"nucleus\", \"top-k\"])\n",
    "\n",
    "configs = [\n",
    "    dict(zip(hp_keys, combo))\n",
    "    for combo in product(\n",
    "        seq_len,\n",
    "        n_embd,\n",
    "        n_ff_ratio,\n",
    "        n_head,\n",
    "        n_kv_head,\n",
    "        n_layer,\n",
    "        norm_first,\n",
    "        use_rms_norm,\n",
    "        switch_first,\n",
    "        use_rotary_embd,\n",
    "        softmax_off_by_one,\n",
    "        switch,\n",
    "        every_n_switch,\n",
    "        capacity_factor,\n",
    "        drop_tokens,\n",
    "        n_experts,\n",
    "        activation,\n",
    "        mlp_dropout,\n",
    "        expert_dropout,\n",
    "        scale,\n",
    "        decode_method,\n",
    "    )\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=metric_keys + param_keys + hp_keys + other_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "68c9e9d8-7c79-4db7-bec8-f9b921de6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_configs(configs):\n",
    "    \"\"\"\n",
    "    Filter illogical config combinations.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for config in configs:\n",
    "        too_many_kv_heads = config[\"n_kv_head\"] > config[\"n_head\"]\n",
    "        no_switch_ever = config[\"every_n_switch\"] > config[\"n_layer\"]\n",
    "        switch_first_but_also_no = config[\"switch_first\"] and not config[\"switch\"]\n",
    "        scale_but_no_rotary = config[\"scale\"] != 1 and not config[\"use_rotary_embd\"]\n",
    "        conds = [\n",
    "            too_many_kv_heads,\n",
    "            no_switch_ever,\n",
    "            switch_first_but_also_no,\n",
    "            scale_but_no_rotary,\n",
    "        ]\n",
    "        if any(conds):\n",
    "            continue\n",
    "        res.append(config)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0b178b9-36bc-425b-9de8-396e4bf2cb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = filter_configs(configs)\n",
    "set_seed(SEED)\n",
    "random.shuffle(configs)\n",
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "701e5d3c-76eb-40ee-9ab6-3c4fded7c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Tokens/s</th>\n",
       "      <th>Bandwidth (GB/s)</th>\n",
       "      <th>Train Memory</th>\n",
       "      <th>Inference Tokens/s</th>\n",
       "      <th>Inference Memory</th>\n",
       "      <th>num_params</th>\n",
       "      <th>num_active_params</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>n_embd</th>\n",
       "      <th>...</th>\n",
       "      <th>every_n_switch</th>\n",
       "      <th>capacity_factor</th>\n",
       "      <th>drop_tokens</th>\n",
       "      <th>n_experts</th>\n",
       "      <th>activation</th>\n",
       "      <th>mlp_dropout</th>\n",
       "      <th>expert_dropout</th>\n",
       "      <th>scale</th>\n",
       "      <th>gqa</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Train Loss, Train Tokens/s, Bandwidth (GB/s), Train Memory, Inference Tokens/s, Inference Memory, num_params, num_active_params, seq_len, n_embd, n_ff_ratio, n_head, n_kv_head, n_layer, norm_first, switch_first, use_rotary_embd, softmax_off_by_one, switch, every_n_switch, capacity_factor, drop_tokens, n_experts, activation, mlp_dropout, expert_dropout, scale, gqa, generated_text]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0350a7-05ef-47bd-80ea-c060b83844ae",
   "metadata": {},
   "source": [
    "### Train + Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c818f3b8-97c9-40ee-9612-3c7bb7a71fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config_df(row, config):\n",
    "    \"\"\"\n",
    "    Set config parameters into the specified DataFrame row.\n",
    "    \"\"\"\n",
    "    row = row.copy()\n",
    "    for k, v in config.items():\n",
    "        row[k] = v\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fa0c5a04-d051-487b-8c6a-855794b0f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config 1/672\n",
      "Performing inference\n",
      "Training config 2/672\n",
      "Performing inference\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(configs):\n",
    "\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "    ## Train\n",
    "    print(f\"Training config {i+1}/{len(configs)}\")\n",
    "    (\n",
    "        train_losses,\n",
    "        val_losses,\n",
    "        train_times,\n",
    "        val_aux_losses,\n",
    "        dropped,\n",
    "        generated_text,\n",
    "        mem_data,\n",
    "        num_params,\n",
    "        num_active_params,\n",
    "        model_size,\n",
    "        gqa,\n",
    "    ) = bench_train(config, device=device)\n",
    "\n",
    "    ## Inference\n",
    "    print(f\"Performing inference \\n\")\n",
    "    NUM_TOKENS = 20  ####\n",
    "    gen_times, gen_mem_data = bench_inference(\n",
    "        config, device=device, num_tokens=NUM_TOKENS, gen_count=3\n",
    "    )\n",
    "\n",
    "    # Compute stats\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    tokens_per_sec = np.mean(train_times)\n",
    "    bandwidth = model_size * tokens_per_sec / 1e9  # GB/s\n",
    "\n",
    "    gen_tokens_per_sec = NUM_TOKENS / np.mean(gen_times)\n",
    "\n",
    "    ## Update results dataframe\n",
    "    df.loc[i] = {\n",
    "        \"Train Loss\": avg_train_loss,\n",
    "        \"Val Loss\": avg_val_loss,\n",
    "        \"Train Tokens/s\": tokens_per_sec,\n",
    "        \"Bandwidth (GB/s)\": bandwidth,\n",
    "        \"Train Memory (GB)\": mem_data,\n",
    "        \"Inference Tokens/s\": gen_tokens_per_sec,\n",
    "        \"Inference Memory (GB)\": gen_mem_data,\n",
    "        \"num_params\": num_params,\n",
    "        \"num_active_params\": num_active_params,\n",
    "        \"gqa\": gqa,\n",
    "        \"top5-generated_text\": generated_text,\n",
    "    }\n",
    "    df.loc[i] = set_config_df(df.loc[i], config)\n",
    "    df.to_csv(f\"{path}/benchmark_logs/bench_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "929b24e3-ee56-4558-8e8e-ed4358134f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Tokens/s</th>\n",
       "      <th>Bandwidth (GB/s)</th>\n",
       "      <th>Train Memory</th>\n",
       "      <th>Inference Tokens/s</th>\n",
       "      <th>Inference Memory</th>\n",
       "      <th>num_params</th>\n",
       "      <th>num_active_params</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>n_embd</th>\n",
       "      <th>...</th>\n",
       "      <th>every_n_switch</th>\n",
       "      <th>capacity_factor</th>\n",
       "      <th>drop_tokens</th>\n",
       "      <th>n_experts</th>\n",
       "      <th>activation</th>\n",
       "      <th>mlp_dropout</th>\n",
       "      <th>expert_dropout</th>\n",
       "      <th>scale</th>\n",
       "      <th>gqa</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.455641</td>\n",
       "      <td>6196.759307</td>\n",
       "      <td>125.873062</td>\n",
       "      <td>{'reserved_memory': 0, 'allocated_memory': 0}</td>\n",
       "      <td>126.404787</td>\n",
       "      <td>{'reserved_memory': 0, 'allocated_memory': 0}</td>\n",
       "      <td>5078085</td>\n",
       "      <td>3894593</td>\n",
       "      <td>128.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>GELU</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>Rt'IcEShOII spt, bemehithethesthy bCI thy mes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.854943</td>\n",
       "      <td>5442.228753</td>\n",
       "      <td>181.301473</td>\n",
       "      <td>{'reserved_memory': 0, 'allocated_memory': 0}</td>\n",
       "      <td>118.746279</td>\n",
       "      <td>{'reserved_memory': 0, 'allocated_memory': 0}</td>\n",
       "      <td>8328265</td>\n",
       "      <td>4779329</td>\n",
       "      <td>128.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>GELU</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>AOOOOAMOOLOEOEOOOEOLOEOEOOLOEEEAOEEEEEEEEOEEOo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Train Loss  Train Tokens/s  Bandwidth (GB/s)  \\\n",
       "0    7.455641     6196.759307        125.873062   \n",
       "1    6.854943     5442.228753        181.301473   \n",
       "\n",
       "                                    Train Memory  Inference Tokens/s  \\\n",
       "0  {'reserved_memory': 0, 'allocated_memory': 0}          126.404787   \n",
       "1  {'reserved_memory': 0, 'allocated_memory': 0}          118.746279   \n",
       "\n",
       "                                Inference Memory  num_params  \\\n",
       "0  {'reserved_memory': 0, 'allocated_memory': 0}     5078085   \n",
       "1  {'reserved_memory': 0, 'allocated_memory': 0}     8328265   \n",
       "\n",
       "   num_active_params  seq_len  n_embd  ...  every_n_switch  capacity_factor  \\\n",
       "0            3894593    128.0   384.0  ...             2.0             1.00   \n",
       "1            4779329    128.0   384.0  ...             2.0             1.25   \n",
       "\n",
       "   drop_tokens  n_experts activation mlp_dropout expert_dropout scale    gqa  \\\n",
       "0         True        2.0       GELU         0.1            0.4   0.5   True   \n",
       "1         True        4.0       GELU         0.1            0.4   0.5  False   \n",
       "\n",
       "                                      generated_text  \n",
       "0  Rt'IcEShOII spt, bemehithethesthy bCI thy mes ...  \n",
       "1  AOOOOAMOOLOEOEOOOEOLOEOEOOLOEEEAOEEEEEEEEOEEOo...  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "55b3d743-ffa2-4afc-9c40-3b5403016af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Train Loss                                                     6.854943\n",
       "Train Tokens/s                                              5442.228753\n",
       "Bandwidth (GB/s)                                             181.301473\n",
       "Train Memory              {'reserved_memory': 0, 'allocated_memory': 0}\n",
       "Inference Tokens/s                                           118.746279\n",
       "Inference Memory          {'reserved_memory': 0, 'allocated_memory': 0}\n",
       "num_params                                                      8328265\n",
       "num_active_params                                               4779329\n",
       "seq_len                                                           128.0\n",
       "n_embd                                                            384.0\n",
       "n_ff_ratio                                                          2.0\n",
       "n_head                                                              4.0\n",
       "n_kv_head                                                           4.0\n",
       "n_layer                                                             4.0\n",
       "norm_first                                                         True\n",
       "switch_first                                                       True\n",
       "use_rotary_embd                                                    True\n",
       "softmax_off_by_one                                                 True\n",
       "switch                                                             True\n",
       "every_n_switch                                                      2.0\n",
       "capacity_factor                                                    1.25\n",
       "drop_tokens                                                        True\n",
       "n_experts                                                           4.0\n",
       "activation                                                         GELU\n",
       "mlp_dropout                                                         0.1\n",
       "expert_dropout                                                      0.4\n",
       "scale                                                               0.5\n",
       "gqa                                                               False\n",
       "generated_text        AOOOOAMOOLOEOEOOOEOLOEOEOOLOEEEAOEEEEEEEEOEEOo...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1][\"top5-generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
