{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853fb4d7-ade1-4952-929b-582e6b26bff8",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd4a31-9a93-44e1-81a6-795469abddb0",
   "metadata": {},
   "source": [
    "This notebook provides official time and memory benchmarking experiments on Switch vs. vanilla decoder transformers and various architectural and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6e43b6-b751-41b3-b98c-ba972b0f3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5289c7-9233-4f31-b12b-7dc62b8466f7",
   "metadata": {},
   "source": [
    "# Models and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869574a-762a-427a-9496-bd74abb4f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "MAX_ITERS = 50000  # max num batches to train\n",
    "PRINT_ITERS = 50  # frequency to print train loss\n",
    "EVAL_ITERS = 500  # frequency to evaluate val loss and generate text from model\n",
    "EVAL_ITER_COUNT = 100  # number of batches to estimate val loss with\n",
    "# given a 10% val split, we have 111540 char, so 100 batches * batch size 16 * seq len 128 = roughly 2x num of chars chosen\n",
    "# EVAL_ITER_COUNT * BATCH_SIZE\n",
    "SAVE_ITERS = 1000  # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_KV_HEAD = 2  # GQA\n",
    "N_LAYER = 4\n",
    "\n",
    "# automatic mixed precision (will be disabled if CPU, not available)\n",
    "USE_AMP = True\n",
    "\n",
    "# RoPE\n",
    "ROPE_SCALE = 0.5\n",
    "\n",
    "## Switch-specific hyperparameters\n",
    "CAPACITY_FACTOR = 1.25\n",
    "N_EXPERT = 2\n",
    "AUX_LOSS_COEF = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4831b-29d4-4c4d-a5df-24bbdd6386d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "root = root if drive is None else \"/content/drive/MyDrive/moe-kit\"\n",
    "path = path if drive is None else \"/content/drive/MyDrive/moe-kit/switch_transformer\"\n",
    "\n",
    "# cannot train in mixed precision on CPU (GradScaler needs cuda)\n",
    "USE_AMP = USE_AMP if device.type == \"cuda\" else False\n",
    "# Tesla T4 does not support bfloat16, CPU does not support float16\n",
    "AMP_DTYPE = torch.float16 if device.type == \"cuda\" else torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c235e-09ee-4d47-b353-04940e670ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed\n",
    "from models.transformer import MLP, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e2ad0-010e-49cd-9a07-6c95a33d072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f761e-fd58-4405-a086-f6139d6fc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + SEQ_LEN + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861d918-9d2c-44aa-ada6-22187def2523",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa23b3-45fe-4c2a-9935-72eedcbb7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "switch_model = Transformer(\n",
    "    VOCAB_SIZE,\n",
    "    SEQ_LEN,\n",
    "    N_EMBD,\n",
    "    N_HEAD,\n",
    "    N_FF,\n",
    "    N_LAYER,\n",
    "    device=device,\n",
    "    n_kv_head=N_KV_HEAD,\n",
    "    norm_first=True,\n",
    "    use_rotary_embd=True,\n",
    "    softmax_off_by_one=False,\n",
    "    switch=True,\n",
    "    switch_first=True,\n",
    "    every_n_switch=2,\n",
    "    capacity_factor=CAPACITY_FACTOR,\n",
    "    drop_tokens=True,\n",
    "    n_experts=N_EXPERT,\n",
    "    expert=MLP,\n",
    "    use_amp=USE_AMP,\n",
    "    amp_dtype=AMP_DTYPE,\n",
    "    activation=\"GELU\",\n",
    "    mlp_dropout=0.1,\n",
    "    expert_dropout=0.4,\n",
    "    scale=ROPE_SCALE,\n",
    ")\n",
    "\n",
    "# Gradient scaling for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "optimizer = torch.optim.AdamW(switch_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105d3f7-d4a9-4d52-914e-f369928adfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(switch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10a614-f7a7-49ed-ba3b-316322a9a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size in bytes\n",
    "SWITCH_MODEL_SIZE = sum(\n",
    "    [\n",
    "        p.numel() * p.dtype.itemsize\n",
    "        for p in itertools.chain(switch_model.parameters(), switch_model.buffers())\n",
    "    ]\n",
    ")\n",
    "SWITCH_MODEL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d859570-97aa-4c1f-9b22-2a6bdac28f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "vanilla_model = Transformer(\n",
    "    VOCAB_SIZE,\n",
    "    SEQ_LEN,\n",
    "    N_EMBD,\n",
    "    N_HEAD,\n",
    "    N_FF,\n",
    "    N_LAYER,\n",
    "    device=device,\n",
    "    n_kv_head=N_KV_HEAD,\n",
    "    norm_first=True,\n",
    "    use_rotary_embd=True,\n",
    "    softmax_off_by_one=False,\n",
    "    use_amp=USE_AMP,\n",
    "    amp_dtype=AMP_DTYPE,\n",
    "    activation=\"GELU\",\n",
    "    switch=False,\n",
    "    mlp_dropout=0.1,\n",
    "    scale=ROPE_SCALE,\n",
    ")\n",
    "\n",
    "# Gradient scaling for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "optimizer = torch.optim.AdamW(vanilla_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509e3f6-7c4e-463f-af9f-19d21ff14a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(switch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7905b-94ea-4902-b776-8d8adeca9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size in bytes\n",
    "VANILLA_MODEL_SIZE = sum(\n",
    "    [\n",
    "        p.numel() * p.dtype.itemsize\n",
    "        for p in itertools.chain(vanilla_model.parameters(), vanilla_model.buffers())\n",
    "    ]\n",
    ")\n",
    "VANILLA_MODEL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be310bd-c56c-4ed3-b68b-39e658354771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ce_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Computes cross-entropy loss.\n",
    "    Inputs:\n",
    "        -logits: Model output of shape (B, S, vocab_size)\n",
    "        -counts:\n",
    "    \"\"\"\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c7654-ba15-4361-9ec2-4ea836dba142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aux_loss(counts, prob_sum):\n",
    "    \"\"\"\n",
    "    Computes Switch Transformer auxiliary loss.\n",
    "    Inputs:\n",
    "        -counts: Number of tokens passed to each expert in each switch layer (num_switch_layers x n_experts)\n",
    "        Note this is NOT equivalent to n_layer; num_switch_layers depends on `switch_first` and `every_n_switch`\n",
    "        -prob_sum: Sum of probs across all tokens for each expert (num_switch_layers x n_experts)\n",
    "    \"\"\"\n",
    "\n",
    "    # total number of tokens routed in that layer\n",
    "    token_count = counts.sum(dim=-1, keepdims=True)\n",
    "\n",
    "    # prop of tokens dispatched to each expert\n",
    "    route_frac = counts / token_count\n",
    "\n",
    "    # fraction of total probability allocated for each expert\n",
    "    # recall prob_sum := softmaxed probs, which added to 1 across the experts for each token\n",
    "    # we divide by num_tokens so that the overall 2D scalar sum of prob_frac is 1\n",
    "    # intuitively we are forcing the total prob for each layer across the experts to be 1 so we can take proportions,\n",
    "    # the same way as above\n",
    "    prob_frac = prob_sum / token_count\n",
    "\n",
    "    # Auxiliary loss\n",
    "    # L = N \\sum_{i=1}^N f_i • P_i\n",
    "    aux_loss = N_EXPERT * (route_frac * prob_frac).sum()\n",
    "    return aux_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4b46f-257c-4b84-9d17-cf0d36949a6a",
   "metadata": {},
   "source": [
    "## TODO: write bench_train and bench_inference fns\n",
    "    ## to-test:\n",
    "        -training: architecture vs. samples/s, loss vs. samples seen (sample efficiency)\n",
    "        -inference: num_tokens vs. time, decoding method vs. tokens/s, architecture vs tokens/s\n",
    "    -architecture: switch (1/2/4/8 experts, capacity) vs. vanilla, hidden_dim/n_layer/n_head, total_params, ROPE vs. cos/sin PE, etc.\n",
    "        -also benchmark memory, e.g. effect of GQA\n",
    "        -Roadmap: Benchmark specific time/memory spent for MLP vs. switch FF, attention vs. MLP proportions within a block, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9259123-7388-4dad-9a60-bc4b0ee29650",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write params above into config files, write get_model wrappers, etc., to use in these functions\n",
    "## then you can easily delete model and config after every run\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_train():\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def bench_inference():\n",
    "    pass\n",
    "\n",
    "\n",
    "# start = perf_counter()\n",
    "# ## pass\n",
    "# total_time = perf_counter() - start # in ms\n",
    "# mem_data = {'reserved_memory': torch.cuda.memory_reserved(0),\n",
    "#             'allocated_memory': torch.cuda.memory_allocated(0)}\n",
    "# del model\n",
    "# del config\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "# return total_time, mem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cf45b-dfe0-45db-ada3-01bca4d0c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try profiler for fun\n",
    "\n",
    "# with profile(activities=[\n",
    "#         ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#              profile_memory=True,\n",
    "#              record_shapes=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         model(inputs)\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb7fa9-87bf-4953-badb-8732f927fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### driver code:\n",
    "\n",
    "### create configs for what you want to try, e.g. a yaml file or just native dicts/lists here\n",
    "## create result dataframe with configs\n",
    "## loop over all and run training and inference fns, append results\n",
    "## plot results from dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
