{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ca647d-de3f-4eac-8774-23446a407383",
   "metadata": {},
   "source": [
    "# todo: try actually running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5f235-fb3a-4f9b-b8a9-539cae02fac9",
   "metadata": {},
   "source": [
    "#### tovalidate: whether early stopping of wandb.log actually works. maybe change config to only LR 1 or smth outrageous to try and trigger the condition \n",
    "#### remark: changed print and eval iters to functions of batch size since BS changes in the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43b8d2e-c9ab-4e8b-b9bd-28f50c3f386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583c1a9f-86d7-49b7-8f5d-012221e77644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import Api\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea16d18-a454-413f-ac52-da6d9dc77b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterru3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f7ee6-dc5b-4548-8f90-105502df5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb19b1eb-945d-4900-9d30-913a98718b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{root}/switch_sweep.yaml\", \"r\") as file:\n",
    "    sweep_config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15028f9b-d6b9-40a0-ae43-cc6f826651af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
       " 'parameters': {'LR': {'values': [0.0005, 0.001, 0.003, 0.005]},\n",
       "  'batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
       "  'optimizer': {'values': ['adamw', 'sgd']},\n",
       "  'activation': {'values': ['GELU', 'GEGLU', 'SwiGLU']},\n",
       "  'n_experts': {'values': [2, 4, 8, 16, 32, 64]},\n",
       "  'capacity_factor': {'values': [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]},\n",
       "  'aux_loss_coef': {'values': [0.005, 0.01, 0.05, 0.1, 0.15]},\n",
       "  'norm_first': {'values': [True, False]},\n",
       "  'switch_first': {'values': [True, False]},\n",
       "  'every_n_switch': {'values': [1, 2, 3, 4]},\n",
       "  'mlp_dropout': {'values': [0.1, 0.2, 0.3, 0.4]},\n",
       "  'expert_dropout': {'values': [0.1, 0.2, 0.3, 0.4]},\n",
       "  'rope_scale': {'values': [0.25, 0.5, 0.75, 1]}},\n",
       " 'early_terminate': {'type': 'hyperband', 's': 2, 'eta': 3, 'max_iter': 27}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994626f2-a125-40a6-b777-a3c5bbf82edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: qw2sczpt\n",
      "Sweep URL: https://wandb.ai/terru3/switch_moe/sweeps/qw2sczpt\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"switch_moe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d40f5-2bc2-4a3c-ad20-f973978ff1b2",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b05b899-86b9-4414-9f80-e197b9d3d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "# LR = 1e-3\n",
    "# BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "# MAX_ITERS = 50000  # max num batches to train\n",
    "# PRINT_ITERS = 50  # frequency to print train loss\n",
    "# EVAL_ITERS = 500  # frequency to evaluate val loss and generate text from model\n",
    "# EVAL_ITER_COUNT = 100  # number of batches to estimate val loss with\n",
    "# given a 10% val split, we have 111540 char, so 100 batches * batch size 16 * seq len 128 = roughly 2x num of chars chosen\n",
    "# EVAL_ITER_COUNT * BATCH_SIZE\n",
    "# SAVE_ITERS = 1000  # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_KV_HEAD = 2  # GQA\n",
    "N_LAYER = 4\n",
    "\n",
    "# automatic mixed precision (will be disabled if CPU, not available)\n",
    "USE_AMP = True\n",
    "\n",
    "# RoPE\n",
    "# ROPE_SCALE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1328ae-f388-4c15-8e78-a1c9430c88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: switch_wandb_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN\n"
     ]
    }
   ],
   "source": [
    "## Switch-specific hyperparameters\n",
    "# CAPACITY_FACTOR = 1.25\n",
    "# N_EXPERT = 2\n",
    "# AUX_LOSS_COEF = 0.01\n",
    "\n",
    "MODEL_NAME = (\n",
    "    f\"switch_wandb_{N_LAYER}_LAYERs_{N_HEAD}_HEAD_{N_EMBD}_EMBD_DIM_{SEQ_LEN}_SEQ_LEN\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b757be9-b268-493c-88a1-53b95db6e9a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6e1107-9998-4282-a8ca-0636e3f8e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002c526f-cb1a-4834-ad5e-ff1bce1dec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b93bcb63-2867-4027-bee9-b0b4ca2a58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "root = root if drive is None else \"/content/drive/MyDrive/moe-kit\"\n",
    "path = path if drive is None else \"/content/drive/MyDrive/moe-kit/switch_transformer\"\n",
    "\n",
    "# cannot train in mixed precision on CPU (GradScaler needs cuda)\n",
    "USE_AMP = USE_AMP if device.type == \"cuda\" else False\n",
    "# Tesla T4 does not support bfloat16, CPU does not support float16\n",
    "AMP_DTYPE = torch.float16 if device.type == \"cuda\" else torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb48bc4-c538-4b31-add1-2750a08bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed\n",
    "from models.transformer import MLP, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "686a47d1-9b20-455d-8775-774bff55aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713212-4a60-4e60-90f4-f6e0a704a299",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b6d9fe-8f18-474d-aefb-4e44fd0b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f\"Vocab: {chars}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "545415c0-5e49-42c9-b10a-1432028233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 47, 52, 63, 7, 57, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 57, 1, 57, 47, 41, 49]\n",
      "tiny-shakespeare is sick\n"
     ]
    }
   ],
   "source": [
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "print(encode(\"tiny-shakespeare is sick\"))\n",
    "print(decode(encode(\"tiny-shakespeare is sick\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0124c4-58be-4acd-8249-fd9e5cecbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 1003854 val_data len: 111540\n"
     ]
    }
   ],
   "source": [
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data len:\", len(train_data), \"val_data len:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690640c-df7d-4d0c-bee7-f8ff428df809",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17fdebcd-6e75-4c00-b03d-8c06fbd32a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (batch_size,))\n",
    "    x = torch.stack([data[i : i + SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + SEQ_LEN + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48e940d-c7af-4367-81a9-3310531dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ce_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Computes cross-entropy loss.\n",
    "    Inputs:\n",
    "        -logits: Model output of shape (B, S, vocab_size)\n",
    "        -counts:\n",
    "    \"\"\"\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdbf6e1a-3965-49ed-ac0e-9031dc9a5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aux_loss(counts, prob_sum, n_expert):\n",
    "    \"\"\"\n",
    "    Computes Switch Transformer auxiliary loss.\n",
    "    Inputs:\n",
    "        -counts: Number of tokens passed to each expert in each switch layer (num_switch_layers x n_experts)\n",
    "        Note this is NOT equivalent to n_layer; num_switch_layers depends on `switch_first` and `every_n_switch`\n",
    "        -prob_sum: Sum of probs across all tokens for each expert (num_switch_layers x n_experts)\n",
    "    \"\"\"\n",
    "\n",
    "    # total number of tokens routed in that layer\n",
    "    token_count = counts.sum(dim=-1, keepdims=True)\n",
    "\n",
    "    # prop of tokens dispatched to each expert\n",
    "    route_frac = counts / token_count\n",
    "\n",
    "    # fraction of total probability allocated for each expert\n",
    "    # recall prob_sum := softmaxed probs, which added to 1 across the experts for each token\n",
    "    # we divide by num_tokens so that the overall 2D scalar sum of prob_frac is 1\n",
    "    # intuitively we are forcing the total prob for each layer across the experts to be 1 so we can take proportions,\n",
    "    # the same way as above\n",
    "    prob_frac = prob_sum / token_count\n",
    "\n",
    "    # Auxiliary loss\n",
    "    # L = N \\sum_{i=1}^N f_i • P_i\n",
    "    aux_loss = n_expert * (route_frac * prob_frac).sum()\n",
    "    return aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e7a8320-5a45-4d5b-8a5e-0680a94c6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    set_seed(SEED)\n",
    "\n",
    "    model = Transformer(\n",
    "        VOCAB_SIZE,\n",
    "        SEQ_LEN,\n",
    "        N_EMBD,\n",
    "        N_HEAD,\n",
    "        N_FF,\n",
    "        N_LAYER,\n",
    "        device=device,\n",
    "        n_kv_head=N_KV_HEAD,\n",
    "        norm_first=config.get(\"norm_first\"),\n",
    "        use_rotary_embd=True,\n",
    "        softmax_off_by_one=False,\n",
    "        switch=True,\n",
    "        switch_first=config.get(\"switch_first\"),\n",
    "        every_n_switch=config.get(\"every_n_switch\"),\n",
    "        capacity_factor=config.get(\"capacity_factor\"),\n",
    "        drop_tokens=True,\n",
    "        n_experts=config.get(\"n_experts\"),\n",
    "        expert=MLP,\n",
    "        use_amp=USE_AMP,\n",
    "        activation=config.get(\"activation\"),\n",
    "        mlp_dropout=config.get(\"mlp_dropout\"),\n",
    "        expert_dropout=config.get(\"expert_dropout\"),\n",
    "        scale=config.get(\"rope_scale\"),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd1dbec1-a774-4ff3-b614-6dd61c587398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(config, model):\n",
    "    if config.get(\"optimizer\") == \"adamw\":\n",
    "        return torch.optim.AdamW(model.parameters(), lr=config.get(\"LR\"))\n",
    "    elif config.get(\"optimizer\") == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=config.get(\"LR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca27c6e-2d5f-4131-87fa-275d8bc35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    config=None,\n",
    "    train_loss_list=None,\n",
    "    val_loss_list=None,\n",
    "    train_time_list=None,\n",
    "    val_aux_loss_list=None,\n",
    "    dropped_list=None,\n",
    "):\n",
    "\n",
    "    train_losses = train_loss_list if train_loss_list is not None else []\n",
    "    val_losses = val_loss_list if val_loss_list is not None else []\n",
    "    train_times = train_time_list if train_time_list is not None else []\n",
    "    val_aux_losses = val_aux_loss_list if val_aux_loss_list is not None else []\n",
    "    dropped = dropped_list if dropped_list is not None else []\n",
    "\n",
    "    #### NEW:\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config) as wandb_r:\n",
    "        # also has `resume` arg,\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb_r.config\n",
    "        print(f\"Run ID: {wandb_r.id}, Name: {wandb_r.name}\")\n",
    "\n",
    "        # training setup\n",
    "        model = build_model(config)\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = build_optimizer(config, model)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "        # model size in bytes\n",
    "        MODEL_SIZE = sum(\n",
    "            [\n",
    "                p.numel() * p.dtype.itemsize\n",
    "                for p in itertools.chain(model.parameters(), model.buffers())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # configure print / logging freq to be fn of changing batch size\n",
    "        # same for eval iter count for estimating val loss in eval fn.\n",
    "        B = config.get(\"batch_size\")\n",
    "\n",
    "        MAX_ITERS = int(8e5 / B)\n",
    "        PRINT_ITERS = MAX_ITERS // 1000\n",
    "        EVAL_ITERS = MAX_ITERS // 100\n",
    "        SAVE_ITERS = MAX_ITERS // 500  ########## TEMP. should be 50\n",
    "\n",
    "        # begin training\n",
    "        for step in range(MAX_ITERS):\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            inputs, targets = get_batch(\"train\", B)\n",
    "            with torch.autocast(\n",
    "                device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP\n",
    "            ):\n",
    "                if model.switch:\n",
    "                    logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                    loss = calc_ce_loss(logits, targets)\n",
    "                    aux_loss = calc_aux_loss(counts, prob_sum, config.get(\"n_experts\"))\n",
    "                    loss += config.get(\"aux_loss_coef\") * aux_loss\n",
    "                    drop_frac = (np.array(n_dropped) / (B * SEQ_LEN)).tolist()\n",
    "                    dropped.append(drop_frac)  # for logging purposes\n",
    "                else:\n",
    "                    logits = model(inputs)\n",
    "                    loss = calc_ce_loss(logits, targets)\n",
    "\n",
    "            train_losses.append(loss.item())  # for printing\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # loss.backward()\n",
    "\n",
    "            # Monitor gradient norm\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            with torch.autocast(\n",
    "                device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP\n",
    "            ):\n",
    "                grads = [\n",
    "                    param.grad.detach().flatten()\n",
    "                    for param in model.parameters()\n",
    "                    if param.grad is not None\n",
    "                ]\n",
    "                norm = torch.cat(grads).norm()\n",
    "\n",
    "            train_time = time.perf_counter() - start\n",
    "            tokens_per_sec = (1 / train_time) * B * SEQ_LEN\n",
    "            train_times.append(tokens_per_sec)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # print training statistics\n",
    "            if step % PRINT_ITERS == 0 and step != 0:\n",
    "                print(\n",
    "                    f\"Step {step}/{MAX_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.3f} |\",\n",
    "                    f\"Grad Norm: {norm:.2f} | Running Avg Tokens/Sec: {np.mean(train_times):.2f} |\",\n",
    "                    f\"Bandwidth: {MODEL_SIZE * np.mean(train_times) / 1e9:.2f} GB/s\",\n",
    "                )\n",
    "\n",
    "                # wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train_loss\": loss.item(),\n",
    "                        \"samples_seen\": (step + 1) * B,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # estimate val loss, generate text and save\n",
    "            if step % EVAL_ITERS == 0 and step != 0:\n",
    "                val_losses, val_aux_losses = estimate_loss(\n",
    "                    config, model, val_losses, val_aux_losses, device\n",
    "                )\n",
    "\n",
    "                # for wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"val_loss\": val_losses[-1],\n",
    "                        \"samples_seen\": (step + 1) * B,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # no idea if this works or not. also didn't add api run id into name to avoid overlap, should I??\n",
    "            if step % SAVE_ITERS == 0 and step != 0:\n",
    "                model_artifact = wandb.Artifact(\n",
    "                    MODEL_NAME, type=\"model\", metadata=dict(config)\n",
    "                )\n",
    "\n",
    "                torch.save(\n",
    "                    model.state_dict(), f\"{path}/wandb_artifacts/{MODEL_NAME}.pt\"\n",
    "                )\n",
    "                model_artifact.add_file(f\"{path}/wandb_artifacts/{MODEL_NAME}.pt\")\n",
    "                wandb_r.log_artifact(\n",
    "                    model_artifact\n",
    "                )  # log artifact version e.g. \"MODEL_NAME:v0\"\n",
    "                # caution: calls to log_artifact are performed asynchronously for performant uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0abba78-5cd4-4ee7-b116-203fdeef3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(config, model, val_losses, val_aux_losses, device):\n",
    "    model.eval()\n",
    "    B = config.get(\"batch_size\")\n",
    "    EVAL_ITER_COUNT = int(1600 / B)\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    aux_losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch(\"test\", B)\n",
    "        with torch.autocast(device_type=device.type, dtype=AMP_DTYPE, enabled=USE_AMP):\n",
    "            if model.switch:\n",
    "                logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "                aux_losses[k] = calc_aux_loss(counts, prob_sum, config.get(\"n_experts\"))\n",
    "                losses[k] += config.get(\"aux_loss_coef\") * aux_losses[k]\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "                losses[k] = calc_ce_loss(logits, targets)\n",
    "    val_loss, val_aux_loss = losses.mean().item(), aux_losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    val_aux_losses.append(val_aux_loss)  # track separate aux loss for logging\n",
    "    # keep model in eval, next call is to .generate() anyway\n",
    "    print(f\"Est. Val Loss: {val_loss:.3f} | Est. Aux Loss: {val_aux_loss:.3f}\")\n",
    "    return val_losses, val_aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fa5b683-77e7-4e7e-938d-6e1f7e29a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sngup0hu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLR: 0.003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: GELU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taux_loss_coef: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcapacity_factor: 1.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tevery_n_switch: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texpert_dropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmlp_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_experts: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnorm_first: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trope_scale: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tswitch_first: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Terru/Desktop/self-learn/moe-kit/switch_transformer/wandb/run-20240116_205947-sngup0hu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/terru3/switch_moe/runs/sngup0hu' target=\"_blank\">worldly-sweep-1</a></strong> to <a href='https://wandb.ai/terru3/switch_moe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/terru3/switch_moe/sweeps/joxutbnu' target=\"_blank\">https://wandb.ai/terru3/switch_moe/sweeps/joxutbnu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/terru3/switch_moe' target=\"_blank\">https://wandb.ai/terru3/switch_moe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/terru3/switch_moe/sweeps/joxutbnu' target=\"_blank\">https://wandb.ai/terru3/switch_moe/sweeps/joxutbnu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/terru3/switch_moe/runs/sngup0hu' target=\"_blank\">https://wandb.ai/terru3/switch_moe/runs/sngup0hu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25/25000 | Running Avg Train Loss: 5.039 | Grad Norm: 1.53 | Running Avg Tokens/Sec: 7449.05 | Bandwidth: 28.00 GB/s\n",
      "Step 50/25000 | Running Avg Train Loss: 4.036 | Grad Norm: 0.75 | Running Avg Tokens/Sec: 7716.43 | Bandwidth: 29.01 GB/s\n",
      "Step 75/25000 | Running Avg Train Loss: 3.607 | Grad Norm: 0.77 | Running Avg Tokens/Sec: 7498.41 | Bandwidth: 28.19 GB/s\n",
      "Step 100/25000 | Running Avg Train Loss: 3.356 | Grad Norm: 0.70 | Running Avg Tokens/Sec: 7545.13 | Bandwidth: 28.36 GB/s\n",
      "Step 125/25000 | Running Avg Train Loss: 3.189 | Grad Norm: 0.78 | Running Avg Tokens/Sec: 7635.85 | Bandwidth: 28.70 GB/s\n",
      "Step 150/25000 | Running Avg Train Loss: 3.068 | Grad Norm: 0.61 | Running Avg Tokens/Sec: 7757.37 | Bandwidth: 29.16 GB/s\n",
      "Step 175/25000 | Running Avg Train Loss: 2.976 | Grad Norm: 0.69 | Running Avg Tokens/Sec: 7830.98 | Bandwidth: 29.44 GB/s\n",
      "Step 200/25000 | Running Avg Train Loss: 2.899 | Grad Norm: 0.68 | Running Avg Tokens/Sec: 7863.53 | Bandwidth: 29.56 GB/s\n",
      "Step 225/25000 | Running Avg Train Loss: 2.835 | Grad Norm: 0.70 | Running Avg Tokens/Sec: 7754.52 | Bandwidth: 29.15 GB/s\n",
      "Step 250/25000 | Running Avg Train Loss: 2.779 | Grad Norm: 0.78 | Running Avg Tokens/Sec: 7705.22 | Bandwidth: 28.96 GB/s\n",
      "Est. Val Loss: 2.184 | Est. Aux Loss: 1.000\n",
      "Step 275/25000 | Running Avg Train Loss: 2.730 | Grad Norm: 0.65 | Running Avg Tokens/Sec: 7738.70 | Bandwidth: 29.09 GB/s\n",
      "Step 300/25000 | Running Avg Train Loss: 2.686 | Grad Norm: 0.68 | Running Avg Tokens/Sec: 7664.56 | Bandwidth: 28.81 GB/s\n",
      "Step 325/25000 | Running Avg Train Loss: 2.647 | Grad Norm: 0.61 | Running Avg Tokens/Sec: 7635.53 | Bandwidth: 28.70 GB/s\n",
      "Step 350/25000 | Running Avg Train Loss: 2.611 | Grad Norm: 0.55 | Running Avg Tokens/Sec: 7592.57 | Bandwidth: 28.54 GB/s\n",
      "Step 375/25000 | Running Avg Train Loss: 2.579 | Grad Norm: 0.73 | Running Avg Tokens/Sec: 7616.13 | Bandwidth: 28.63 GB/s\n",
      "Step 400/25000 | Running Avg Train Loss: 2.548 | Grad Norm: 0.54 | Running Avg Tokens/Sec: 7615.75 | Bandwidth: 28.63 GB/s\n",
      "Step 425/25000 | Running Avg Train Loss: 2.520 | Grad Norm: 0.63 | Running Avg Tokens/Sec: 7634.77 | Bandwidth: 28.70 GB/s\n",
      "Step 450/25000 | Running Avg Train Loss: 2.494 | Grad Norm: 0.63 | Running Avg Tokens/Sec: 7640.55 | Bandwidth: 28.72 GB/s\n",
      "Step 475/25000 | Running Avg Train Loss: 2.470 | Grad Norm: 0.52 | Running Avg Tokens/Sec: 7655.23 | Bandwidth: 28.78 GB/s\n",
      "Step 500/25000 | Running Avg Train Loss: 2.446 | Grad Norm: 0.57 | Running Avg Tokens/Sec: 7666.95 | Bandwidth: 28.82 GB/s\n",
      "Est. Val Loss: 1.988 | Est. Aux Loss: 1.000\n",
      "Step 525/25000 | Running Avg Train Loss: 2.425 | Grad Norm: 0.56 | Running Avg Tokens/Sec: 7672.85 | Bandwidth: 28.84 GB/s\n",
      "Step 550/25000 | Running Avg Train Loss: 2.404 | Grad Norm: 0.60 | Running Avg Tokens/Sec: 7694.38 | Bandwidth: 28.92 GB/s\n",
      "Step 575/25000 | Running Avg Train Loss: 2.385 | Grad Norm: 0.62 | Running Avg Tokens/Sec: 7714.71 | Bandwidth: 29.00 GB/s\n",
      "Step 600/25000 | Running Avg Train Loss: 2.367 | Grad Norm: 0.50 | Running Avg Tokens/Sec: 7727.10 | Bandwidth: 29.05 GB/s\n",
      "Step 625/25000 | Running Avg Train Loss: 2.349 | Grad Norm: 0.55 | Running Avg Tokens/Sec: 7720.89 | Bandwidth: 29.02 GB/s\n",
      "Step 650/25000 | Running Avg Train Loss: 2.333 | Grad Norm: 0.53 | Running Avg Tokens/Sec: 7721.67 | Bandwidth: 29.03 GB/s\n",
      "Step 675/25000 | Running Avg Train Loss: 2.317 | Grad Norm: 0.56 | Running Avg Tokens/Sec: 7731.76 | Bandwidth: 29.06 GB/s\n",
      "Step 700/25000 | Running Avg Train Loss: 2.302 | Grad Norm: 0.58 | Running Avg Tokens/Sec: 7735.81 | Bandwidth: 29.08 GB/s\n",
      "Step 725/25000 | Running Avg Train Loss: 2.287 | Grad Norm: 0.55 | Running Avg Tokens/Sec: 7746.30 | Bandwidth: 29.12 GB/s\n",
      "Step 750/25000 | Running Avg Train Loss: 2.274 | Grad Norm: 0.51 | Running Avg Tokens/Sec: 7737.61 | Bandwidth: 29.09 GB/s\n",
      "Est. Val Loss: 1.911 | Est. Aux Loss: 1.001\n",
      "Step 775/25000 | Running Avg Train Loss: 2.260 | Grad Norm: 0.52 | Running Avg Tokens/Sec: 7706.43 | Bandwidth: 28.97 GB/s\n",
      "Step 800/25000 | Running Avg Train Loss: 2.247 | Grad Norm: 0.61 | Running Avg Tokens/Sec: 7696.52 | Bandwidth: 28.93 GB/s\n",
      "Step 825/25000 | Running Avg Train Loss: 2.235 | Grad Norm: 0.57 | Running Avg Tokens/Sec: 7709.24 | Bandwidth: 28.98 GB/s\n",
      "Step 850/25000 | Running Avg Train Loss: 2.224 | Grad Norm: 0.50 | Running Avg Tokens/Sec: 7699.57 | Bandwidth: 28.94 GB/s\n",
      "Step 875/25000 | Running Avg Train Loss: 2.212 | Grad Norm: 0.59 | Running Avg Tokens/Sec: 7672.91 | Bandwidth: 28.84 GB/s\n",
      "Step 900/25000 | Running Avg Train Loss: 2.202 | Grad Norm: 0.55 | Running Avg Tokens/Sec: 7652.43 | Bandwidth: 28.77 GB/s\n",
      "Step 925/25000 | Running Avg Train Loss: 2.192 | Grad Norm: 0.53 | Running Avg Tokens/Sec: 7654.82 | Bandwidth: 28.78 GB/s\n",
      "Step 950/25000 | Running Avg Train Loss: 2.181 | Grad Norm: 0.51 | Running Avg Tokens/Sec: 7659.03 | Bandwidth: 28.79 GB/s\n",
      "Step 975/25000 | Running Avg Train Loss: 2.171 | Grad Norm: 0.53 | Running Avg Tokens/Sec: 7654.75 | Bandwidth: 28.78 GB/s\n",
      "Step 1000/25000 | Running Avg Train Loss: 2.162 | Grad Norm: 0.50 | Running Avg Tokens/Sec: 7652.57 | Bandwidth: 28.77 GB/s\n",
      "Est. Val Loss: 1.856 | Est. Aux Loss: 1.001\n",
      "Step 1025/25000 | Running Avg Train Loss: 2.152 | Grad Norm: 0.52 | Running Avg Tokens/Sec: 7645.50 | Bandwidth: 28.74 GB/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38f2ceb9-d273-427c-8196-239f3cac0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### if you want to load saved model artifact for further wandb runs, etc., do this\n",
    "# model_artifact = api.artifact(f\"terru3/switch_moe/{MODEL_NAME}:latest\")\n",
    "# model_dir = model_artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85c98505-a0ce-45b0-ae35-7007d83c03b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load in wandb checkpoints (note only model saved, not optimizer)\n",
    "\n",
    "# create model, Transformer(_____)\n",
    "model.load_state_dict(torch.load(f\"{path}/wandb_artifacts/{MODEL_NAME}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081394e-7c89-4bf8-87fa-561380aaaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: try accessing best run\n",
    "## most ppl do sweep = api.sweep but I'm not using API for sweep, so how to access??? plus\n",
    "## even when you use API that's inside train() no?\n",
    "\n",
    "# sweep.best_run()\n",
    "# sweep.best_run().config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3548724e-f4bd-453b-bf1e-2162d57729bd",
   "metadata": {},
   "source": [
    "# Run this when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c7bc4bf-f17a-4886-9ba3-9729c5082b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▂▁</td></tr><tr><td>samples_seen</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.8559</td></tr><tr><td>samples_seen</td><td>32832</td></tr><tr><td>train_loss</td><td>1.78517</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-1</strong> at: <a href='https://wandb.ai/terru3/switch_moe/runs/sngup0hu' target=\"_blank\">https://wandb.ai/terru3/switch_moe/runs/sngup0hu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240116_205947-sngup0hu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1050/25000 | Running Avg Train Loss: 2.143 |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f1322-e411-4f6c-bdcf-58798b17d6c3",
   "metadata": {},
   "source": [
    "# –––––––––––––––––––––––––––––––––––––––––––––––-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95154-d641-4d9b-9e8a-c054d5921921",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5157dd6-d6d6-42af-9104-272094033ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "772a7473-2e8f-4c61-87d9-612e01150b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRY: thou arte a my she\n",
      "Which have may and of contain.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good as I tall no knrow, for shalt agarnt\n",
      "And mpo; and Kong a m, not outhpile Mesce.\n",
      "\n",
      "HENRY VI:\n",
      "When I will thy lookess, oner the pexstrey\n",
      "The the the hee voagh gresed livioe.\n",
      "\n",
      "MENCIO:\n",
      "My her callis his peaced of to that\n",
      "We where's by shall bore: as shall myselvea\n",
      "The plender feuls!\n",
      "\n",
      "PAPELLANT:\n",
      "In the the into balby me dods to love,\n",
      "In but the giving of nyou ase. I tall it-me?'e Goveuling\n",
      "The theer haught art praver count madeng Camen:\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"TERRY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
