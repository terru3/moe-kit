{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d40f5-2bc2-4a3c-ad20-f973978ff1b2",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b05b899-86b9-4414-9f80-e197b9d3d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "MAX_ITERS = 50000  # max num batches to train\n",
    "PRINT_ITERS = 50  # frequency to print train loss\n",
    "EVAL_ITERS = 250  # frequency to evaluate val loss and generate text from model\n",
    "EVAL_ITER_COUNT = 500  # number of batches to estimate val loss with\n",
    "SAVE_ITERS = 1000  # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1328ae-f388-4c15-8e78-a1c9430c88bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN\n"
     ]
    }
   ],
   "source": [
    "## Switch-specific hyperparameters\n",
    "CAPACITY_FACTOR = 1.25\n",
    "N_EXPERT = 2\n",
    "AUX_LOSS_COEF = 0.01\n",
    "\n",
    "MODEL_NAME = (\n",
    "    f\"switch_{N_LAYER}_LAYERs_{N_HEAD}_HEAD_{N_EMBD}_EMBD_DIM_{SEQ_LEN}_SEQ_LEN\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b757be9-b268-493c-88a1-53b95db6e9a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6e1107-9998-4282-a8ca-0636e3f8e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb48bc4-c538-4b31-add1-2750a08bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed, device\n",
    "from models.transformer import MLP, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686a47d1-9b20-455d-8775-774bff55aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713212-4a60-4e60-90f4-f6e0a704a299",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b6d9fe-8f18-474d-aefb-4e44fd0b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f\"Vocab: {chars}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545415c0-5e49-42c9-b10a-1432028233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 47, 52, 63, 7, 57, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 57, 1, 57, 47, 41, 49]\n",
      "tiny-shakespeare is sick\n"
     ]
    }
   ],
   "source": [
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2txt = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: \"\".join([idx2txt[i] for i in l])\n",
    "\n",
    "print(encode(\"tiny-shakespeare is sick\"))\n",
    "print(decode(encode(\"tiny-shakespeare is sick\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0124c4-58be-4acd-8249-fd9e5cecbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 1003854 val_data len: 111540\n"
     ]
    }
   ],
   "source": [
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data len:\", len(train_data), \"val_data len:\", len(val_data))\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + SEQ_LEN + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af611b-be70-4737-84df-f7bf13b057c9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb0eb6-b509-4da1-b361-e34684177abe",
   "metadata": {},
   "source": [
    "### TODO: tweak hyperparameters to keep effective parameter count or FLOP count constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679ef31f-5253-49f9-9e24-c64b05655cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "model = Transformer(\n",
    "    VOCAB_SIZE,\n",
    "    SEQ_LEN,\n",
    "    N_EMBD,\n",
    "    N_HEAD,\n",
    "    N_FF,\n",
    "    N_LAYER,\n",
    "    switch=True,\n",
    "    capacity_factor=CAPACITY_FACTOR,\n",
    "    drop_tokens=True,\n",
    "    n_experts=N_EXPERT,\n",
    "    expert=MLP,\n",
    "    device=device,\n",
    "    mlp_dropout=0.1,\n",
    "    expert_dropout=0.4,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eacebc5-8b3a-4c5d-9c89-f946fd25e948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "├─Embedding: 1-1                                   8,320\n",
       "├─PositionalEncoding: 1-2                          --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─Block: 2-1                                  --\n",
       "│    │    └─MultiHeadAttention: 3-1                65,536\n",
       "│    │    └─SwitchFeedForward: 3-2                 263,682\n",
       "│    │    └─LayerNorm: 3-3                         256\n",
       "│    │    └─LayerNorm: 3-4                         256\n",
       "│    │    └─Dropout: 3-5                           --\n",
       "│    │    └─Dropout: 3-6                           --\n",
       "│    └─Block: 2-2                                  --\n",
       "│    │    └─MultiHeadAttention: 3-7                65,536\n",
       "│    │    └─MLP: 3-8                               131,712\n",
       "│    │    └─LayerNorm: 3-9                         256\n",
       "│    │    └─LayerNorm: 3-10                        256\n",
       "│    │    └─Dropout: 3-11                          --\n",
       "│    │    └─Dropout: 3-12                          --\n",
       "│    └─Block: 2-3                                  --\n",
       "│    │    └─MultiHeadAttention: 3-13               65,536\n",
       "│    │    └─SwitchFeedForward: 3-14                263,682\n",
       "│    │    └─LayerNorm: 3-15                        256\n",
       "│    │    └─LayerNorm: 3-16                        256\n",
       "│    │    └─Dropout: 3-17                          --\n",
       "│    │    └─Dropout: 3-18                          --\n",
       "│    └─Block: 2-4                                  --\n",
       "│    │    └─MultiHeadAttention: 3-19               65,536\n",
       "│    │    └─MLP: 3-20                              131,712\n",
       "│    │    └─LayerNorm: 3-21                        256\n",
       "│    │    └─LayerNorm: 3-22                        256\n",
       "│    │    └─Dropout: 3-23                          --\n",
       "│    │    └─Dropout: 3-24                          --\n",
       "├─Linear: 1-4                                      8,385\n",
       "├─Dropout: 1-5                                     --\n",
       "===========================================================================\n",
       "Total params: 1,071,685\n",
       "Trainable params: 1,071,685\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c9b8a-bf50-427a-9b9c-e6f95a3c63c7",
   "metadata": {},
   "source": [
    "## Computing activated parameter count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7fbf0-f87c-4652-83c9-b2d9081fb68f",
   "metadata": {},
   "source": [
    "Here, we find any parameters unique to the switch layer and to experts beyond the first and subtract from the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96619ec1-24cf-4f35-b22d-91e2409d5461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807745"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_additional_params = 0\n",
    "switch_layer_names = [f\"experts.{i}\" for i in range(1, N_EXPERT)] + [\"switch\"]\n",
    "for (name, layer) in model.named_parameters():\n",
    "    # if experts.i (i > 0) or switch in name, subtract from total count\n",
    "    if any(substr in name for substr in switch_layer_names):\n",
    "        switch_additional_params += layer.numel()\n",
    "\n",
    "active_param_count = (\n",
    "    sum([p.numel() for p in model.parameters()]) - switch_additional_params\n",
    ")\n",
    "active_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "a48e940d-c7af-4367-81a9-3310531dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ce_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Computes cross-entropy loss.\n",
    "    Inputs:\n",
    "        -logits: Model output of shape (B, S, vocab_size)\n",
    "        -counts:\n",
    "    \"\"\"\n",
    "    B, S, C = logits.shape\n",
    "    logits = logits.view(B * S, C)\n",
    "    targets = targets.view(B * S)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "bdbf6e1a-3965-49ed-ac0e-9031dc9a5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aux_loss(counts, prob_sum):\n",
    "    \"\"\"\n",
    "    Computes Switch Transformer auxiliary loss.\n",
    "    Inputs:\n",
    "        -counts: Number of tokens passed to each expert in each switch layer (num_switch_layers x n_experts)\n",
    "        Note this is NOT equivalent to n_layer; num_switch_layers = (n_layer//2) + (n_layer % 2)\n",
    "        -prob_sum: Sum of probs across all tokens for each expert (num_switch_layers x n_experts)\n",
    "    \"\"\"\n",
    "\n",
    "    # total number of tokens routed in that layer\n",
    "    token_count = counts.sum(dim=-1, keepdims=True)\n",
    "\n",
    "    # prop of tokens dispatched to each expert\n",
    "    route_frac = counts / token_count\n",
    "\n",
    "    # fraction of total probability allocated for each expert\n",
    "    # recall prob_sum := softmaxed probs, which added to 1 across the experts for each token\n",
    "    # we divide by num_tokens so that the overall 2D scalar sum of prob_frac is 1\n",
    "    # intuitively we are forcing the total prob for each layer across the experts to be 1 so we can take proportions,\n",
    "    # the same way as above\n",
    "    prob_frac = prob_sum / token_count\n",
    "\n",
    "    # Auxiliary loss\n",
    "    # L = N \\sum_{i=1}^N f_i • P_i\n",
    "    aux_loss = N_EXPERT * (route_frac * prob_frac).sum()\n",
    "    return aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "bca27c6e-2d5f-4131-87fa-275d8bc35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    device,\n",
    "    train_loss_list=None,\n",
    "    val_loss_list=None,\n",
    "    train_time_list=None,\n",
    "    val_aux_loss_list=None,\n",
    "    dropped_list=None,\n",
    "):\n",
    "\n",
    "    train_losses = train_loss_list if train_loss_list is not None else []\n",
    "    val_losses = val_loss_list if val_loss_list is not None else []\n",
    "    train_times = train_time_list if train_time_list is not None else []\n",
    "    val_aux_losses = val_aux_loss_list if val_aux_loss_list is not None else []\n",
    "    dropped = dropped_list if dropped_list is not None else []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up prompt generation\n",
    "    generation_file_path = f\"{path}/outputs/OUTPUT_{MODEL_NAME}_SEED_{SEED}.txt\"\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "    cond_prompts = [\"KING TERRY: Thou art\", \"DANIEL: Ay, my dear,\"]\n",
    "\n",
    "    cond_token_list = [encode(prompt) for prompt in cond_prompts]\n",
    "\n",
    "    for step in range(MAX_ITERS):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs, targets = get_batch(\"train\")\n",
    "        if model.switch:\n",
    "            logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "            loss = calc_ce_loss(logits, targets)\n",
    "            aux_loss = calc_aux_loss(counts, prob_sum)\n",
    "            loss += AUX_LOSS_COEF * aux_loss\n",
    "            dropped.append(n_dropped)  # for logging purposes\n",
    "        else:\n",
    "            logits = model(inputs)\n",
    "            loss = calc_ce_loss(logits, targets)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        # Monitor gradient norm\n",
    "        grads = [\n",
    "            param.grad.detach().flatten()\n",
    "            for param in model.parameters()\n",
    "            if param.grad is not None\n",
    "        ]\n",
    "        norm = torch.cat(grads).norm()\n",
    "\n",
    "        train_time = time.perf_counter() - start\n",
    "        tokens_per_sec = (1 / train_time) * BATCH_SIZE * SEQ_LEN\n",
    "        train_times.append(tokens_per_sec)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training statistics\n",
    "        if step % PRINT_ITERS == 0 and step != 0:\n",
    "            print(\n",
    "                f\"Step {step}/{MAX_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.5f} |\",\n",
    "                f\"Grad Norm: {norm:.3f} | Running Avg Tokens/Sec: {np.mean(train_times):.3f}\",\n",
    "            )\n",
    "\n",
    "        # estimate val loss, generate text and save\n",
    "        if step % EVAL_ITERS == 0 and step != 0:\n",
    "            val_losses, val_aux_losses = estimate_loss(\n",
    "                model, val_losses, val_aux_losses\n",
    "            )\n",
    "            generate(model, generation_file_path, empty_tokens, cond_token_list, step)\n",
    "            model.train()\n",
    "\n",
    "        # save model, val losses (not train_losses), train times\n",
    "        if step % SAVE_ITERS == 0 and step != 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                f\"{path}/checkpoints/{MODEL_NAME}_STEP_{step}_SEED_{SEED}.pt\",\n",
    "            )\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "            ) as f:\n",
    "                json.dump(val_losses, f)\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_aux_losses.json\", \"w\"\n",
    "            ) as f2:\n",
    "                json.dump(val_aux_losses, f2)\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_times.json\", \"w\"\n",
    "            ) as f3:\n",
    "                json.dump(\n",
    "                    train_times[EVAL_ITERS::EVAL_ITERS], f3\n",
    "                )  # match freq of val_losses\n",
    "                # note this means if you load from checkpoint to continue training you will have a sparser train_times\n",
    "                # list in computing running avg\n",
    "\n",
    "            with open(\n",
    "                f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_dropped.json\", \"w\"\n",
    "            ) as f4:\n",
    "                json.dump(dropped[EVAL_ITERS::EVAL_ITERS], f4)  # same here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "e0abba78-5cd4-4ee7-b116-203fdeef3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_losses, val_aux_losses):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    aux_losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch(\"test\")\n",
    "        if model.switch:\n",
    "            logits, counts, prob_sum, n_dropped = model(inputs)\n",
    "            losses[k] = calc_ce_loss(logits, targets)\n",
    "            aux_losses[k] = calc_aux_loss(counts, prob_sum)\n",
    "            losses[k] += AUX_LOSS_COEF * aux_losses[k]\n",
    "        else:\n",
    "            losses[k] = calc_ce_loss(logits, targets)\n",
    "    val_loss, val_aux_loss = losses.mean().item(), aux_losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    val_aux_losses.append(val_aux_loss)  # track separate aux loss for logging\n",
    "    # keep model in eval, next call is to .generate() anyway\n",
    "    print(f\"Est. Val Loss: {val_loss:.5f} | Est. Aux Loss: {val_aux_loss:.5f}\")\n",
    "    return val_losses, val_aux_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "9a44ceb7-9dfe-4591-b7e4-b2486bb0eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, generation_file_path, empty_tokens, cond_token_list, step):\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    uncond_res1 = decode(\n",
    "        model.generate(empty_tokens, method=\"top-k\", k=5, max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    "    uncond_res2 = decode(\n",
    "        model.generate(\n",
    "            empty_tokens, method=\"nucleus\", p_nucleus=0.5, max_new_tokens=500\n",
    "        )[0].tolist()\n",
    "    )\n",
    "\n",
    "    cond_res_list = []\n",
    "    for prompt in cond_token_list:\n",
    "        cond_res = decode(\n",
    "            model.generate(\n",
    "                torch.tensor(prompt).unsqueeze(0).long().to(device),\n",
    "                method=\"top-k\",\n",
    "                k=5,\n",
    "                max_new_tokens=500,\n",
    "            )[0].tolist()\n",
    "        )\n",
    "        cond_res_list.append(cond_res)\n",
    "\n",
    "    cond_res_list = \"\\n\\n\".join(cond_res_list)\n",
    "\n",
    "    generation_text = f\"\"\"{MODEL_NAME} Output, Step {step}\n",
    "    UNCONDITIONAL GENERATION:\n",
    "\n",
    "    Top-k (5) (500 max_tokens):\n",
    "    {uncond_res1}\n",
    "\n",
    "    Nucleus (0.5) (500 max_tokens):\n",
    "    {uncond_res2}\n",
    "\n",
    "    #####################################################\n",
    "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
    "    {cond_res_list}\n",
    "    -----------------------------------------------------\n",
    "    \"\"\"\n",
    "    with open(generation_file_path, \"a\") as file:\n",
    "        file.write(generation_text)\n",
    "    print(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "e5840a0c-62b9-4011-a370-8cdf1ba4be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50/50000 | Running Avg Train Loss: 4.26320 | Grad Norm: 1.009 | Running Avg Tokens/Sec: 6688.876\n",
      "Step 100/50000 | Running Avg Train Loss: 3.80411 | Grad Norm: 1.250 | Running Avg Tokens/Sec: 6746.328\n",
      "Step 150/50000 | Running Avg Train Loss: 3.56538 | Grad Norm: 1.857 | Running Avg Tokens/Sec: 6763.554\n",
      "Step 200/50000 | Running Avg Train Loss: 3.39562 | Grad Norm: 1.342 | Running Avg Tokens/Sec: 6709.624\n",
      "Step 250/50000 | Running Avg Train Loss: 3.27404 | Grad Norm: 1.251 | Running Avg Tokens/Sec: 6643.860\n",
      "Est. Val Loss: 2.62356 | Est. Aux Loss: 2.01001\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 250\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "\n",
      "Are  coor oooto thelanttsst sond bateses m man m win m bes tounderthe an withilouneselle t thirer toulir seng t terllore bour athes w b wore ssessearate alllllese serol lallel soulland ssss wind t ararathilan s tor thind angor sens tene sthan anerouss arl s astele toung thit wer therer seras we wes thand boulathe alle s cis as athinde t the bend se woust thes tele t torelon winsousel bowinlland and ase as tat arorales toras llendarale aran thes arerarorllererore s te thalllllloulalessthel toron\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ing sother s athend s thinde out manenore t moun me s there the manderere ben s s thend thillle t t sore there s ther t ben courlare lesthe ce solllllleralll ler thesous souson lerour bll ans t s s sellelelel balerathell wisel tor thend t the tor sound wang t se an sthesend tor thalerone won ll atoular and the thelllle worl werourel l t t l athalatoulere t allllllellllleller the t worerel all bend tore chand lllllllll s bengansthand theror thalere sour lller llerous s lant tous merthanthe the th\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou artherst ke w at t s the d thatas we s t b borth t thisote we s t banelat as bal tothe bllle westhe wisteris anor t wourlel wangelealleralllllessss taralangelllllallore wind cher s sellangerounssoundesthengot t t t se boulan s son bat we blllant we wond,\n",
      "\n",
      "\n",
      "CAlland,\n",
      "Thille the sss the soure angoutorongel terlell t ben win atathelllllllllend walle sowis we tero bll t barous sel t be tourelesel withowotounsorelasthir t werouthalllarerares allin terenst tantathangoresst the tor sowathoundathan tesorlle\n",
      "\n",
      "DANIEL: Ay, my dear, we tsth warereeanourid d t thon won borele ander mandout alerteng bl b thestend be blengotherenor wire b thanest t wan sthe bllllilasoun asterer t athes sthil ar sesonensthen t thout se t bell stherenonout thas wis sestererarongand cer allllle balest ther tes bere tarasessorene alandatoun te talas waras bll alle w tellire towelastotour thelore send t towhane arenest wines wel astharond th boulliner sender thest w llaler bor we t arellat at wor bllles t welllourelen tonowol cator s bland wand te\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 300/50000 | Running Avg Train Loss: 3.18377 | Grad Norm: 0.772 | Running Avg Tokens/Sec: 6610.147\n",
      "Step 350/50000 | Running Avg Train Loss: 3.11229 | Grad Norm: 1.231 | Running Avg Tokens/Sec: 6577.654\n",
      "Step 400/50000 | Running Avg Train Loss: 3.05471 | Grad Norm: 0.962 | Running Avg Tokens/Sec: 6568.742\n",
      "Step 450/50000 | Running Avg Train Loss: 3.00631 | Grad Norm: 0.852 | Running Avg Tokens/Sec: 6539.872\n",
      "Step 500/50000 | Running Avg Train Loss: 2.96397 | Grad Norm: 1.061 | Running Avg Tokens/Sec: 6554.312\n",
      "Est. Val Loss: 2.46547 | Est. Aux Loss: 2.00401\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 500\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "Ans thaso hesong thes so withir wour mees winghal win me shast s senen t manear thon to soulorirer d\n",
      "An miseng t teallase m mandites m o wore shes darore mall te s se han thyoun tho toulllll mind t tharathil thowis t s wine wor send ther seran dandend.\n",
      "\n",
      "\n",
      "IUSENGURGENENGUThe E:\n",
      "Wer therer meras we wes thand bo myour the thar s te me shour t wherend se woust thes te wathere\n",
      "And windise win winorind we dase me th minoranth to tharind wenge toun d\n",
      "As mendarore tishowe s te thalleller t thesthin d the\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "Whe meat hes me tore the tofous me therore soure tond hend this man there the s sererer thand me t the is there shand to f thaserourere\n",
      "\n",
      "The thas s s shasore de mather t souso theso thesor mare s thas wo thatharon se\n",
      "Whe me the she d thane de tor sour s wharo se on our thendes meso than then me mond thes seseseserd\n",
      "\n",
      "Whe s wourer we s there there s marousord s s wen thashe so thase we tous thandend tore mes the thas se s dond mathand the meror were sour s mengonde d the mand d s meron tore me d t\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art sesouse wastit s thind thato wone thin tore, teshis te we s thithalll menes\n",
      "Wor my wand and the wis aris t or toularond wenge s sthe ond seshishe t share\n",
      "Ther tour tingr s this teand meroun tounde mas towat d thatharis the son math thando merer thilland w mandere than the she the soure weris\n",
      "Whinden thelell t sto who marendellld thasher mane sowis we ouro tor mienenes se s thy tilonde s tisthowond myore te\n",
      "Wourer s be thericherares mende terene t manger thereser sthithind\n",
      "\n",
      "Whisendou mathes my t\n",
      "\n",
      "DANIEL: Ay, my dear, we ther ware, thes wind wer mone on torere matere anto m aler wer than moustend te t isof sere\n",
      "Ant wis s thane mer t o she thare t owoun owe ser tilares\n",
      "For t me he s whithen mithou the t this shelontont t thas wiseres wisengong\n",
      "Bu ter warathe malest ther tes sere theases whene wen soust the tharo thowise t male d me tie these matord w the myousend titowhane m tendo wines we t shas onder thye sther mender theso wind mer d me malend me withano th s s me malllowiterd\n",
      "Tondan sha se tinondowonder, \n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 550/50000 | Running Avg Train Loss: 2.92532 | Grad Norm: 1.029 | Running Avg Tokens/Sec: 6543.461\n",
      "Step 600/50000 | Running Avg Train Loss: 2.88968 | Grad Norm: 0.833 | Running Avg Tokens/Sec: 6539.411\n",
      "Step 650/50000 | Running Avg Train Loss: 2.85716 | Grad Norm: 0.768 | Running Avg Tokens/Sec: 6529.158\n",
      "Step 700/50000 | Running Avg Train Loss: 2.82681 | Grad Norm: 0.840 | Running Avg Tokens/Sec: 6522.984\n",
      "Step 750/50000 | Running Avg Train Loss: 2.79871 | Grad Norm: 0.738 | Running Avg Tokens/Sec: 6508.906\n",
      "Est. Val Loss: 2.31441 | Est. Aux Loss: 2.00828\n",
      "switch_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 750\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "And this bers tour thant the hend be mees\n",
      "But war win thiss to me mane t mane mard\n",
      "Monge the tith thes is meande that\n",
      "Whe mour athes me soure shes ticken ash the bo thit these\n",
      "Thours thish son bo tith bathes tho tor thind tower seas ther sur sulleres\n",
      "And this stee tof my thie ber thorst mears be tral ange\n",
      "Wit tour the thar sute he bure the tof sus he ous\n",
      "Tout wis wat tore the tof se dre dint thange ston be tat\n",
      "To be me tof shor thar bee she mand arit,\n",
      "The tish shee to be sthie bul thesthe ound b\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "ARUS:\n",
      "Wher we the thand the the mand tot me mere torer tore the man there the send\n",
      "Bo the me the than the the at the tof shat sheare ang we my she thent sof\n",
      "The mat sour she be ther theas a s the thald the se bererd ther wit\n",
      "There wit hat the the sof se he wof me shears\n",
      "Wof mer thare me here me mend thease the thend be the\n",
      "Theat bear ther my thas therere win be ben thase ond thas\n",
      "The the mas wous me mean so and me ber songe thathe theall ther sof\n",
      "Ther me mer sour dous thand be as ben hare the he\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou art, bord me thant shes to that suse sin my mes\n",
      "And, thote he she manth, tour so to my thourd thing there\n",
      "This that wou tea s the bust, thoulless, tharthe, wen tou sor tithe a bule teand ter\n",
      "Terange mim sha the the se bour the sof mat be be o mered this\n",
      "And thous mit the tere she the soured this\n",
      "Whinthene hit sthast bur bearenderers thas brices\n",
      "Wofas mat ourd of sour mimas all the tou me simy\n",
      "Athom thous mar sour tror tout there thares as\n",
      "Is thatest than thangerter thes tof ho thises\n",
      "Whart than my \n",
      "\n",
      "DANIEL: Ay, my dear, woutst, ware, ther be my trat, tho wistere ane my anto meat wing tor\n",
      "Thoul than be the son trend to mat sund tha ton so the blle thow,\n",
      "Thyo whe thinghes sthinger he she to he mith be se tof mend were thast thaser,\n",
      "I me waseng my wis mis that the berices, thean ane hard.\n",
      "\n",
      "GUCINGENGUS:\n",
      "Tou the thare tho son thar the me the the and,\n",
      "And wath tore send tof bener a tices beres o be ang thof thande\n",
      "Whyom mende tho sof merse mour as thend meat thand, bus bung, thind,\n",
      "Wheath mean hou se tinth hous trow\n",
      "    -----------------------------------------------------\n",
      "    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[810], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Driver code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[807], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device, train_loss_list, val_loss_list, train_time_list, val_aux_loss_list, dropped_list)\u001b[0m\n\u001b[1;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calc_ce_loss(logits, targets)\n\u001b[1;32m     38\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m  \u001b[38;5;66;03m# Monitor gradient norm\u001b[39;00m\n\u001b[1;32m     42\u001b[0m grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     43\u001b[0m         param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fbb65-5f1b-4756-aa5c-2b35b68bb2eb",
   "metadata": {},
   "source": [
    "## We see similar training speed, but we have 200,000 more parameters to work with.\n",
    "\n",
    "### it remains to be seen if our model is more sample-efficient. Our loss now includes an auxiliary loss which inflates the numbers in comparison to a vanilla transformer. For efficient comparison we subtract the aux loss * its coefficient (0.01), but unfortunately it seems like the loss is still slightly higher than a vanilla transformer and its speed slightly slower. More investigation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95154-d641-4d9b-9e8a-c054d5921921",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5157dd6-d6d6-42af-9104-272094033ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "772a7473-2e8f-4c61-87d9-612e01150b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRY: thou arte a my she\n",
      "Which have may and of contain.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good as I tall no knrow, for shalt agarnt\n",
      "And mpo; and Kong a m, not outhpile Mesce.\n",
      "\n",
      "HENRY VI:\n",
      "When I will thy lookess, oner the pexstrey\n",
      "The the the hee voagh gresed livioe.\n",
      "\n",
      "MENCIO:\n",
      "My her callis his peaced of to that\n",
      "We where's by shall bore: as shall myselvea\n",
      "The plender feuls!\n",
      "\n",
      "PAPELLANT:\n",
      "In the the into balby me dods to love,\n",
      "In but the giving of nyou ase. I tall it-me?'e Goveuling\n",
      "The theer haught art praver count madeng Camen:\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"TERRY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(\n",
    "    decode(\n",
    "        model.generate(torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
