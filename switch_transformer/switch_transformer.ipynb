{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d40f5-2bc2-4a3c-ad20-f973978ff1b2",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b05b899-86b9-4414-9f80-e197b9d3d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "root = '../'\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 128\n",
    "MAX_ITERS = 50000 # max num batches to train\n",
    "PRINT_ITERS = 50 # frequency to print train loss\n",
    "EVAL_ITERS = 250 # frequency to evaluate val loss and generate text from model\n",
    "EVAL_ITER_COUNT = 500 # number of batches to estimate val loss with\n",
    "SAVE_ITERS = 1000 # frequency to save model and losses\n",
    "N_EMBD = 128\n",
    "N_FF = N_EMBD * 4\n",
    "N_HEAD = 4\n",
    "N_LAYER = 4\n",
    "\n",
    "MODEL_NAME = f\"switch_{N_LAYER}_LAYERs_{N_HEAD}_HEAD_{N_EMBD}_EMBD_DIM_{SEQ_LEN}_SEQ_LEN\"\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b757be9-b268-493c-88a1-53b95db6e9a4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6e1107-9998-4282-a8ca-0636e3f8e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb48bc4-c538-4b31-add1-2750a08bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils import set_seed, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686a47d1-9b20-455d-8775-774bff55aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff0156-4f9f-4936-92eb-894edc90ed5f",
   "metadata": {},
   "source": [
    "# Model\n",
    "#### (TEMP). once validated, move into /models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe4061-1809-4aac-b161-677fd2687b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, n_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(n_ff, n_embd),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089f347-a15c-4fae-8940-dac44e92129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head # Dimension of each head's key, query, and value\n",
    "        \n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd, bias=False)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, D = x.size()\n",
    "        # split dimension into n_head * head_dim, then transpose the sequence length w/ n_head\n",
    "        # output: [B, n_head, S, head_dim]\n",
    "        return x.view(B, S, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, S, head_dim = x.size() # _ is n_head which we will merge\n",
    "        # output: [B, S, n_embd]\n",
    "        return x.transpose(1, 2).contiguous().view(B, S, self.n_embd)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, dropout, mask=None):\n",
    "        # q,k,v are [B, n_head, S, head_dim]\n",
    "        # wei = [B, n_head, S, S]\n",
    "        wei = q @ k.transpose(-2,-1) / np.sqrt(self.head_dim)\n",
    "        # mask is [B, 1, S, S]\n",
    "        if mask is not None:\n",
    "          wei = wei.masked_fill(mask, float('-inf'))\n",
    "        wei = dropout(F.softmax(wei, dim=-1))\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, S, n_embd)\n",
    "        # Step 1 and 2: Project full query, key, value, then split via reshaping\n",
    "        q = self.split_heads(self.query(x))\n",
    "        k = self.split_heads(self.key(x))\n",
    "        v = self.split_heads(self.value(x))\n",
    "\n",
    "        # Step 3: Compute scaled dot-product attention with causal mask\n",
    "        attn = self.scaled_dot_product(q, k, v, self.drop, mask)\n",
    "\n",
    "        # Step 4 and 5: Concatenate attention scores, return projected output matrix\n",
    "        out = self.out(self.combine_heads(attn)) # (B, S, n_embd)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219f573-b54a-4e3b-9f32-181494db1ed1",
   "metadata": {},
   "source": [
    "### TODO: Switch Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373280e-e1e6-4139-be28-b21ca942c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SwitchFeedForward(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  d_model,\n",
    "#                  n_ff,\n",
    "#                  capacity_factor,\n",
    "#                  drop_tokens: bool,\n",
    "#                  n_experts,\n",
    "#                  expert: MLP,\n",
    "#                  dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.capacity_factor = capacity_factor\n",
    "#         self.n_experts = n_experts\n",
    "#         self.drop_tokens = drop_tokens\n",
    "        \n",
    "#         self.experts = nn.ModuleList([copy.deepcopy(expert(d_model,\n",
    "#                                                           n_ff,\n",
    "#                                                           dropout) for _ in range(n_experts)])\n",
    "\n",
    "#         # Routing layer\n",
    "#         self.switch = nn.Linear(d_model, n_experts)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # determine x shape\n",
    "\n",
    "#         # pass through self.switch then softmax\n",
    "#         # take torch.max dim = -1 to return max val and indices\n",
    "#         # check capacity and drop tokens\n",
    "#         # feed tokens to relevant experts\n",
    "#         # obtain output from experts and identity from dropped tokens, scale with gated probs\n",
    "#         # return output, + other metadata for loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f36b92-e686-46f7-8a78-5b5e4578c7a9",
   "metadata": {},
   "source": [
    "## TODO: Switch Block\n",
    "\n",
    "#### does Switch alternate regular MLP with SwitchFeedForward? maybe not\n",
    "#### could offer that functionality though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36f9f7-8736-4655-8e1d-7e2e4d2861c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO\n",
    "\n",
    "# class SwitchBlock(nn.Module):\n",
    "#     def __init__(self, n_embd, n_head, n_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.sa = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        # self.mlp = SwitchFeedForward(n_embd, n_ff,\n",
    "        #                              capacity_factor, \n",
    "        #                              drop_tokens,\n",
    "        #                              n_experts,\n",
    "        #                              expert=MLP,\n",
    "        #                              dropout=dropout)\n",
    "#         self.ln1 = nn.LayerNorm(n_embd)\n",
    "#         self.ln2 = nn.LayerNorm(n_embd)\n",
    "#         self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         # residual connection (stream)\n",
    "#         # pre layer norm\n",
    "#         x = x + self.drop(self.sa(self.ln1(x), mask))\n",
    "#         x = x + self.drop(self.mlp(self.ln2(x)))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bc573-2383-47d6-b8ca-cfbe78945952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  \"\"\"\n",
    "  Formula taken from the original Transformer paper:\n",
    "  PE(pos, 2i (even)) = sin(pos/(10000^{2i/d_model}))\n",
    "  PE(pos, 2i+1 (odd)) = cos(pos/(10000^{2i/d_model}))\n",
    "\n",
    "  See reference for more details:\n",
    "  https://kikaben.com/transformers-positional-encoding/\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model, max_len):\n",
    "      # just set d_model = n_embd and max_len = seq_len\n",
    "      super().__init__()\n",
    "\n",
    "      position = torch.arange(max_len).unsqueeze(1) # [max_len, 1]\n",
    "      divisor = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model)) # [d_model / 2, half for each of sin and cos]\n",
    "      pe = torch.zeros(max_len, d_model)\n",
    "      pe[:, 0::2] = torch.sin(position * divisor)\n",
    "      pe[:, 1::2] = torch.cos(position * divisor)\n",
    "      self.register_buffer('pe', pe) # result: self.pe = [max_len, d_model], mapping each token index to a vector of length d_model as desired\n",
    "\n",
    "  def forward(self, x):\n",
    "      # x = torch.arange(seq_length) has shape [seq_length], so x.size(0) extracts it, then we index self.pe for the first seq_length mappings\n",
    "      # note we do not add the positional embeddings to x itself yet, we simply return them\n",
    "      # output = (seq_length, d_model=n_embd)\n",
    "      return self.pe[:x.size(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f0aa8-2a74-4c24-baaf-7d494a0b9b47",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "-Selective precision  \n",
    "-Smaller weight initialization  \n",
    "-Separate higher expert dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fdc10-9ee4-45f4-bcb8-0dbfda2029c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO\n",
    "\n",
    "# class SwitchTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size, seq_length,\n",
    "#                  n_embd, n_head, n_ff, n_layer,\n",
    "#                  device, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "#         self.position_embedding = PositionalEncoding(n_embd, seq_length)\n",
    "\n",
    "#         self.blocks = nn.Sequential(*[Block(n_embd,\n",
    "#                                             n_head,\n",
    "#                                             n_ff,\n",
    "#                                             dropout) for _ in range(n_layer)])\n",
    "#         self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "#         self.drop = nn.Dropout(dropout)\n",
    "#         self.seq_length = seq_length\n",
    "#         self.device = device\n",
    "#         self.init_params()\n",
    "\n",
    "#     # weight initialization (Xavier uniform)\n",
    "#     def init_params(self, default_initialization=False):\n",
    "#         if not default_initialization:\n",
    "#             for name, p in self.named_parameters():\n",
    "#                 if p.dim() > 1:\n",
    "#                     nn.init.xavier_uniform_(p)\n",
    "\n",
    "#     # Remark: Xavier normal is not supported at this time.\n",
    "\n",
    "#     def get_causal_mask(self,  x):\n",
    "#         \"\"\"\n",
    "#         Generates causal mask for decoding\n",
    "#         \"\"\"\n",
    "#         B, S = x.shape # x = (batch_size x seq_len)\n",
    "#         attn_shape = (B, 1, S, S)\n",
    "#         subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') # k = 1 shifts the diagonal, so that the main diagonal gets 0's\n",
    "#         return (torch.from_numpy(subsequent_mask) == 0).to(self.device)\n",
    "#         # True along main diagonal + below, False elsewhere\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = x.to(torch.int64)\n",
    "#         B, S = x.shape\n",
    "\n",
    "#         # get mask\n",
    "#         mask = self.get_causal_mask(x).to(self.device)\n",
    "#         # mask = (B x 1 x S x S)\n",
    "\n",
    "#         tok_emb = self.token_embedding(x)\n",
    "#         pos_emb = self.position_embedding(torch.arange(S))\n",
    "#         x = self.drop(tok_emb + pos_emb)\n",
    "#         # (B, S, n_embd)\n",
    "#         for block in self.blocks:\n",
    "#             x = block(x, ~mask) # (B, S, n_embd)\n",
    "#         # negate mask to fill originally False values with -inf later\n",
    "#         logits = self.lm_head(x) # (B, S, vocab_size)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "\n",
    "#     def generate(self, input_ids, method='multinomial',\n",
    "#                  max_new_tokens=1000, temp=None,\n",
    "#                  num_beams=None, p_nucleus=None, k=None):\n",
    "\n",
    "#         # input_ids begins as (B, S)\n",
    "#         self.eval()\n",
    "\n",
    "#         for _ in range(max_new_tokens):\n",
    "#             if method in ['multinomial', 'temperature', 'greedy', 'nucleus', 'top-k']:\n",
    "#                 # i) Truncate to the most recent `max length` tokens\n",
    "#                 text_cond = input_ids[:, -self.seq_length:]\n",
    "#                 # ii) Retrieve predictions\n",
    "#                 with torch.no_grad():\n",
    "#                     logits = self(text_cond)\n",
    "#                 # model output: (B, S, vocab_size)\n",
    "#                 # iii) Find last token logits of each\n",
    "#                 logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "#                 # if temperature sampling, divide logits by temp before applying softmax\n",
    "#                 if method == 'temperature':\n",
    "#                     logits = logits / temp\n",
    "\n",
    "#                 # iv) Take softmax along each\n",
    "#                 probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#                 # v) Sample next token depending on method\n",
    "#                 if method == 'greedy':\n",
    "#                     next_idx = probs.argmax(dim=-1).unsqueeze(-1)\n",
    "\n",
    "#                 elif method in ['multinomial', 'temperature', 'nucleus', 'top-k']:\n",
    "#                     if method == 'nucleus':\n",
    "#                         assert p_nucleus is not None and (0 < p_nucleus) and (p_nucleus <= 1)\n",
    "\n",
    "#                         sorted_probs, sorted_idx = probs.sort(dim=-1, descending=True)\n",
    "#                         prob_cumsum = sorted_probs.cumsum(dim=-1)\n",
    "#                         idx_remove = prob_cumsum > p_nucleus\n",
    "#                         # shift one right to ensure the first token is above the threshold\n",
    "#                         idx_remove[..., 1:] = idx_remove[..., :-1].clone()\n",
    "#                         idx_remove[..., 0] = False\n",
    "#                         # retrieve original indices by reverse-sorting\n",
    "#                         remove_mask = idx_remove.gather(dim=-1,\n",
    "#                                           index=sorted_idx.argsort(dim=-1))\n",
    "#                         # ^ specifically, we do this by first argsorting the indices which were returned from argsort\n",
    "#                         # you can show that this returns indices that when used to subset a sorted array, returns the original array in unsorted order\n",
    "#                         # https://stackoverflow.com/questions/52127723/pytorch-better-way-to-get-back-original-tensor-order-after-torch-sort\n",
    "#                         probs[remove_mask] = 0\n",
    "\n",
    "#                     if method == 'top-k':\n",
    "#                         remove_mask = probs < torch.topk(probs, k).values[..., -1, None] # topk returns (B, 1), leaving only the\n",
    "#                         # kth largest probs (i.e. the cutoff value for each). Then mask is same size as probs (B, vocab_size)\n",
    "#                         probs[remove_mask] = 0\n",
    "\n",
    "#                     # Sample probabilistically via scores\n",
    "#                     next_idx = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "#                 # vi) Autoregressively append to input_text\n",
    "#                 input_ids = torch.cat((input_ids, next_idx), dim=-1)\n",
    "\n",
    "#                 # now input_text = (B, S + 1)\n",
    "        \n",
    "#         return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713212-4a60-4e60-90f4-f6e0a704a299",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b6d9fe-8f18-474d-aefb-4e44fd0b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{root}/data/tiny-shakespeare.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f'Vocab: {chars}')\n",
    "print(f'Vocab size: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545415c0-5e49-42c9-b10a-1432028233b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 47, 52, 63, 7, 57, 46, 39, 49, 43, 57, 54, 43, 39, 56, 43, 1, 47, 57, 1, 57, 47, 41, 49]\n",
      "tiny-shakespeare is sick\n"
     ]
    }
   ],
   "source": [
    "# Prepare mappings / tokenizer\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx2txt = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [txt2idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx2txt[i] for i in l])\n",
    "\n",
    "print(encode(\"tiny-shakespeare is sick\"))\n",
    "print(decode(encode(\"tiny-shakespeare is sick\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0124c4-58be-4acd-8249-fd9e5cecbaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data len: 1003854 val_data len: 111540\n"
     ]
    }
   ],
   "source": [
    "# tokenizer data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 90-10 split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print('train_data len:', len(train_data), 'val_data len:', len(val_data))\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - SEQ_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+SEQ_LEN] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+SEQ_LEN+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af611b-be70-4737-84df-f7bf13b057c9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb0eb6-b509-4da1-b361-e34684177abe",
   "metadata": {},
   "source": [
    "### TODO: tweak hyperparameters to keep effective parameter count or FLOP count constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "679ef31f-5253-49f9-9e24-c64b05655cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "# model = SwitchTransformer(VOCAB_SIZE, SEQ_LEN,\n",
    "#                  N_EMBD, N_HEAD, N_FF, N_LAYER,\n",
    "#                  device, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eacebc5-8b3a-4c5d-9c89-f946fd25e948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "VanillaTransformer                       --\n",
       "├─Embedding: 1-1                         8,320\n",
       "├─PositionalEncoding: 1-2                --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Block: 2-1                        --\n",
       "│    │    └─MultiHeadAttention: 3-1      65,536\n",
       "│    │    └─MLP: 3-2                     131,712\n",
       "│    │    └─LayerNorm: 3-3               256\n",
       "│    │    └─LayerNorm: 3-4               256\n",
       "│    │    └─Dropout: 3-5                 --\n",
       "│    └─Block: 2-2                        --\n",
       "│    │    └─MultiHeadAttention: 3-6      65,536\n",
       "│    │    └─MLP: 3-7                     131,712\n",
       "│    │    └─LayerNorm: 3-8               256\n",
       "│    │    └─LayerNorm: 3-9               256\n",
       "│    │    └─Dropout: 3-10                --\n",
       "│    └─Block: 2-3                        --\n",
       "│    │    └─MultiHeadAttention: 3-11     65,536\n",
       "│    │    └─MLP: 3-12                    131,712\n",
       "│    │    └─LayerNorm: 3-13              256\n",
       "│    │    └─LayerNorm: 3-14              256\n",
       "│    │    └─Dropout: 3-15                --\n",
       "│    └─Block: 2-4                        --\n",
       "│    │    └─MultiHeadAttention: 3-16     65,536\n",
       "│    │    └─MLP: 3-17                    131,712\n",
       "│    │    └─LayerNorm: 3-18              256\n",
       "│    │    └─LayerNorm: 3-19              256\n",
       "│    │    └─Dropout: 3-20                --\n",
       "├─Linear: 1-4                            8,385\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 807,745\n",
       "Trainable params: 807,745\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd0479-ed4f-454c-b6b3-59bce94101d3",
   "metadata": {},
   "source": [
    "## TODO: add auxiliary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48e940d-c7af-4367-81a9-3310531dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO\n",
    "# def calc_loss(logits, targets):\n",
    "#     B, S, C = logits.shape\n",
    "#     logits = logits.view(B*S, C)\n",
    "#     targets = targets.view(B*S)\n",
    "#     loss = F.cross_entropy(logits, targets)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "af0a898b-5f96-44da-ac41-9a23616ebfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/10000 | Loss: 6.962159633636475\n",
      "Step 50/10000 | Loss: 3.401878595352173\n",
      "Step 100/10000 | Loss: 3.175950765609741\n",
      "Step 150/10000 | Loss: 2.812167167663574\n",
      "Step 200/10000 | Loss: 2.6810946464538574\n",
      "Step 250/10000 | Loss: 2.664418935775757\n",
      "Step 300/10000 | Loss: 2.659294366836548\n",
      "Step 350/10000 | Loss: 2.694620370864868\n",
      "Step 400/10000 | Loss: 2.6138129234313965\n",
      "Step 450/10000 | Loss: 2.5488266944885254\n",
      "Step 500/10000 | Loss: 2.5062320232391357\n",
      "Step 550/10000 | Loss: 2.4803411960601807\n",
      "Step 600/10000 | Loss: 2.4527230262756348\n",
      "Step 650/10000 | Loss: 2.38985013961792\n",
      "Step 700/10000 | Loss: 2.3893115520477295\n",
      "Step 750/10000 | Loss: 2.3820767402648926\n",
      "Step 800/10000 | Loss: 2.32847261428833\n",
      "Step 850/10000 | Loss: 2.3164374828338623\n",
      "Step 900/10000 | Loss: 2.2766780853271484\n",
      "Step 950/10000 | Loss: 2.297649383544922\n",
      "Step 1000/10000 | Loss: 2.2336761951446533\n",
      "Step 1050/10000 | Loss: 2.255802869796753\n",
      "Step 1100/10000 | Loss: 2.123323440551758\n",
      "Step 1150/10000 | Loss: 2.198436737060547\n",
      "Step 1200/10000 | Loss: 2.165727376937866\n",
      "Step 1250/10000 | Loss: 2.104663133621216\n",
      "Step 1300/10000 | Loss: 2.106135129928589\n",
      "Step 1350/10000 | Loss: 2.0613210201263428\n",
      "Step 1400/10000 | Loss: 2.0755412578582764\n",
      "Step 1450/10000 | Loss: 2.054353952407837\n",
      "Step 1500/10000 | Loss: 1.9930700063705444\n",
      "Step 1550/10000 | Loss: 2.0012974739074707\n",
      "Step 1600/10000 | Loss: 2.0864081382751465\n",
      "Step 1650/10000 | Loss: 2.019371747970581\n",
      "Step 1700/10000 | Loss: 1.9948196411132812\n",
      "Step 1750/10000 | Loss: 1.9606355428695679\n",
      "Step 1800/10000 | Loss: 1.9251341819763184\n",
      "Step 1850/10000 | Loss: 1.913225769996643\n",
      "Step 1900/10000 | Loss: 1.843820333480835\n",
      "Step 1950/10000 | Loss: 1.9268293380737305\n",
      "Step 2000/10000 | Loss: 1.9561188220977783\n",
      "Step 2050/10000 | Loss: 1.885495662689209\n",
      "Step 2100/10000 | Loss: 1.815986156463623\n",
      "Step 2150/10000 | Loss: 1.8532180786132812\n",
      "Step 2200/10000 | Loss: 1.8662980794906616\n",
      "Step 2250/10000 | Loss: 1.827746868133545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss(logits, targets)\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[103], line 54\u001b[0m, in \u001b[0;36mVanillaTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# (B, S, n_embd)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, n_embd)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# negate mask to fill originally False values with -inf later\u001b[39;00m\n\u001b[1;32m     56\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, S, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[101], line 14\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# residual connection (stream)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# pre layer norm\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x), mask))\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 13\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/moe-kit/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "for step in range(10000): # around 50 min per 10000 steps, so 3.3 batches/sec = 53 samples per sec = 6780 char/s\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    inputs, targets = get_batch('train')\n",
    "    logits = model(inputs)\n",
    "    loss = calc_loss(logits, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % PRINT_ITER == 0:\n",
    "        print(f\"Step {step}/10000 | Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff60e40-9e28-4b4f-b8b9-75a758875531",
   "metadata": {},
   "source": [
    "### TODO: functions now work. but need to reconsider freq. to save train times. don't want lit every step. maybe same freq as EVAL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e489a443-2b22-41db-967a-7ca8c6fc3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "model = VanillaTransformer(VOCAB_SIZE, SEQ_LEN,\n",
    "                 N_EMBD, N_HEAD, N_FF, N_LAYER,\n",
    "                 device, dropout=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca27c6e-2d5f-4131-87fa-275d8bc35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, device, \n",
    "          train_loss_list=None, val_loss_list=None, train_time_list=None):\n",
    "\n",
    "    train_losses = train_loss_list if train_loss_list is not None else []\n",
    "    val_losses = val_loss_list if val_loss_list is not None else []\n",
    "    train_times = train_time_list if train_time_list is not None else []\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    # Set up prompt generation\n",
    "    generation_file_path = f\"{path}/outputs/OUTPUT_{MODEL_NAME}_SEED_{SEED}.txt\"\n",
    "    empty_tokens = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "    cond_prompts = [\"KING TERRY: Thou art\",\n",
    "                    \"DANIEL: Ay, my dear,\"]\n",
    "\n",
    "    cond_token_list = [encode(prompt) for prompt in cond_prompts]\n",
    "\n",
    "    for step in range(MAX_ITERS):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        inputs, targets = get_batch('train')\n",
    "        logits = model(inputs)\n",
    "        loss = calc_loss(logits, targets)\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "         # Monitor gradient norm\n",
    "        grads = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "        norm = torch.cat(grads).norm()\n",
    "\n",
    "        train_time = time.perf_counter()-start\n",
    "        tokens_per_sec = (1/train_time) * BATCH_SIZE * SEQ_LEN\n",
    "        train_times.append(tokens_per_sec)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training statistics\n",
    "        if step % PRINT_ITERS == 0 and step != 0:\n",
    "            print(f\"Step {step}/{MAX_ITERS} | Running Avg Train Loss: {np.mean(train_losses):.5f} |\",\n",
    "                  f\"Grad Norm: {norm:.3f} | Running Avg Tokens/Sec: {np.mean(train_times):.3f}\")\n",
    "\n",
    "        # estimate val loss, generate text and save\n",
    "        if step % EVAL_ITERS == 0 and step != 0:\n",
    "            val_losses = estimate_loss(model, val_losses)\n",
    "            generate(model, generation_file_path, empty_tokens, cond_token_list, step)\n",
    "            model.train()\n",
    "\n",
    "        # save model, val losses (not train_losses), train times\n",
    "        if step % SAVE_ITERS == 0 and step != 0:\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()},\n",
    "                f'{path}/checkpoints/{MODEL_NAME}_STEP_{step}_SEED_{SEED}.pt')\n",
    "            \n",
    "        with open(f'{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json', 'w') as f:\n",
    "            json.dump(val_losses, f)\n",
    "\n",
    "        with open(f'{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_times.json', 'w') as f2:\n",
    "            json.dump(train_times[EVAL_ITERS::EVAL_ITERS], f2) # match freq of val_losses\n",
    "            # note this means if you load from checkpoint to continue training you will have a sparser train_times\n",
    "            # list in computing running avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0abba78-5cd4-4ee7-b116-203fdeef3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, val_losses):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_ITER_COUNT)\n",
    "    for k in range(EVAL_ITER_COUNT):\n",
    "        inputs, targets = get_batch('test')\n",
    "        logits = model(inputs)\n",
    "        losses[k] = calc_loss(logits, targets).item()\n",
    "    val_loss = losses.mean().item()\n",
    "    val_losses.append(val_loss)\n",
    "    # keep model in eval, next call is to .generate() anyway\n",
    "    print(f\"Est. Val Loss: {val_loss:.5f}\")\n",
    "    return val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a44ceb7-9dfe-4591-b7e4-b2486bb0eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, generation_file_path, empty_tokens, cond_token_list, step):\n",
    "\n",
    "    set_seed(42)\n",
    "    \n",
    "    uncond_res1 = decode(model.generate(empty_tokens,\n",
    "                                        method='top-k',\n",
    "                                        k=5,\n",
    "                                        max_new_tokens=500)[0].tolist())\n",
    "    uncond_res2 = decode(model.generate(empty_tokens,\n",
    "                                        method='nucleus',\n",
    "                                        p_nucleus=0.5,\n",
    "                                        max_new_tokens=500)[0].tolist())\n",
    "\n",
    "    cond_res_list = []\n",
    "    for prompt in cond_token_list:\n",
    "        cond_res = decode(model.generate(torch.tensor(prompt).unsqueeze(0).long().to(device),\n",
    "                      method='top-k', k=5,\n",
    "                      max_new_tokens=500)[0].tolist())\n",
    "        cond_res_list.append(cond_res)\n",
    "    \n",
    "    cond_res_list = '\\n\\n'.join(cond_res_list)\n",
    "    \n",
    "    generation_text = f\"\"\"{MODEL_NAME} Output, Step {step}\n",
    "    UNCONDITIONAL GENERATION:\n",
    "\n",
    "    Top-k (5) (500 max_tokens):\n",
    "    {uncond_res1}\n",
    "\n",
    "    Nucleus (0.5) (500 max_tokens):\n",
    "    {uncond_res2}\n",
    "\n",
    "    #####################################################\n",
    "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
    "    {cond_res_list}\n",
    "    -----------------------------------------------------\n",
    "    \"\"\"\n",
    "    with open(generation_file_path, 'a') as file:\n",
    "      file.write(generation_text)\n",
    "    print(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5840a0c-62b9-4011-a370-8cdf1ba4be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/50000 | Running Avg Train Loss: 6.24023 | Grad Norm: 20.527 | Running Avg Tokens/Sec: 2993.337\n",
      "Step 50/50000 | Running Avg Train Loss: 4.21355 | Grad Norm: 1.394 | Running Avg Tokens/Sec: 6741.455\n",
      "Step 100/50000 | Running Avg Train Loss: 3.77272 | Grad Norm: 2.327 | Running Avg Tokens/Sec: 6705.369\n",
      "Est. Val Loss: 3.10417\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 100\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "  ae  t ae e ha t ses sthist sh toue hee th   h at s sh hes ho s he th a  as hi thon enes t t t ha ate h  seat s teatothe  tha hh ta h h s\n",
      "\n",
      "e shes harot  a he t he s hhh  aresh  thoshe  sh s\n",
      "h ho t hhhe hhea h sh s h s hhhe hoth  he he\n",
      "\n",
      " s\n",
      "hah ha hos s ahh he s\n",
      "heh  hhho hahe he  thha t he at   toth tha a hothhar se  h h h s a a th ho h t hh h sh she anhan h s tha  t  the he han\n",
      "\n",
      "he h s h har a soh ha t ah ha   a ha th tos shor h arhe thohh  tho arorarohe\n",
      "\n",
      "\n",
      " hos\n",
      "\n",
      "\n",
      "ho  thar h at\n",
      "he thos hhe t\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "e t  othe  e  torolos th the she t th s e t t th  athahe t t l mos eehers es th l lihe seshes h t e sh as seseas thas a seh th s t he e\n",
      " es h thhe is sh soth ihe ha hh h shhan theoo   has a s t s s he athae t s thos th h h t e thar h h at the  he sou s  h aro he  e hat th he s   th i anehehe e thon she a s the he th eo t houe\n",
      "athaeo h t t  thor  he hane the\n",
      " os he  thashohor hous\n",
      " th eohar   hath  the a sh t h aroh se s i he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "an hhh he\n",
      "hahhe he s h th heothe\n",
      "on s s  athe ah  e h shathe  he ho\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou artte s h t s at t s hhheh thho h se s a h  t th t th w te he e t ha th h as es h ssss haehaeass th  shathhe a hhh  a a hes hanhe haheheahohh  tho hearan\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "he t he\n",
      " ahhh h t  s hehha teho t\n",
      "hohe h ha thot t h th a horhan   s a hath  hah a h thehoh hha h   the h h thhe the sh athe s\n",
      "\n",
      "he a anotheo thohe h the t he   hhhe\n",
      "\n",
      "\n",
      " tare s t\n",
      "ahhha  h hoso s   t  t\n",
      "\n",
      " tot h t  heo he h  h hho  ahe ar a thos\n",
      "\n",
      "athot\n",
      "a hath  he haro th  shheheare\n",
      "hat\n",
      "\n",
      "a thathat  a s thhhat that hho the ho aha he\n",
      "athha t s h h \n",
      "\n",
      "DANIEL: Ay, my dear, te thea ha testhe hahea l t t thas athatou an e har to t s th te   han as s has ee hahes t the h t wis h tha he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " s sh   t\n",
      " s t hahhe ase he  t athes\n",
      " thha ar he a hhothhe t th a hhe\n",
      "  h hon\n",
      "hhe thos\n",
      " t th hehe\n",
      "he s hatharon\n",
      "\n",
      "a  are a hathe  s ara theohhes he t ha athe h ane h t sotot han ha ah  a a  h h a t he he the ath he\n",
      "ath a s th hore s ho t t hha t a t h t h sosho    anha h ath th h s  hh  hose\n",
      "he s\n",
      " s  h se t s     he he  tat ha hos h shh ar ha  hor theathorhan hha s\n",
      " s hothhorane he\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 150/50000 | Running Avg Train Loss: 3.52977 | Grad Norm: 1.253 | Running Avg Tokens/Sec: 6653.827\n",
      "Step 200/50000 | Running Avg Train Loss: 3.35472 | Grad Norm: 1.103 | Running Avg Tokens/Sec: 6666.295\n",
      "Est. Val Loss: 2.65699\n",
      "vt_4_LAYERs_4_HEAD_128_EMBD_DIM_128_SEQ_LEN Output, Step 200\n",
      "    UNCONDITIONAL GENERATION:\n",
      "\n",
      "    Top-k (5) (500 max_tokens):\n",
      "    \n",
      "U on s sor asalas s lang s ther thal less m mas l se se bes me s senthe mis alil son tals t t t ar porer moungal te llase bone le male asesat seraser ben alle tee s ser s aleso sout and s s an bo athingathen herast an s men mor seneneeer saran an st mere t s as tes totho thit ter therer me as be men thand\n",
      "\n",
      "\n",
      "Woutit aren s,\n",
      "As as an s me me he mend se mangan thal ble t toreend bandise men menorind an sase me th minorane ato as s andoraleesoun mano mend,\n",
      "\n",
      "\n",
      "\n",
      "Wereronges t ses s s atothales, the toron\n",
      "\n",
      "    Nucleus (0.5) (500 max_tokens):\n",
      "    \n",
      "OI\n",
      "UONe thas me tolos the thesse mes t t mes t me athathare so mes merend the p lend mesesthe me and hanthe se s thas an se there me lenges me so s s sere thene me asotherothe theso t bon an me s s ses the se manerathe the t mese s hen ane thande sound mang t se an s m thene man therenen ben me m m s t asengese beng me t me mend asor t s t bothaseng bereng angounen thase her hen m than har m t the mare mes t llaron se s beng mathand has merer sere son me mengond t s s thane an as hen athe me the\n",
      "\n",
      "    #####################################################\n",
      "    CONDITIONAL GENERATION (Top-k (5), 500 max_tokens):\n",
      "    KING TERRY: Thou arton souse s at t s soren le mas se ser as mesast thimore an ple manthe bes pal thes thand an then the ar s anor t alarond hange alllere as seser tharangare mate t ber mind ale se se sangeronesange mithengot t t t sathorerer mo areee anore asotill thillan b mound,\n",
      "H t an ble sas the se be angouthen thone alere t sto be he atheerend thas mas s beno s mat lero bll mot anes se s ale til me sal an sonothousalalasthingere the th s heren as menge tereest th sathangores t sthesar so atisee athan t s more\n",
      "\n",
      "DANIEL: Ay, my dear, te ther byand te s an at t thon thalasalan an man anto m ales te t man mouseend al banes theren tit mat sand se at an b the s me t asoun ase hee tilares sthanger he s s s hen m male hee t be s s herenono t thas hiserer sasengongan sorisallle t balest ther hes sene harases s mat aland toun te tharo t as me man t be me tit athenengothale thee se send t t beneril tindo alo s se b ang hand th the st hig mousater s m meraler s me malend me t at an borine allerallorithen honoron mator s bland\n",
      "\n",
      "Wher a\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "Step 250/50000 | Running Avg Train Loss: 3.22721 | Grad Norm: 1.102 | Running Avg Tokens/Sec: 6621.057\n"
     ]
    }
   ],
   "source": [
    "## Driver code\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf95154-d641-4d9b-9e8a-c054d5921921",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ee940-9794-467a-8088-2b59ac5aaeae",
   "metadata": {},
   "source": [
    "*After 2250 steps *  16 batch_size, training loss 1.8277:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67bda2ae-5fa0-4120-9b55-41121d9ca152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AUCENTIO:\n",
      "Which the may sich that nough to the slay'd one so to;\n",
      "His be knot, I Vistrengs, thy the good our kim in to call:\n",
      "No, thou man I good, Say, for pmburds tell eack.\n",
      "\n",
      "HESSend.\n",
      "\n",
      "MENCIO:\n",
      "Dever and will'd my Vaing, life to suke in lise,\n",
      "These'll was of yret, fol smy his no Fear shom gestard:\n",
      "Retil appoutis commentex e'epon tend her his him buse,\n",
      "And what ityer am the iends, come; God foll ding:\n",
      "by appeerk.\n",
      "\n",
      "LOUCIO:\n",
      "Petwild, bake you, that I same, what wear;\n",
      "from in in or my speak as For Jul\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "772a7473-2e8f-4c61-87d9-612e01150b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRY: thou arte a my she\n",
      "Which have may and of contain.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Good as I tall no knrow, for shalt agarnt\n",
      "And mpo; and Kong a m, not outhpile Mesce.\n",
      "\n",
      "HENRY VI:\n",
      "When I will thy lookess, oner the pexstrey\n",
      "The the the hee voagh gresed livioe.\n",
      "\n",
      "MENCIO:\n",
      "My her callis his peaced of to that\n",
      "We where's by shall bore: as shall myselvea\n",
      "The plender feuls!\n",
      "\n",
      "PAPELLANT:\n",
      "In the the into balby me dods to love,\n",
      "In but the giving of nyou ase. I tall it-me?'e Goveuling\n",
      "The theer haught art praver count madeng Camen:\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"TERRY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(decode(model.generate(torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
