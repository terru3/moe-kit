Transformer(
  (token_embedding): Embedding(65, 128)
  (position_embedding): PositionalEncoding()
  (blocks): Sequential(
    (0): Block(
      (sa): MultiHeadAttention(
        (drop): Dropout(p=0.1, inplace=False)
        (query): Linear(in_features=128, out_features=128, bias=False)
        (key): Linear(in_features=128, out_features=128, bias=False)
        (value): Linear(in_features=128, out_features=128, bias=False)
        (out): Linear(in_features=128, out_features=128, bias=False)
      )
      (ff): MLP(
        (net): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SwiGLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (mlp_drop): Dropout(p=0.1, inplace=False)
      (expert_drop): Dropout(p=0.3, inplace=False)
    )
    (1): Block(
      (sa): MultiHeadAttention(
        (drop): Dropout(p=0.1, inplace=False)
        (query): Linear(in_features=128, out_features=128, bias=False)
        (key): Linear(in_features=128, out_features=128, bias=False)
        (value): Linear(in_features=128, out_features=128, bias=False)
        (out): Linear(in_features=128, out_features=128, bias=False)
      )
      (ff): MLP(
        (net): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SwiGLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (mlp_drop): Dropout(p=0.1, inplace=False)
      (expert_drop): Dropout(p=0.3, inplace=False)
    )
    (2): Block(
      (sa): MultiHeadAttention(
        (drop): Dropout(p=0.1, inplace=False)
        (query): Linear(in_features=128, out_features=128, bias=False)
        (key): Linear(in_features=128, out_features=128, bias=False)
        (value): Linear(in_features=128, out_features=128, bias=False)
        (out): Linear(in_features=128, out_features=128, bias=False)
      )
      (ff): MLP(
        (net): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SwiGLU()
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (mlp_drop): Dropout(p=0.1, inplace=False)
      (expert_drop): Dropout(p=0.3, inplace=False)
    )
    (3): Block(
      (sa): MultiHeadAttention(
        (drop): Dropout(p=0.1, inplace=False)
        (query): Linear(in_features=128, out_features=128, bias=False)
        (key): Linear(in_features=128, out_features=128, bias=False)
        (value): Linear(in_features=128, out_features=128, bias=False)
        (out): Linear(in_features=128, out_features=128, bias=False)
      )
      (ff): SwitchFeedForward(
        (experts): ModuleList(
          (0-63): 64 x MLP(
            (net): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): SwiGLU()
              (2): Dropout(p=0.1, inplace=False)
              (3): Linear(in_features=256, out_features=128, bias=True)
            )
          )
        )
        (switch): Linear(in_features=128, out_features=64, bias=True)
      )
      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (mlp_drop): Dropout(p=0.1, inplace=False)
      (expert_drop): Dropout(p=0.3, inplace=False)
    )
  )
  (lm_head): Linear(in_features=128, out_features=65, bias=True)
  (drop): Dropout(p=0.1, inplace=False)
)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.01
)
<torch.cuda.amp.grad_scaler.GradScaler object at 0x1408e8290>